==================================================================================================== - 2025-05-26 21:34:18,577 - log
        - random_seed : 2025 - 2025-05-26 21:34:18,578 - log
        - lr : 0.0002 - 2025-05-26 21:34:18,578 - log
        - weight_decay : 0.01 - 2025-05-26 21:34:18,578 - log
        - correct_bias : False - 2025-05-26 21:34:18,578 - log
        - adam_epislon : 1e-06 - 2025-05-26 21:34:18,578 - log
        - no_decay_bias : False - 2025-05-26 21:34:18,578 - log
        - adam_beta1 : 0.9 - 2025-05-26 21:34:18,578 - log
        - adam_beta2 : 0.999 - 2025-05-26 21:34:18,578 - log
        - scheduler : linear - 2025-05-26 21:34:18,578 - log
        - max_step : None - 2025-05-26 21:34:18,578 - log
        - max_epoch : 5 - 2025-05-26 21:34:18,578 - log
        - warmup_step : 500 - 2025-05-26 21:34:18,578 - log
        - i_steps : 0 - 2025-05-26 21:34:18,578 - log
        - i_lrs : 0.00025 - 2025-05-26 21:34:18,578 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-26 21:34:18,578 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-26 21:34:18,578 - log
        - train_batch_size : 2 - 2025-05-26 21:34:18,578 - log
        - valid_batch_size : 1 - 2025-05-26 21:34:18,578 - log
        - grad_acc : 2 - 2025-05-26 21:34:18,578 - log
        - clip : 0.0 - 2025-05-26 21:34:18,578 - log
        - seq_len : 64 - 2025-05-26 21:34:18,578 - log
        - model_card : gpt2.sm - 2025-05-26 21:34:18,578 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-26 21:34:18,578 - log
        - fp16 : False - 2025-05-26 21:34:18,578 - log
        - log_interval : 100 - 2025-05-26 21:34:18,578 - log
        - eval_interval : 2000 - 2025-05-26 21:34:18,578 - log
        - save_interval : 1000 - 2025-05-26 21:34:18,578 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-26 21:34:18,578 - log
        - lora_dim : 4 - 2025-05-26 21:34:18,578 - log
        - lora_alpha : 32 - 2025-05-26 21:34:18,578 - log
        - obj : clm - 2025-05-26 21:34:18,578 - log
        - lora_dropout : 0.1 - 2025-05-26 21:34:18,578 - log
        - label_smooth : 0.1 - 2025-05-26 21:34:18,578 - log
        - roll_interval : -1 - 2025-05-26 21:34:18,579 - log
        - roll_lr : 1e-05 - 2025-05-26 21:34:18,579 - log
        - roll_step : 100 - 2025-05-26 21:34:18,579 - log
        - eval_epoch : 1 - 2025-05-26 21:34:18,579 - log
        - device : cuda - 2025-05-26 21:34:18,579 - log
==================================================================================================== - 2025-05-26 21:34:18,579 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-26 21:34:18,579 - log
==================================================================================================== - 2025-05-26 22:54:16,724 - log
        - random_seed : 2025 - 2025-05-26 22:54:16,724 - log
        - lr : 0.0002 - 2025-05-26 22:54:16,724 - log
        - weight_decay : 0.01 - 2025-05-26 22:54:16,724 - log
        - correct_bias : False - 2025-05-26 22:54:16,724 - log
        - adam_epislon : 1e-06 - 2025-05-26 22:54:16,724 - log
        - no_decay_bias : False - 2025-05-26 22:54:16,725 - log
        - adam_beta1 : 0.9 - 2025-05-26 22:54:16,725 - log
        - adam_beta2 : 0.999 - 2025-05-26 22:54:16,725 - log
        - scheduler : linear - 2025-05-26 22:54:16,725 - log
        - max_step : None - 2025-05-26 22:54:16,725 - log
        - max_epoch : 5 - 2025-05-26 22:54:16,725 - log
        - warmup_step : 500 - 2025-05-26 22:54:16,725 - log
        - i_steps : 0 - 2025-05-26 22:54:16,725 - log
        - i_lrs : 0.00025 - 2025-05-26 22:54:16,725 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-26 22:54:16,725 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-26 22:54:16,725 - log
        - train_batch_size : 2 - 2025-05-26 22:54:16,725 - log
        - valid_batch_size : 1 - 2025-05-26 22:54:16,725 - log
        - grad_acc : 2 - 2025-05-26 22:54:16,725 - log
        - clip : 0.0 - 2025-05-26 22:54:16,725 - log
        - seq_len : 64 - 2025-05-26 22:54:16,725 - log
        - model_card : gpt2.sm - 2025-05-26 22:54:16,725 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-26 22:54:16,725 - log
        - fp16 : False - 2025-05-26 22:54:16,725 - log
        - log_interval : 100 - 2025-05-26 22:54:16,725 - log
        - eval_interval : 2000 - 2025-05-26 22:54:16,725 - log
        - save_interval : 1000 - 2025-05-26 22:54:16,725 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-26 22:54:16,725 - log
        - lora_dim : 4 - 2025-05-26 22:54:16,725 - log
        - lora_alpha : 32 - 2025-05-26 22:54:16,725 - log
        - obj : clm - 2025-05-26 22:54:16,725 - log
        - lora_dropout : 0.1 - 2025-05-26 22:54:16,725 - log
        - label_smooth : 0.1 - 2025-05-26 22:54:16,725 - log
        - roll_interval : -1 - 2025-05-26 22:54:16,725 - log
        - roll_lr : 1e-05 - 2025-05-26 22:54:16,725 - log
        - roll_step : 100 - 2025-05-26 22:54:16,725 - log
        - eval_epoch : 1 - 2025-05-26 22:54:16,725 - log
        - device : cuda - 2025-05-26 22:54:16,725 - log
==================================================================================================== - 2025-05-26 22:54:16,725 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-26 22:54:16,725 - log
==================================================================================================== - 2025-05-26 22:54:53,784 - log
        - random_seed : 2025 - 2025-05-26 22:54:53,784 - log
        - lr : 0.0002 - 2025-05-26 22:54:53,784 - log
        - weight_decay : 0.01 - 2025-05-26 22:54:53,784 - log
        - correct_bias : False - 2025-05-26 22:54:53,784 - log
        - adam_epislon : 1e-06 - 2025-05-26 22:54:53,784 - log
        - no_decay_bias : False - 2025-05-26 22:54:53,784 - log
        - adam_beta1 : 0.9 - 2025-05-26 22:54:53,785 - log
        - adam_beta2 : 0.999 - 2025-05-26 22:54:53,785 - log
        - scheduler : linear - 2025-05-26 22:54:53,785 - log
        - max_step : None - 2025-05-26 22:54:53,785 - log
        - max_epoch : 5 - 2025-05-26 22:54:53,785 - log
        - warmup_step : 500 - 2025-05-26 22:54:53,785 - log
        - i_steps : 0 - 2025-05-26 22:54:53,785 - log
        - i_lrs : 0.00025 - 2025-05-26 22:54:53,785 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-26 22:54:53,785 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-26 22:54:53,785 - log
        - train_batch_size : 2 - 2025-05-26 22:54:53,785 - log
        - valid_batch_size : 1 - 2025-05-26 22:54:53,785 - log
        - grad_acc : 2 - 2025-05-26 22:54:53,785 - log
        - clip : 0.0 - 2025-05-26 22:54:53,785 - log
        - seq_len : 64 - 2025-05-26 22:54:53,785 - log
        - model_card : gpt2.sm - 2025-05-26 22:54:53,785 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-26 22:54:53,785 - log
        - fp16 : False - 2025-05-26 22:54:53,785 - log
        - log_interval : 100 - 2025-05-26 22:54:53,785 - log
        - eval_interval : 2000 - 2025-05-26 22:54:53,785 - log
        - save_interval : 1000 - 2025-05-26 22:54:53,785 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-26 22:54:53,785 - log
        - lora_dim : 4 - 2025-05-26 22:54:53,785 - log
        - lora_alpha : 32 - 2025-05-26 22:54:53,785 - log
        - obj : clm - 2025-05-26 22:54:53,785 - log
        - lora_dropout : 0.1 - 2025-05-26 22:54:53,785 - log
        - label_smooth : 0.1 - 2025-05-26 22:54:53,785 - log
        - roll_interval : -1 - 2025-05-26 22:54:53,785 - log
        - roll_lr : 1e-05 - 2025-05-26 22:54:53,785 - log
        - roll_step : 100 - 2025-05-26 22:54:53,785 - log
        - eval_epoch : 1 - 2025-05-26 22:54:53,785 - log
        - device : cuda - 2025-05-26 22:54:53,785 - log
==================================================================================================== - 2025-05-26 22:54:53,785 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-26 22:54:53,785 - log
==================================================================================================== - 2025-05-26 23:35:43,014 - log
        - random_seed : 2025 - 2025-05-26 23:35:43,014 - log
        - lr : 0.0002 - 2025-05-26 23:35:43,014 - log
        - weight_decay : 0.01 - 2025-05-26 23:35:43,014 - log
        - correct_bias : False - 2025-05-26 23:35:43,014 - log
        - adam_epislon : 1e-06 - 2025-05-26 23:35:43,014 - log
        - no_decay_bias : False - 2025-05-26 23:35:43,014 - log
        - adam_beta1 : 0.9 - 2025-05-26 23:35:43,014 - log
        - adam_beta2 : 0.999 - 2025-05-26 23:35:43,014 - log
        - scheduler : linear - 2025-05-26 23:35:43,014 - log
        - max_step : None - 2025-05-26 23:35:43,014 - log
        - max_epoch : 5 - 2025-05-26 23:35:43,014 - log
        - warmup_step : 500 - 2025-05-26 23:35:43,014 - log
        - i_steps : 0 - 2025-05-26 23:35:43,014 - log
        - i_lrs : 0.00025 - 2025-05-26 23:35:43,014 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-26 23:35:43,014 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-26 23:35:43,014 - log
        - train_batch_size : 2 - 2025-05-26 23:35:43,014 - log
        - valid_batch_size : 1 - 2025-05-26 23:35:43,014 - log
        - grad_acc : 2 - 2025-05-26 23:35:43,014 - log
        - clip : 0.0 - 2025-05-26 23:35:43,014 - log
        - seq_len : 64 - 2025-05-26 23:35:43,014 - log
        - model_card : gpt2.sm - 2025-05-26 23:35:43,014 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-26 23:35:43,014 - log
        - fp16 : False - 2025-05-26 23:35:43,014 - log
        - log_interval : 100 - 2025-05-26 23:35:43,014 - log
        - eval_interval : 2000 - 2025-05-26 23:35:43,015 - log
        - save_interval : 1000 - 2025-05-26 23:35:43,015 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-26 23:35:43,015 - log
        - lora_dim : 4 - 2025-05-26 23:35:43,015 - log
        - lora_alpha : 32 - 2025-05-26 23:35:43,015 - log
        - obj : clm - 2025-05-26 23:35:43,015 - log
        - lora_dropout : 0.1 - 2025-05-26 23:35:43,015 - log
        - label_smooth : 0.1 - 2025-05-26 23:35:43,015 - log
        - roll_interval : -1 - 2025-05-26 23:35:43,015 - log
        - roll_lr : 1e-05 - 2025-05-26 23:35:43,015 - log
        - roll_step : 100 - 2025-05-26 23:35:43,015 - log
        - eval_epoch : 1 - 2025-05-26 23:35:43,015 - log
        - device : cuda - 2025-05-26 23:35:43,015 - log
==================================================================================================== - 2025-05-26 23:35:43,015 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-26 23:35:43,015 - log
loading model pretrained weight. - 2025-05-26 23:35:53,149 - log
set max_step: 0 - 2025-05-26 23:36:02,397 - log
start to train the model................ 1 - 2025-05-26 23:36:02,398 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.0.pkl - 2025-05-26 23:36:05,723 - log
---------------------------------------------------------------------------------------------------- - 2025-05-26 23:36:07,344 - log
End of training - 2025-05-26 23:36:07,344 - log
ms/batch  0.00 - 2025-05-26 23:36:07,344 - log
==================================================================================================== - 2025-05-26 23:48:14,739 - log
        - random_seed : 2025 - 2025-05-26 23:48:14,739 - log
        - lr : 0.0002 - 2025-05-26 23:48:14,739 - log
        - weight_decay : 0.01 - 2025-05-26 23:48:14,739 - log
        - correct_bias : False - 2025-05-26 23:48:14,739 - log
        - adam_epislon : 1e-06 - 2025-05-26 23:48:14,739 - log
        - no_decay_bias : False - 2025-05-26 23:48:14,739 - log
        - adam_beta1 : 0.9 - 2025-05-26 23:48:14,739 - log
        - adam_beta2 : 0.999 - 2025-05-26 23:48:14,739 - log
        - scheduler : linear - 2025-05-26 23:48:14,739 - log
        - max_step : None - 2025-05-26 23:48:14,739 - log
        - max_epoch : 5 - 2025-05-26 23:48:14,739 - log
        - warmup_step : 500 - 2025-05-26 23:48:14,739 - log
        - i_steps : 0 - 2025-05-26 23:48:14,739 - log
        - i_lrs : 0.00025 - 2025-05-26 23:48:14,739 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-26 23:48:14,739 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-26 23:48:14,739 - log
        - train_batch_size : 2 - 2025-05-26 23:48:14,739 - log
        - valid_batch_size : 1 - 2025-05-26 23:48:14,739 - log
        - grad_acc : 2 - 2025-05-26 23:48:14,739 - log
        - clip : 0.0 - 2025-05-26 23:48:14,739 - log
        - seq_len : 64 - 2025-05-26 23:48:14,739 - log
        - model_card : gpt2.sm - 2025-05-26 23:48:14,739 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-26 23:48:14,739 - log
        - fp16 : False - 2025-05-26 23:48:14,739 - log
        - log_interval : 100 - 2025-05-26 23:48:14,739 - log
        - eval_interval : 2000 - 2025-05-26 23:48:14,739 - log
        - save_interval : 1000 - 2025-05-26 23:48:14,739 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-26 23:48:14,739 - log
        - lora_dim : 4 - 2025-05-26 23:48:14,739 - log
        - lora_alpha : 32 - 2025-05-26 23:48:14,739 - log
        - obj : clm - 2025-05-26 23:48:14,739 - log
        - lora_dropout : 0.1 - 2025-05-26 23:48:14,739 - log
        - label_smooth : 0.1 - 2025-05-26 23:48:14,739 - log
        - roll_interval : -1 - 2025-05-26 23:48:14,739 - log
        - roll_lr : 1e-05 - 2025-05-26 23:48:14,739 - log
        - roll_step : 100 - 2025-05-26 23:48:14,739 - log
        - eval_epoch : 1 - 2025-05-26 23:48:14,739 - log
        - device : cuda - 2025-05-26 23:48:14,739 - log
==================================================================================================== - 2025-05-26 23:48:14,740 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-26 23:48:14,740 - log
loading model pretrained weight. - 2025-05-26 23:48:15,161 - log
set max_step: 105155 - 2025-05-26 23:48:16,140 - log
start to train the model................ 1 - 2025-05-26 23:48:16,140 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 477.51 | loss  5.34 | avg loss  5.90 | ppl 363.80 - 2025-05-26 23:49:03,891 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.37 | loss  3.76 | avg loss  4.82 | ppl 124.58 - 2025-05-26 23:49:06,830 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.31 | loss  3.23 | avg loss  3.60 | ppl 36.42 - 2025-05-26 23:49:09,761 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.40 | loss  2.83 | avg loss  3.46 | ppl 31.68 - 2025-05-26 23:49:12,702 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.29 | loss  3.22 | avg loss  3.27 | ppl 26.28 - 2025-05-26 23:49:15,632 - log
| epoch   1 step      600 |    600 batches | lr 0.0002 | ms/batch 29.28 | loss  2.93 | avg loss  3.18 | ppl 24.13 - 2025-05-26 23:49:18,560 - log
| epoch   1 step      700 |    700 batches | lr 0.0002 | ms/batch 29.54 | loss  3.51 | avg loss  3.17 | ppl 23.74 - 2025-05-26 23:49:21,515 - log
| epoch   1 step      800 |    800 batches | lr 0.000199 | ms/batch 29.38 | loss  2.95 | avg loss  3.14 | ppl 23.15 - 2025-05-26 23:49:24,453 - log
| epoch   1 step      900 |    900 batches | lr 0.000199 | ms/batch 29.70 | loss  4.15 | avg loss  3.03 | ppl 20.74 - 2025-05-26 23:49:27,424 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000199 | ms/batch 29.54 | loss  3.75 | avg loss  3.06 | ppl 21.42 - 2025-05-26 23:49:30,378 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-26 23:49:30,378 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000199 | ms/batch 29.92 | loss  2.85 | avg loss  3.02 | ppl 20.47 - 2025-05-26 23:49:33,371 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000199 | ms/batch 29.57 | loss  3.04 | avg loss  3.05 | ppl 21.06 - 2025-05-26 23:49:36,328 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000198 | ms/batch 29.65 | loss  3.55 | avg loss  3.09 | ppl 21.90 - 2025-05-26 23:49:39,293 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000198 | ms/batch 29.51 | loss  3.40 | avg loss  3.00 | ppl 20.13 - 2025-05-26 23:49:42,245 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000198 | ms/batch 29.46 | loss  2.97 | avg loss  3.00 | ppl 20.18 - 2025-05-26 23:49:45,191 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000198 | ms/batch 29.47 | loss  2.73 | avg loss  3.00 | ppl 20.16 - 2025-05-26 23:49:48,138 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000198 | ms/batch 29.53 | loss  2.84 | avg loss  2.95 | ppl 19.19 - 2025-05-26 23:49:51,091 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000198 | ms/batch 29.72 | loss  2.42 | avg loss  2.96 | ppl 19.28 - 2025-05-26 23:49:54,063 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000197 | ms/batch 29.39 | loss  2.65 | avg loss  2.94 | ppl 18.91 - 2025-05-26 23:49:57,002 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000197 | ms/batch 29.91 | loss  2.68 | avg loss  2.93 | ppl 18.77 - 2025-05-26 23:49:59,994 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-26 23:49:59,994 - log
==================================================================================================== - 2025-05-28 11:19:44,757 - log
        - random_seed : 2025 - 2025-05-28 11:19:44,757 - log
        - lr : 0.0002 - 2025-05-28 11:19:44,757 - log
        - weight_decay : 0.01 - 2025-05-28 11:19:44,757 - log
        - correct_bias : False - 2025-05-28 11:19:44,757 - log
        - adam_epislon : 1e-06 - 2025-05-28 11:19:44,757 - log
        - no_decay_bias : False - 2025-05-28 11:19:44,757 - log
        - adam_beta1 : 0.9 - 2025-05-28 11:19:44,757 - log
        - adam_beta2 : 0.999 - 2025-05-28 11:19:44,757 - log
        - scheduler : linear - 2025-05-28 11:19:44,757 - log
        - max_step : None - 2025-05-28 11:19:44,757 - log
        - max_epoch : 5 - 2025-05-28 11:19:44,757 - log
        - warmup_step : 500 - 2025-05-28 11:19:44,757 - log
        - i_steps : 0 - 2025-05-28 11:19:44,757 - log
        - i_lrs : 0.00025 - 2025-05-28 11:19:44,757 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 11:19:44,757 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 11:19:44,757 - log
        - train_batch_size : 2 - 2025-05-28 11:19:44,757 - log
        - valid_batch_size : 1 - 2025-05-28 11:19:44,757 - log
        - grad_acc : 2 - 2025-05-28 11:19:44,757 - log
        - clip : 0.0 - 2025-05-28 11:19:44,757 - log
        - seq_len : 64 - 2025-05-28 11:19:44,757 - log
        - model_card : gpt2.sm - 2025-05-28 11:19:44,757 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 11:19:44,757 - log
        - fp16 : False - 2025-05-28 11:19:44,757 - log
        - log_interval : 100 - 2025-05-28 11:19:44,757 - log
        - eval_interval : 2000 - 2025-05-28 11:19:44,757 - log
        - save_interval : 1000 - 2025-05-28 11:19:44,757 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 11:19:44,757 - log
        - lora_dim : 4 - 2025-05-28 11:19:44,757 - log
        - lora_alpha : 32 - 2025-05-28 11:19:44,757 - log
        - obj : clm - 2025-05-28 11:19:44,757 - log
        - lora_dropout : 0.1 - 2025-05-28 11:19:44,757 - log
        - label_smooth : 0.1 - 2025-05-28 11:19:44,757 - log
        - roll_interval : -1 - 2025-05-28 11:19:44,757 - log
        - roll_lr : 1e-05 - 2025-05-28 11:19:44,757 - log
        - roll_step : 100 - 2025-05-28 11:19:44,758 - log
        - eval_epoch : 1 - 2025-05-28 11:19:44,758 - log
        - device : cuda - 2025-05-28 11:19:44,758 - log
==================================================================================================== - 2025-05-28 11:19:44,758 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 11:19:44,758 - log
loading model pretrained weight. - 2025-05-28 11:19:45,158 - log
set max_step: 105155 - 2025-05-28 11:19:46,373 - log
start to train the model................ 1 - 2025-05-28 11:19:46,373 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.60 | loss  5.34 | avg loss  5.90 | ppl 363.80 - 2025-05-28 11:19:49,734 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.39 | loss  3.76 | avg loss  4.82 | ppl 124.58 - 2025-05-28 11:19:52,673 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.64 | loss  3.23 | avg loss  3.60 | ppl 36.42 - 2025-05-28 11:19:55,637 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.67 | loss  2.83 | avg loss  3.46 | ppl 31.68 - 2025-05-28 11:19:58,604 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.45 | loss  3.22 | avg loss  3.27 | ppl 26.28 - 2025-05-28 11:20:01,550 - log
| epoch   1 step      600 |    600 batches | lr 0.0002 | ms/batch 29.49 | loss  2.93 | avg loss  3.18 | ppl 24.13 - 2025-05-28 11:20:04,499 - log
| epoch   1 step      700 |    700 batches | lr 0.0002 | ms/batch 29.81 | loss  3.51 | avg loss  3.17 | ppl 23.74 - 2025-05-28 11:20:07,480 - log
| epoch   1 step      800 |    800 batches | lr 0.000199 | ms/batch 29.92 | loss  2.95 | avg loss  3.14 | ppl 23.15 - 2025-05-28 11:20:10,472 - log
| epoch   1 step      900 |    900 batches | lr 0.000199 | ms/batch 29.50 | loss  4.15 | avg loss  3.03 | ppl 20.74 - 2025-05-28 11:20:13,422 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000199 | ms/batch 29.49 | loss  3.75 | avg loss  3.06 | ppl 21.42 - 2025-05-28 11:20:16,371 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 11:20:16,372 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000199 | ms/batch 29.63 | loss  2.85 | avg loss  3.02 | ppl 20.47 - 2025-05-28 11:20:19,335 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000199 | ms/batch 29.58 | loss  3.04 | avg loss  3.05 | ppl 21.06 - 2025-05-28 11:20:22,293 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000198 | ms/batch 29.62 | loss  3.55 | avg loss  3.09 | ppl 21.90 - 2025-05-28 11:20:25,256 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000198 | ms/batch 29.47 | loss  3.40 | avg loss  3.00 | ppl 20.13 - 2025-05-28 11:20:28,203 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000198 | ms/batch 29.62 | loss  2.97 | avg loss  3.00 | ppl 20.18 - 2025-05-28 11:20:31,166 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000198 | ms/batch 29.80 | loss  2.73 | avg loss  3.00 | ppl 20.16 - 2025-05-28 11:20:34,146 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000198 | ms/batch 29.53 | loss  2.84 | avg loss  2.95 | ppl 19.19 - 2025-05-28 11:20:37,099 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000198 | ms/batch 29.60 | loss  2.42 | avg loss  2.96 | ppl 19.28 - 2025-05-28 11:20:40,060 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000197 | ms/batch 29.59 | loss  2.65 | avg loss  2.94 | ppl 18.91 - 2025-05-28 11:20:43,019 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000197 | ms/batch 29.58 | loss  2.68 | avg loss  2.93 | ppl 18.77 - 2025-05-28 11:20:45,977 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 11:20:45,977 - log
==================================================================================================== - 2025-05-28 12:02:05,228 - log
        - random_seed : 2025 - 2025-05-28 12:02:05,228 - log
        - lr : 0.0002 - 2025-05-28 12:02:05,228 - log
        - weight_decay : 0.01 - 2025-05-28 12:02:05,228 - log
        - correct_bias : False - 2025-05-28 12:02:05,228 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:02:05,228 - log
        - no_decay_bias : False - 2025-05-28 12:02:05,228 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:02:05,228 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:02:05,228 - log
        - scheduler : linear - 2025-05-28 12:02:05,228 - log
        - max_step : None - 2025-05-28 12:02:05,228 - log
        - max_epoch : 5 - 2025-05-28 12:02:05,228 - log
        - warmup_step : 500 - 2025-05-28 12:02:05,228 - log
        - i_steps : 0 - 2025-05-28 12:02:05,228 - log
        - i_lrs : 0.00025 - 2025-05-28 12:02:05,228 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:02:05,228 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:02:05,228 - log
        - train_batch_size : 2 - 2025-05-28 12:02:05,229 - log
        - valid_batch_size : 1 - 2025-05-28 12:02:05,229 - log
        - grad_acc : 2 - 2025-05-28 12:02:05,229 - log
        - clip : 0.0 - 2025-05-28 12:02:05,229 - log
        - seq_len : 64 - 2025-05-28 12:02:05,229 - log
        - model_card : gpt2.sm - 2025-05-28 12:02:05,229 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:02:05,229 - log
        - fp16 : False - 2025-05-28 12:02:05,229 - log
        - log_interval : 100 - 2025-05-28 12:02:05,229 - log
        - eval_interval : 2000 - 2025-05-28 12:02:05,229 - log
        - save_interval : 1000 - 2025-05-28 12:02:05,229 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:02:05,229 - log
        - lora_dim : 4 - 2025-05-28 12:02:05,229 - log
        - lora_alpha : 32 - 2025-05-28 12:02:05,229 - log
        - obj : clm - 2025-05-28 12:02:05,229 - log
        - lora_dropout : 0.1 - 2025-05-28 12:02:05,229 - log
        - label_smooth : 0.1 - 2025-05-28 12:02:05,229 - log
        - roll_interval : -1 - 2025-05-28 12:02:05,229 - log
        - roll_lr : 1e-05 - 2025-05-28 12:02:05,229 - log
        - roll_step : 100 - 2025-05-28 12:02:05,229 - log
        - eval_epoch : 1 - 2025-05-28 12:02:05,229 - log
        - device : cuda - 2025-05-28 12:02:05,229 - log
==================================================================================================== - 2025-05-28 12:02:05,229 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:02:05,229 - log
loading model pretrained weight. - 2025-05-28 12:02:05,633 - log
set max_step: 105155 - 2025-05-28 12:02:06,518 - log
start to train the model................ 1 - 2025-05-28 12:02:06,518 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.70 | loss  5.34 | avg loss  5.90 | ppl 363.80 - 2025-05-28 12:02:09,988 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.00 | loss  3.76 | avg loss  4.82 | ppl 124.58 - 2025-05-28 12:02:12,989 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.87 | loss  3.23 | avg loss  3.60 | ppl 36.42 - 2025-05-28 12:02:15,977 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.01 | loss  2.83 | avg loss  3.46 | ppl 31.68 - 2025-05-28 12:02:18,978 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.86 | loss  3.22 | avg loss  3.27 | ppl 26.28 - 2025-05-28 12:02:21,964 - log
| epoch   1 step      600 |    600 batches | lr 0.0002 | ms/batch 30.03 | loss  2.93 | avg loss  3.18 | ppl 24.13 - 2025-05-28 12:02:24,968 - log
| epoch   1 step      700 |    700 batches | lr 0.0002 | ms/batch 29.87 | loss  3.51 | avg loss  3.17 | ppl 23.74 - 2025-05-28 12:02:27,955 - log
| epoch   1 step      800 |    800 batches | lr 0.000199 | ms/batch 29.89 | loss  2.95 | avg loss  3.14 | ppl 23.15 - 2025-05-28 12:02:30,945 - log
| epoch   1 step      900 |    900 batches | lr 0.000199 | ms/batch 29.96 | loss  4.15 | avg loss  3.03 | ppl 20.74 - 2025-05-28 12:02:33,941 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000199 | ms/batch 30.19 | loss  3.75 | avg loss  3.06 | ppl 21.42 - 2025-05-28 12:02:36,960 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:02:36,960 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000199 | ms/batch 30.01 | loss  2.85 | avg loss  3.02 | ppl 20.47 - 2025-05-28 12:02:39,961 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000199 | ms/batch 29.97 | loss  3.04 | avg loss  3.05 | ppl 21.06 - 2025-05-28 12:02:42,958 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000198 | ms/batch 30.06 | loss  3.55 | avg loss  3.09 | ppl 21.90 - 2025-05-28 12:02:45,964 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000198 | ms/batch 29.93 | loss  3.40 | avg loss  3.00 | ppl 20.13 - 2025-05-28 12:02:48,958 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000198 | ms/batch 30.04 | loss  2.97 | avg loss  3.00 | ppl 20.18 - 2025-05-28 12:02:51,962 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000198 | ms/batch 29.95 | loss  2.73 | avg loss  3.00 | ppl 20.16 - 2025-05-28 12:02:54,958 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000198 | ms/batch 30.19 | loss  2.84 | avg loss  2.95 | ppl 19.19 - 2025-05-28 12:02:57,977 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000198 | ms/batch 29.84 | loss  2.42 | avg loss  2.96 | ppl 19.28 - 2025-05-28 12:03:00,961 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000197 | ms/batch 29.82 | loss  2.65 | avg loss  2.94 | ppl 18.91 - 2025-05-28 12:03:03,943 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000197 | ms/batch 29.79 | loss  2.68 | avg loss  2.93 | ppl 18.77 - 2025-05-28 12:03:06,922 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:03:06,923 - log
==================================================================================================== - 2025-05-28 12:12:53,861 - log
        - random_seed : 2025 - 2025-05-28 12:12:53,861 - log
        - lr : 0.0002 - 2025-05-28 12:12:53,861 - log
        - weight_decay : 0.01 - 2025-05-28 12:12:53,861 - log
        - correct_bias : False - 2025-05-28 12:12:53,861 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:12:53,861 - log
        - no_decay_bias : False - 2025-05-28 12:12:53,861 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:12:53,861 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:12:53,861 - log
        - scheduler : linear - 2025-05-28 12:12:53,861 - log
        - max_step : None - 2025-05-28 12:12:53,861 - log
        - max_epoch : 5 - 2025-05-28 12:12:53,861 - log
        - warmup_step : 500 - 2025-05-28 12:12:53,861 - log
        - i_steps : 0 - 2025-05-28 12:12:53,861 - log
        - i_lrs : 0.00025 - 2025-05-28 12:12:53,861 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:12:53,861 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:12:53,861 - log
        - train_batch_size : 2 - 2025-05-28 12:12:53,861 - log
        - valid_batch_size : 1 - 2025-05-28 12:12:53,861 - log
        - grad_acc : 2 - 2025-05-28 12:12:53,861 - log
        - clip : 0.0 - 2025-05-28 12:12:53,861 - log
        - seq_len : 64 - 2025-05-28 12:12:53,861 - log
        - model_card : gpt2.sm - 2025-05-28 12:12:53,861 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:12:53,862 - log
        - fp16 : False - 2025-05-28 12:12:53,862 - log
        - log_interval : 100 - 2025-05-28 12:12:53,862 - log
        - eval_interval : 2000 - 2025-05-28 12:12:53,862 - log
        - save_interval : 1000 - 2025-05-28 12:12:53,862 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:12:53,862 - log
        - lora_dim : 4 - 2025-05-28 12:12:53,862 - log
        - lora_alpha : 32 - 2025-05-28 12:12:53,862 - log
        - obj : clm - 2025-05-28 12:12:53,862 - log
        - lora_dropout : 0.1 - 2025-05-28 12:12:53,862 - log
        - label_smooth : 0.1 - 2025-05-28 12:12:53,862 - log
        - roll_interval : -1 - 2025-05-28 12:12:53,862 - log
        - roll_lr : 1e-05 - 2025-05-28 12:12:53,862 - log
        - roll_step : 100 - 2025-05-28 12:12:53,862 - log
        - eval_epoch : 1 - 2025-05-28 12:12:53,862 - log
        - device : cuda - 2025-05-28 12:12:53,862 - log
==================================================================================================== - 2025-05-28 12:12:53,862 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:12:53,862 - log
loading model pretrained weight. - 2025-05-28 12:12:54,262 - log
set max_step: 105155 - 2025-05-28 12:12:55,246 - log
start to train the model................ 1 - 2025-05-28 12:12:55,246 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.65 | loss  5.34 | avg loss  5.90 | ppl 363.80 - 2025-05-28 12:12:58,611 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.02 | loss  3.76 | avg loss  4.82 | ppl 124.58 - 2025-05-28 12:13:01,614 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.46 | loss  3.23 | avg loss  3.60 | ppl 36.42 - 2025-05-28 12:13:04,660 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.46 | loss  2.83 | avg loss  3.46 | ppl 31.68 - 2025-05-28 12:13:07,707 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 30.47 | loss  3.22 | avg loss  3.27 | ppl 26.28 - 2025-05-28 12:13:10,754 - log
| epoch   1 step      600 |    600 batches | lr 0.0002 | ms/batch 30.48 | loss  2.93 | avg loss  3.18 | ppl 24.13 - 2025-05-28 12:13:13,803 - log
| epoch   1 step      700 |    700 batches | lr 0.0002 | ms/batch 29.93 | loss  3.51 | avg loss  3.17 | ppl 23.74 - 2025-05-28 12:13:16,797 - log
| epoch   1 step      800 |    800 batches | lr 0.000199 | ms/batch 29.98 | loss  2.95 | avg loss  3.14 | ppl 23.15 - 2025-05-28 12:13:19,795 - log
| epoch   1 step      900 |    900 batches | lr 0.000199 | ms/batch 29.92 | loss  4.15 | avg loss  3.03 | ppl 20.74 - 2025-05-28 12:13:22,788 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000199 | ms/batch 29.82 | loss  3.75 | avg loss  3.06 | ppl 21.42 - 2025-05-28 12:13:25,771 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:13:25,771 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000199 | ms/batch 29.62 | loss  2.85 | avg loss  3.02 | ppl 20.47 - 2025-05-28 12:13:28,733 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000199 | ms/batch 29.85 | loss  3.04 | avg loss  3.05 | ppl 21.06 - 2025-05-28 12:13:31,719 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000198 | ms/batch 30.31 | loss  3.55 | avg loss  3.09 | ppl 21.90 - 2025-05-28 12:13:34,750 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000198 | ms/batch 30.02 | loss  3.40 | avg loss  3.00 | ppl 20.13 - 2025-05-28 12:13:37,753 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000198 | ms/batch 29.80 | loss  2.97 | avg loss  3.00 | ppl 20.18 - 2025-05-28 12:13:40,733 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000198 | ms/batch 29.78 | loss  2.73 | avg loss  3.00 | ppl 20.16 - 2025-05-28 12:13:43,712 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000198 | ms/batch 29.72 | loss  2.84 | avg loss  2.95 | ppl 19.19 - 2025-05-28 12:13:46,683 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000198 | ms/batch 29.59 | loss  2.42 | avg loss  2.96 | ppl 19.28 - 2025-05-28 12:13:49,642 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000197 | ms/batch 29.61 | loss  2.65 | avg loss  2.94 | ppl 18.91 - 2025-05-28 12:13:52,604 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000197 | ms/batch 29.88 | loss  2.68 | avg loss  2.93 | ppl 18.77 - 2025-05-28 12:13:55,593 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:13:55,593 - log
==================================================================================================== - 2025-05-28 12:16:34,691 - log
        - random_seed : 2025 - 2025-05-28 12:16:34,692 - log
        - lr : 0.0002 - 2025-05-28 12:16:34,692 - log
        - weight_decay : 0.01 - 2025-05-28 12:16:34,692 - log
        - correct_bias : False - 2025-05-28 12:16:34,692 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:16:34,692 - log
        - no_decay_bias : False - 2025-05-28 12:16:34,692 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:16:34,692 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:16:34,692 - log
        - scheduler : linear - 2025-05-28 12:16:34,692 - log
        - max_step : None - 2025-05-28 12:16:34,692 - log
        - max_epoch : 5 - 2025-05-28 12:16:34,692 - log
        - warmup_step : 500 - 2025-05-28 12:16:34,692 - log
        - i_steps : 0 - 2025-05-28 12:16:34,692 - log
        - i_lrs : 0.00025 - 2025-05-28 12:16:34,692 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:16:34,692 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:16:34,692 - log
        - train_batch_size : 2 - 2025-05-28 12:16:34,692 - log
        - valid_batch_size : 1 - 2025-05-28 12:16:34,692 - log
        - grad_acc : 2 - 2025-05-28 12:16:34,692 - log
        - clip : 0.0 - 2025-05-28 12:16:34,692 - log
        - seq_len : 64 - 2025-05-28 12:16:34,692 - log
        - model_card : gpt2.sm - 2025-05-28 12:16:34,692 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:16:34,692 - log
        - fp16 : False - 2025-05-28 12:16:34,692 - log
        - log_interval : 100 - 2025-05-28 12:16:34,692 - log
        - eval_interval : 2000 - 2025-05-28 12:16:34,692 - log
        - save_interval : 1000 - 2025-05-28 12:16:34,692 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:16:34,692 - log
        - lora_dim : 4 - 2025-05-28 12:16:34,692 - log
        - lora_alpha : 32 - 2025-05-28 12:16:34,692 - log
        - obj : clm - 2025-05-28 12:16:34,692 - log
        - lora_dropout : 0.1 - 2025-05-28 12:16:34,692 - log
        - label_smooth : 0.1 - 2025-05-28 12:16:34,692 - log
        - roll_interval : -1 - 2025-05-28 12:16:34,692 - log
        - roll_lr : 1e-05 - 2025-05-28 12:16:34,692 - log
        - roll_step : 100 - 2025-05-28 12:16:34,692 - log
        - eval_epoch : 1 - 2025-05-28 12:16:34,692 - log
        - device : cuda - 2025-05-28 12:16:34,692 - log
==================================================================================================== - 2025-05-28 12:16:34,692 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:16:34,692 - log
loading model pretrained weight. - 2025-05-28 12:16:34,794 - log
set max_step: 1050 - 2025-05-28 12:16:35,714 - log
start to train the model................ 1 - 2025-05-28 12:16:35,715 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.11 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-05-28 12:16:39,125 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.96 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-05-28 12:16:42,121 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-05-28 12:16:42,396 - log
start to train the model................ 2 - 2025-05-28 12:16:53,062 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 27.65 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-05-28 12:16:55,828 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 30.36 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-05-28 12:16:58,864 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-05-28 12:16:59,455 - log
start to train the model................ 3 - 2025-05-28 12:17:01,062 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 24.28 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-05-28 12:17:03,490 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 30.29 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:17:06,520 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-05-28 12:17:07,393 - log
start to train the model................ 4 - 2025-05-28 12:17:08,998 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 21.19 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-05-28 12:17:11,117 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 29.90 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-05-28 12:17:14,108 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-05-28 12:17:15,290 - log
start to train the model................ 5 - 2025-05-28 12:17:16,868 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 18.00 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-05-28 12:17:18,668 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 30.97 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-05-28 12:17:21,765 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:17:21,766 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-05-28 12:17:23,225 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 12:17:27,851 - log
End of training - 2025-05-28 12:17:27,851 - log
ms/batch 27.67 - 2025-05-28 12:17:27,851 - log
==================================================================================================== - 2025-05-28 12:22:28,277 - log
        - random_seed : 2025 - 2025-05-28 12:22:28,277 - log
        - lr : 0.0002 - 2025-05-28 12:22:28,277 - log
        - weight_decay : 0.01 - 2025-05-28 12:22:28,277 - log
        - correct_bias : False - 2025-05-28 12:22:28,278 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:22:28,278 - log
        - no_decay_bias : False - 2025-05-28 12:22:28,278 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:22:28,278 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:22:28,278 - log
        - scheduler : linear - 2025-05-28 12:22:28,278 - log
        - max_step : None - 2025-05-28 12:22:28,278 - log
        - max_epoch : 5 - 2025-05-28 12:22:28,278 - log
        - warmup_step : 500 - 2025-05-28 12:22:28,278 - log
        - i_steps : 0 - 2025-05-28 12:22:28,278 - log
        - i_lrs : 0.00025 - 2025-05-28 12:22:28,278 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:22:28,278 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:22:28,278 - log
        - train_batch_size : 2 - 2025-05-28 12:22:28,278 - log
        - valid_batch_size : 1 - 2025-05-28 12:22:28,278 - log
        - grad_acc : 2 - 2025-05-28 12:22:28,278 - log
        - clip : 0.0 - 2025-05-28 12:22:28,278 - log
        - seq_len : 64 - 2025-05-28 12:22:28,278 - log
        - model_card : gpt2.sm - 2025-05-28 12:22:28,278 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:22:28,278 - log
        - fp16 : False - 2025-05-28 12:22:28,278 - log
        - log_interval : 100 - 2025-05-28 12:22:28,278 - log
        - eval_interval : 2000 - 2025-05-28 12:22:28,278 - log
        - save_interval : 1000 - 2025-05-28 12:22:28,278 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:22:28,278 - log
        - lora_dim : 4 - 2025-05-28 12:22:28,278 - log
        - lora_alpha : 32 - 2025-05-28 12:22:28,278 - log
        - obj : clm - 2025-05-28 12:22:28,278 - log
        - lora_dropout : 0.1 - 2025-05-28 12:22:28,278 - log
        - label_smooth : 0.1 - 2025-05-28 12:22:28,278 - log
        - roll_interval : -1 - 2025-05-28 12:22:28,278 - log
        - roll_lr : 1e-05 - 2025-05-28 12:22:28,278 - log
        - roll_step : 100 - 2025-05-28 12:22:28,278 - log
        - eval_epoch : 1 - 2025-05-28 12:22:28,278 - log
        - device : cuda - 2025-05-28 12:22:28,278 - log
==================================================================================================== - 2025-05-28 12:22:28,278 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:22:28,278 - log
loading model pretrained weight. - 2025-05-28 12:22:28,380 - log
set max_step: 2105 - 2025-05-28 12:22:29,303 - log
start to train the model................ 1 - 2025-05-28 12:22:29,304 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.29 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:22:32,633 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.01 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:22:35,635 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.34 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:22:38,669 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.39 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:22:41,709 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:22:42,330 - log
start to train the model................ 2 - 2025-05-28 12:22:43,952 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.24 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:22:46,376 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 30.77 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:22:49,453 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 30.62 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:22:52,515 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 30.67 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:22:55,582 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:22:56,827 - log
start to train the model................ 3 - 2025-05-28 12:22:58,489 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.89 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:23:00,277 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.20 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:23:03,298 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:23:03,298 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.21 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:23:06,319 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.38 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:23:09,357 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:23:11,251 - log
start to train the model................ 4 - 2025-05-28 12:23:12,820 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.61 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:23:13,981 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.56 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:23:17,038 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.17 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:23:20,055 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.36 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:23:23,092 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:23:25,621 - log
start to train the model................ 5 - 2025-05-28 12:23:27,222 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.08 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:23:27,730 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.37 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:23:30,768 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.02 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:23:33,769 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.03 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:23:36,773 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:23:36,773 - log
==================================================================================================== - 2025-05-28 12:27:17,613 - log
        - random_seed : 2025 - 2025-05-28 12:27:17,613 - log
        - lr : 0.0002 - 2025-05-28 12:27:17,613 - log
        - weight_decay : 0.01 - 2025-05-28 12:27:17,614 - log
        - correct_bias : False - 2025-05-28 12:27:17,614 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:27:17,614 - log
        - no_decay_bias : False - 2025-05-28 12:27:17,614 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:27:17,614 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:27:17,614 - log
        - scheduler : linear - 2025-05-28 12:27:17,614 - log
        - max_step : None - 2025-05-28 12:27:17,614 - log
        - max_epoch : 5 - 2025-05-28 12:27:17,614 - log
        - warmup_step : 500 - 2025-05-28 12:27:17,614 - log
        - i_steps : 0 - 2025-05-28 12:27:17,614 - log
        - i_lrs : 0.00025 - 2025-05-28 12:27:17,614 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:27:17,614 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:27:17,614 - log
        - train_batch_size : 2 - 2025-05-28 12:27:17,614 - log
        - valid_batch_size : 1 - 2025-05-28 12:27:17,614 - log
        - grad_acc : 2 - 2025-05-28 12:27:17,614 - log
        - clip : 0.0 - 2025-05-28 12:27:17,614 - log
        - seq_len : 64 - 2025-05-28 12:27:17,614 - log
        - model_card : gpt2.sm - 2025-05-28 12:27:17,614 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:27:17,614 - log
        - fp16 : False - 2025-05-28 12:27:17,614 - log
        - log_interval : 100 - 2025-05-28 12:27:17,614 - log
        - eval_interval : 2000 - 2025-05-28 12:27:17,614 - log
        - save_interval : 1000 - 2025-05-28 12:27:17,614 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:27:17,614 - log
        - lora_dim : 4 - 2025-05-28 12:27:17,614 - log
        - lora_alpha : 32 - 2025-05-28 12:27:17,614 - log
        - obj : clm - 2025-05-28 12:27:17,614 - log
        - lora_dropout : 0.1 - 2025-05-28 12:27:17,614 - log
        - label_smooth : 0.1 - 2025-05-28 12:27:17,614 - log
        - roll_interval : -1 - 2025-05-28 12:27:17,614 - log
        - roll_lr : 1e-05 - 2025-05-28 12:27:17,614 - log
        - roll_step : 100 - 2025-05-28 12:27:17,614 - log
        - eval_epoch : 1 - 2025-05-28 12:27:17,614 - log
        - device : cuda - 2025-05-28 12:27:17,614 - log
==================================================================================================== - 2025-05-28 12:27:17,614 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:27:17,614 - log
loading model pretrained weight. - 2025-05-28 12:27:17,709 - log
set max_step: 2105 - 2025-05-28 12:27:18,590 - log
start to train the model................ 1 - 2025-05-28 12:27:18,590 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.48 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:27:21,938 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.03 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:27:24,941 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.29 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:27:27,970 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.45 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:27:31,016 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:27:31,638 - log
start to train the model................ 2 - 2025-05-28 12:27:33,360 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.54 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:27:35,814 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 30.06 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:27:38,821 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 30.18 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:27:41,839 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 30.14 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:27:44,854 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:27:46,096 - log
start to train the model................ 3 - 2025-05-28 12:27:47,882 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 18.06 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:27:49,688 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.47 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:27:52,735 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:27:52,736 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.38 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:27:55,774 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.32 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:27:58,805 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:28:00,704 - log
start to train the model................ 4 - 2025-05-28 12:28:02,420 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.70 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:28:03,589 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.37 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:28:06,627 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.15 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:28:09,642 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.17 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:28:12,660 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:28:15,169 - log
start to train the model................ 5 - 2025-05-28 12:28:16,943 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.23 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:28:17,466 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.37 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:28:20,503 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.11 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:28:23,515 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 29.86 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:28:26,501 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:28:26,502 - log
==================================================================================================== - 2025-05-28 12:37:14,242 - log
        - random_seed : 2025 - 2025-05-28 12:37:14,242 - log
        - lr : 0.0002 - 2025-05-28 12:37:14,242 - log
        - weight_decay : 0.01 - 2025-05-28 12:37:14,242 - log
        - correct_bias : False - 2025-05-28 12:37:14,242 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:37:14,242 - log
        - no_decay_bias : False - 2025-05-28 12:37:14,242 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:37:14,242 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:37:14,242 - log
        - scheduler : linear - 2025-05-28 12:37:14,242 - log
        - max_step : None - 2025-05-28 12:37:14,242 - log
        - max_epoch : 5 - 2025-05-28 12:37:14,242 - log
        - warmup_step : 500 - 2025-05-28 12:37:14,242 - log
        - i_steps : 0 - 2025-05-28 12:37:14,242 - log
        - i_lrs : 0.00025 - 2025-05-28 12:37:14,242 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:37:14,242 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:37:14,242 - log
        - train_batch_size : 2 - 2025-05-28 12:37:14,242 - log
        - valid_batch_size : 1 - 2025-05-28 12:37:14,242 - log
        - grad_acc : 2 - 2025-05-28 12:37:14,242 - log
        - clip : 0.0 - 2025-05-28 12:37:14,242 - log
        - seq_len : 64 - 2025-05-28 12:37:14,242 - log
        - model_card : gpt2.sm - 2025-05-28 12:37:14,243 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:37:14,243 - log
        - fp16 : False - 2025-05-28 12:37:14,243 - log
        - log_interval : 100 - 2025-05-28 12:37:14,243 - log
        - eval_interval : 2000 - 2025-05-28 12:37:14,243 - log
        - save_interval : 1000 - 2025-05-28 12:37:14,243 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:37:14,243 - log
        - lora_dim : 4 - 2025-05-28 12:37:14,243 - log
        - lora_alpha : 32 - 2025-05-28 12:37:14,243 - log
        - obj : clm - 2025-05-28 12:37:14,243 - log
        - lora_dropout : 0.1 - 2025-05-28 12:37:14,243 - log
        - label_smooth : 0.1 - 2025-05-28 12:37:14,243 - log
        - roll_interval : -1 - 2025-05-28 12:37:14,243 - log
        - roll_lr : 1e-05 - 2025-05-28 12:37:14,243 - log
        - roll_step : 100 - 2025-05-28 12:37:14,243 - log
        - eval_epoch : 1 - 2025-05-28 12:37:14,243 - log
        - device : cuda - 2025-05-28 12:37:14,243 - log
==================================================================================================== - 2025-05-28 12:37:14,243 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:37:14,243 - log
loading model pretrained weight. - 2025-05-28 12:37:14,344 - log
set max_step: 2105 - 2025-05-28 12:37:15,290 - log
start to train the model................ 1 - 2025-05-28 12:37:15,290 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.53 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:37:18,644 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.05 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:37:21,649 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.73 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:37:24,623 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.84 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:37:27,607 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:37:28,220 - log
start to train the model................ 2 - 2025-05-28 12:37:30,003 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.26 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:37:32,429 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 29.82 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:37:35,411 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 29.74 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:37:38,386 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 29.73 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:37:41,359 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:37:42,584 - log
start to train the model................ 3 - 2025-05-28 12:37:44,417 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.66 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:37:46,183 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.15 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:37:49,199 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:37:49,199 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.29 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:37:52,228 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.13 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:37:55,241 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:37:57,118 - log
start to train the model................ 4 - 2025-05-28 12:37:58,818 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.60 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:37:59,979 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 29.74 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:38:02,953 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 29.82 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:38:05,935 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 29.58 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:38:08,893 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:38:11,353 - log
start to train the model................ 5 - 2025-05-28 12:38:13,077 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.05 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:38:13,582 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 29.61 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:38:16,542 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 29.69 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:38:19,512 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 29.78 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:38:22,490 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:38:22,490 - log
==================================================================================================== - 2025-05-28 12:41:59,861 - log
        - random_seed : 2025 - 2025-05-28 12:41:59,861 - log
        - lr : 0.0002 - 2025-05-28 12:41:59,861 - log
        - weight_decay : 0.01 - 2025-05-28 12:41:59,861 - log
        - correct_bias : False - 2025-05-28 12:41:59,861 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:41:59,861 - log
        - no_decay_bias : False - 2025-05-28 12:41:59,861 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:41:59,861 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:41:59,861 - log
        - scheduler : linear - 2025-05-28 12:41:59,861 - log
        - max_step : None - 2025-05-28 12:41:59,861 - log
        - max_epoch : 5 - 2025-05-28 12:41:59,861 - log
        - warmup_step : 500 - 2025-05-28 12:41:59,861 - log
        - i_steps : 0 - 2025-05-28 12:41:59,861 - log
        - i_lrs : 0.00025 - 2025-05-28 12:41:59,861 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:41:59,861 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:41:59,861 - log
        - train_batch_size : 2 - 2025-05-28 12:41:59,861 - log
        - valid_batch_size : 1 - 2025-05-28 12:41:59,861 - log
        - grad_acc : 2 - 2025-05-28 12:41:59,862 - log
        - clip : 0.0 - 2025-05-28 12:41:59,862 - log
        - seq_len : 64 - 2025-05-28 12:41:59,862 - log
        - model_card : gpt2.sm - 2025-05-28 12:41:59,862 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:41:59,862 - log
        - fp16 : False - 2025-05-28 12:41:59,862 - log
        - log_interval : 100 - 2025-05-28 12:41:59,862 - log
        - eval_interval : 2000 - 2025-05-28 12:41:59,862 - log
        - save_interval : 1000 - 2025-05-28 12:41:59,862 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:41:59,862 - log
        - lora_dim : 4 - 2025-05-28 12:41:59,862 - log
        - lora_alpha : 32 - 2025-05-28 12:41:59,862 - log
        - obj : clm - 2025-05-28 12:41:59,862 - log
        - lora_dropout : 0.1 - 2025-05-28 12:41:59,862 - log
        - label_smooth : 0.1 - 2025-05-28 12:41:59,862 - log
        - roll_interval : -1 - 2025-05-28 12:41:59,862 - log
        - roll_lr : 1e-05 - 2025-05-28 12:41:59,862 - log
        - roll_step : 100 - 2025-05-28 12:41:59,862 - log
        - eval_epoch : 1 - 2025-05-28 12:41:59,862 - log
        - device : cuda - 2025-05-28 12:41:59,862 - log
==================================================================================================== - 2025-05-28 12:41:59,862 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:41:59,862 - log
loading model pretrained weight. - 2025-05-28 12:41:59,963 - log
set max_step: 2105 - 2025-05-28 12:42:00,937 - log
start to train the model................ 1 - 2025-05-28 12:42:00,938 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.54 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:42:04,392 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.31 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:42:07,424 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.34 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:42:10,459 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.35 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:42:13,495 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:42:14,141 - log
start to train the model................ 2 - 2025-05-28 12:42:15,889 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.31 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:42:18,320 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 30.15 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:42:21,335 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 30.41 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:42:24,377 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 30.31 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:42:27,409 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:42:28,650 - log
start to train the model................ 3 - 2025-05-28 12:42:30,422 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.98 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:42:32,221 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.55 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:42:35,276 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:42:35,276 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.22 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:42:38,298 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.29 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:42:41,328 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:42:43,200 - log
start to train the model................ 4 - 2025-05-28 12:42:44,915 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.75 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:42:46,091 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.09 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:42:49,100 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 29.97 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:42:52,097 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.24 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:42:55,121 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:42:57,609 - log
start to train the model................ 5 - 2025-05-28 12:42:59,358 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.12 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:42:59,870 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 29.73 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:43:02,843 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 29.74 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:43:05,818 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.03 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:43:08,821 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:43:08,821 - log
==================================================================================================== - 2025-05-28 12:49:56,083 - log
        - random_seed : 2025 - 2025-05-28 12:49:56,083 - log
        - lr : 0.0002 - 2025-05-28 12:49:56,083 - log
        - weight_decay : 0.01 - 2025-05-28 12:49:56,083 - log
        - correct_bias : False - 2025-05-28 12:49:56,083 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:49:56,083 - log
        - no_decay_bias : False - 2025-05-28 12:49:56,083 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:49:56,083 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:49:56,083 - log
        - scheduler : linear - 2025-05-28 12:49:56,083 - log
        - max_step : None - 2025-05-28 12:49:56,083 - log
        - max_epoch : 5 - 2025-05-28 12:49:56,083 - log
        - warmup_step : 500 - 2025-05-28 12:49:56,083 - log
        - i_steps : 0 - 2025-05-28 12:49:56,083 - log
        - i_lrs : 0.00025 - 2025-05-28 12:49:56,083 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:49:56,083 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:49:56,083 - log
        - train_batch_size : 2 - 2025-05-28 12:49:56,083 - log
        - valid_batch_size : 1 - 2025-05-28 12:49:56,083 - log
        - grad_acc : 2 - 2025-05-28 12:49:56,083 - log
        - clip : 0.0 - 2025-05-28 12:49:56,083 - log
        - seq_len : 64 - 2025-05-28 12:49:56,083 - log
        - model_card : gpt2.sm - 2025-05-28 12:49:56,083 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:49:56,083 - log
        - fp16 : False - 2025-05-28 12:49:56,083 - log
        - log_interval : 100 - 2025-05-28 12:49:56,083 - log
        - eval_interval : 2000 - 2025-05-28 12:49:56,083 - log
        - save_interval : 1000 - 2025-05-28 12:49:56,083 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:49:56,083 - log
        - lora_dim : 4 - 2025-05-28 12:49:56,083 - log
        - lora_alpha : 32 - 2025-05-28 12:49:56,083 - log
        - obj : clm - 2025-05-28 12:49:56,083 - log
        - lora_dropout : 0.1 - 2025-05-28 12:49:56,083 - log
        - label_smooth : 0.1 - 2025-05-28 12:49:56,084 - log
        - roll_interval : -1 - 2025-05-28 12:49:56,084 - log
        - roll_lr : 1e-05 - 2025-05-28 12:49:56,084 - log
        - roll_step : 100 - 2025-05-28 12:49:56,084 - log
        - eval_epoch : 1 - 2025-05-28 12:49:56,084 - log
        - device : cuda - 2025-05-28 12:49:56,084 - log
==================================================================================================== - 2025-05-28 12:49:56,084 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:49:56,084 - log
loading model pretrained weight. - 2025-05-28 12:49:56,178 - log
set max_step: 2105 - 2025-05-28 12:49:57,084 - log
start to train the model................ 1 - 2025-05-28 12:49:57,084 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.90 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:50:00,474 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.05 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:50:03,480 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.94 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:50:06,474 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.59 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:50:09,534 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:50:10,148 - log
start to train the model................ 2 - 2025-05-28 12:50:11,876 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.09 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:50:14,286 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 29.97 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:50:17,283 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 29.91 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:50:20,275 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 29.74 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:50:23,249 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:50:24,476 - log
start to train the model................ 3 - 2025-05-28 12:50:26,328 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.68 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:50:28,096 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 29.86 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:50:31,082 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:50:31,083 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.22 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:50:34,104 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.75 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:50:37,180 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:50:39,071 - log
start to train the model................ 4 - 2025-05-28 12:50:40,812 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 12.04 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:50:42,016 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.32 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:50:45,049 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.09 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:50:48,058 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.21 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:50:51,079 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:50:53,589 - log
start to train the model................ 5 - 2025-05-28 12:50:55,353 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.23 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:50:55,876 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 29.79 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:50:58,855 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 29.95 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:51:01,851 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 29.96 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:51:04,848 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:51:04,848 - log
==================================================================================================== - 2025-05-28 12:52:09,404 - log
        - random_seed : 2025 - 2025-05-28 12:52:09,404 - log
        - lr : 0.0002 - 2025-05-28 12:52:09,404 - log
        - weight_decay : 0.01 - 2025-05-28 12:52:09,404 - log
        - correct_bias : False - 2025-05-28 12:52:09,404 - log
        - adam_epislon : 1e-06 - 2025-05-28 12:52:09,404 - log
        - no_decay_bias : False - 2025-05-28 12:52:09,404 - log
        - adam_beta1 : 0.9 - 2025-05-28 12:52:09,404 - log
        - adam_beta2 : 0.999 - 2025-05-28 12:52:09,404 - log
        - scheduler : linear - 2025-05-28 12:52:09,404 - log
        - max_step : None - 2025-05-28 12:52:09,404 - log
        - max_epoch : 5 - 2025-05-28 12:52:09,404 - log
        - warmup_step : 500 - 2025-05-28 12:52:09,404 - log
        - i_steps : 0 - 2025-05-28 12:52:09,404 - log
        - i_lrs : 0.00025 - 2025-05-28 12:52:09,404 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 12:52:09,404 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 12:52:09,404 - log
        - train_batch_size : 2 - 2025-05-28 12:52:09,404 - log
        - valid_batch_size : 1 - 2025-05-28 12:52:09,404 - log
        - grad_acc : 2 - 2025-05-28 12:52:09,404 - log
        - clip : 0.0 - 2025-05-28 12:52:09,404 - log
        - seq_len : 64 - 2025-05-28 12:52:09,404 - log
        - model_card : gpt2.sm - 2025-05-28 12:52:09,404 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 12:52:09,404 - log
        - fp16 : False - 2025-05-28 12:52:09,404 - log
        - log_interval : 100 - 2025-05-28 12:52:09,404 - log
        - eval_interval : 2000 - 2025-05-28 12:52:09,404 - log
        - save_interval : 1000 - 2025-05-28 12:52:09,404 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 12:52:09,404 - log
        - lora_dim : 4 - 2025-05-28 12:52:09,404 - log
        - lora_alpha : 32 - 2025-05-28 12:52:09,404 - log
        - obj : clm - 2025-05-28 12:52:09,405 - log
        - lora_dropout : 0.1 - 2025-05-28 12:52:09,405 - log
        - label_smooth : 0.1 - 2025-05-28 12:52:09,405 - log
        - roll_interval : -1 - 2025-05-28 12:52:09,405 - log
        - roll_lr : 1e-05 - 2025-05-28 12:52:09,405 - log
        - roll_step : 100 - 2025-05-28 12:52:09,405 - log
        - eval_epoch : 1 - 2025-05-28 12:52:09,405 - log
        - device : cuda - 2025-05-28 12:52:09,405 - log
==================================================================================================== - 2025-05-28 12:52:09,405 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 12:52:09,405 - log
loading model pretrained weight. - 2025-05-28 12:52:09,501 - log
set max_step: 2105 - 2025-05-28 12:52:10,441 - log
start to train the model................ 1 - 2025-05-28 12:52:10,441 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.36 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 12:52:13,877 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.45 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 12:52:16,923 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.54 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 12:52:19,977 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.58 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 12:52:23,035 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 12:52:23,659 - log
start to train the model................ 2 - 2025-05-28 12:52:25,266 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.10 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 12:52:27,676 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 30.02 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 12:52:30,679 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 29.87 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 12:52:33,666 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 29.99 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 12:52:36,666 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 12:52:37,894 - log
start to train the model................ 3 - 2025-05-28 12:52:39,769 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.61 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 12:52:41,530 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.26 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 12:52:44,556 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 12:52:44,556 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 29.89 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 12:52:47,546 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 29.64 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 12:52:50,510 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 12:52:52,364 - log
start to train the model................ 4 - 2025-05-28 12:52:54,096 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.70 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 12:52:55,266 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.07 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 12:52:58,273 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.15 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 12:53:01,288 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.34 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 12:53:04,323 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 12:53:06,848 - log
start to train the model................ 5 - 2025-05-28 12:53:08,607 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.12 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 12:53:09,119 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.42 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 12:53:12,162 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.59 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 12:53:15,221 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.13 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 12:53:18,234 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 12:53:18,234 - log
==================================================================================================== - 2025-05-28 13:05:04,527 - log
        - random_seed : 2025 - 2025-05-28 13:05:04,527 - log
        - lr : 0.0002 - 2025-05-28 13:05:04,527 - log
        - weight_decay : 0.01 - 2025-05-28 13:05:04,527 - log
        - correct_bias : False - 2025-05-28 13:05:04,527 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:05:04,527 - log
        - no_decay_bias : False - 2025-05-28 13:05:04,527 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:05:04,527 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:05:04,527 - log
        - scheduler : linear - 2025-05-28 13:05:04,527 - log
        - max_step : None - 2025-05-28 13:05:04,527 - log
        - max_epoch : 5 - 2025-05-28 13:05:04,527 - log
        - warmup_step : 500 - 2025-05-28 13:05:04,527 - log
        - i_steps : 0 - 2025-05-28 13:05:04,527 - log
        - i_lrs : 0.00025 - 2025-05-28 13:05:04,527 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:05:04,527 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:05:04,527 - log
        - train_batch_size : 2 - 2025-05-28 13:05:04,527 - log
        - valid_batch_size : 1 - 2025-05-28 13:05:04,527 - log
        - grad_acc : 2 - 2025-05-28 13:05:04,527 - log
        - clip : 0.0 - 2025-05-28 13:05:04,527 - log
        - seq_len : 64 - 2025-05-28 13:05:04,527 - log
        - model_card : gpt2.sm - 2025-05-28 13:05:04,527 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:05:04,527 - log
        - fp16 : False - 2025-05-28 13:05:04,527 - log
        - log_interval : 100 - 2025-05-28 13:05:04,527 - log
        - eval_interval : 2000 - 2025-05-28 13:05:04,527 - log
        - save_interval : 1000 - 2025-05-28 13:05:04,527 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:05:04,527 - log
        - lora_dim : 4 - 2025-05-28 13:05:04,527 - log
        - lora_alpha : 32 - 2025-05-28 13:05:04,527 - log
        - obj : clm - 2025-05-28 13:05:04,527 - log
        - lora_dropout : 0.1 - 2025-05-28 13:05:04,527 - log
        - label_smooth : 0.1 - 2025-05-28 13:05:04,527 - log
        - roll_interval : -1 - 2025-05-28 13:05:04,527 - log
        - roll_lr : 1e-05 - 2025-05-28 13:05:04,527 - log
        - roll_step : 100 - 2025-05-28 13:05:04,528 - log
        - eval_epoch : 1 - 2025-05-28 13:05:04,528 - log
        - device : cuda - 2025-05-28 13:05:04,528 - log
==================================================================================================== - 2025-05-28 13:05:04,528 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:05:04,528 - log
loading model pretrained weight. - 2025-05-28 13:05:04,622 - log
set max_step: 2105 - 2025-05-28 13:05:05,626 - log
start to train the model................ 1 - 2025-05-28 13:05:05,626 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.51 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 13:05:08,978 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.71 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 13:05:11,949 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.61 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 13:05:14,911 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.15 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 13:05:17,926 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 13:05:18,538 - log
start to train the model................ 2 - 2025-05-28 13:05:20,162 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.23 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 13:05:22,586 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 29.97 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 13:05:25,583 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 29.94 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 13:05:28,577 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 30.48 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 13:05:31,625 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 13:05:32,883 - log
start to train the model................ 3 - 2025-05-28 13:05:34,725 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.93 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 13:05:36,519 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 30.46 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 13:05:39,565 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:05:39,566 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 30.39 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 13:05:42,605 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 30.27 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 13:05:45,633 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 13:05:47,530 - log
start to train the model................ 4 - 2025-05-28 13:05:49,247 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.84 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 13:05:50,431 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.32 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 13:05:53,463 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.27 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 13:05:56,490 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.21 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 13:05:59,512 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 13:06:02,051 - log
start to train the model................ 5 - 2025-05-28 13:06:03,780 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.12 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:06:04,293 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.25 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 13:06:07,318 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.05 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 13:06:10,324 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.10 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:06:13,334 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:06:13,334 - log
==================================================================================================== - 2025-05-28 13:07:57,251 - log
        - random_seed : 2025 - 2025-05-28 13:07:57,251 - log
        - lr : 0.0002 - 2025-05-28 13:07:57,251 - log
        - weight_decay : 0.01 - 2025-05-28 13:07:57,251 - log
        - correct_bias : False - 2025-05-28 13:07:57,251 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:07:57,251 - log
        - no_decay_bias : False - 2025-05-28 13:07:57,251 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:07:57,251 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:07:57,251 - log
        - scheduler : linear - 2025-05-28 13:07:57,251 - log
        - max_step : None - 2025-05-28 13:07:57,251 - log
        - max_epoch : 5 - 2025-05-28 13:07:57,251 - log
        - warmup_step : 500 - 2025-05-28 13:07:57,251 - log
        - i_steps : 0 - 2025-05-28 13:07:57,251 - log
        - i_lrs : 0.00025 - 2025-05-28 13:07:57,251 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:07:57,251 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:07:57,252 - log
        - train_batch_size : 2 - 2025-05-28 13:07:57,252 - log
        - valid_batch_size : 1 - 2025-05-28 13:07:57,252 - log
        - grad_acc : 2 - 2025-05-28 13:07:57,252 - log
        - clip : 0.0 - 2025-05-28 13:07:57,252 - log
        - seq_len : 64 - 2025-05-28 13:07:57,252 - log
        - model_card : gpt2.sm - 2025-05-28 13:07:57,252 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:07:57,252 - log
        - fp16 : False - 2025-05-28 13:07:57,252 - log
        - log_interval : 100 - 2025-05-28 13:07:57,252 - log
        - eval_interval : 2000 - 2025-05-28 13:07:57,252 - log
        - save_interval : 1000 - 2025-05-28 13:07:57,252 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:07:57,252 - log
        - lora_dim : 4 - 2025-05-28 13:07:57,252 - log
        - lora_alpha : 32 - 2025-05-28 13:07:57,252 - log
        - obj : clm - 2025-05-28 13:07:57,252 - log
        - lora_dropout : 0.1 - 2025-05-28 13:07:57,252 - log
        - label_smooth : 0.1 - 2025-05-28 13:07:57,252 - log
        - roll_interval : -1 - 2025-05-28 13:07:57,252 - log
        - roll_lr : 1e-05 - 2025-05-28 13:07:57,252 - log
        - roll_step : 100 - 2025-05-28 13:07:57,252 - log
        - eval_epoch : 1 - 2025-05-28 13:07:57,252 - log
        - device : cuda - 2025-05-28 13:07:57,252 - log
==================================================================================================== - 2025-05-28 13:07:57,252 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:07:57,252 - log
loading model pretrained weight. - 2025-05-28 13:07:57,350 - log
set max_step: 2105 - 2025-05-28 13:07:58,296 - log
start to train the model................ 1 - 2025-05-28 13:07:58,296 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.36 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 13:08:01,632 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.98 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 13:08:04,630 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.10 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 13:08:07,641 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.04 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 13:08:10,644 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 13:08:11,259 - log
start to train the model................ 2 - 2025-05-28 13:08:12,868 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.08 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 13:08:15,276 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 29.90 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 13:08:18,267 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 29.94 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 13:08:21,262 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 29.80 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 13:08:24,243 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 13:08:25,514 - log
start to train the model................ 3 - 2025-05-28 13:08:27,378 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.73 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 13:08:29,151 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 29.86 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 13:08:32,137 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:08:32,138 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 29.75 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 13:08:35,113 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 29.66 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 13:08:38,079 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 13:08:39,935 - log
start to train the model................ 4 - 2025-05-28 13:08:41,763 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.56 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 13:08:42,919 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 29.60 | loss  2.27 | avg loss  3.03 | ppl 20.77 - 2025-05-28 13:08:45,880 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 29.57 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 13:08:48,837 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 29.55 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 13:08:51,792 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 13:08:54,247 - log
start to train the model................ 5 - 2025-05-28 13:08:56,092 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.21 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:08:56,613 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.01 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 13:08:59,615 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.00 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 13:09:02,615 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 29.81 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:09:05,596 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:09:05,597 - log
==================================================================================================== - 2025-05-28 13:15:20,507 - log
        - random_seed : 2025 - 2025-05-28 13:15:20,507 - log
        - lr : 0.0002 - 2025-05-28 13:15:20,507 - log
        - weight_decay : 0.01 - 2025-05-28 13:15:20,507 - log
        - correct_bias : False - 2025-05-28 13:15:20,507 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:15:20,507 - log
        - no_decay_bias : False - 2025-05-28 13:15:20,507 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:15:20,507 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:15:20,507 - log
        - scheduler : linear - 2025-05-28 13:15:20,507 - log
        - max_step : None - 2025-05-28 13:15:20,507 - log
        - max_epoch : 5 - 2025-05-28 13:15:20,507 - log
        - warmup_step : 500 - 2025-05-28 13:15:20,507 - log
        - i_steps : 0 - 2025-05-28 13:15:20,507 - log
        - i_lrs : 0.00025 - 2025-05-28 13:15:20,507 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:15:20,507 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:15:20,507 - log
        - train_batch_size : 2 - 2025-05-28 13:15:20,507 - log
        - valid_batch_size : 1 - 2025-05-28 13:15:20,507 - log
        - grad_acc : 2 - 2025-05-28 13:15:20,507 - log
        - clip : 0.0 - 2025-05-28 13:15:20,507 - log
        - seq_len : 64 - 2025-05-28 13:15:20,507 - log
        - model_card : gpt2.sm - 2025-05-28 13:15:20,507 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:15:20,508 - log
        - fp16 : False - 2025-05-28 13:15:20,508 - log
        - log_interval : 100 - 2025-05-28 13:15:20,508 - log
        - eval_interval : 2000 - 2025-05-28 13:15:20,508 - log
        - save_interval : 1000 - 2025-05-28 13:15:20,508 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:15:20,508 - log
        - lora_dim : 4 - 2025-05-28 13:15:20,508 - log
        - lora_alpha : 32 - 2025-05-28 13:15:20,508 - log
        - obj : clm - 2025-05-28 13:15:20,508 - log
        - lora_dropout : 0.1 - 2025-05-28 13:15:20,508 - log
        - label_smooth : 0.1 - 2025-05-28 13:15:20,508 - log
        - roll_interval : -1 - 2025-05-28 13:15:20,508 - log
        - roll_lr : 1e-05 - 2025-05-28 13:15:20,508 - log
        - roll_step : 100 - 2025-05-28 13:15:20,508 - log
        - eval_epoch : 1 - 2025-05-28 13:15:20,508 - log
        - device : cuda - 2025-05-28 13:15:20,508 - log
==================================================================================================== - 2025-05-28 13:15:20,508 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:15:20,508 - log
loading model pretrained weight. - 2025-05-28 13:15:20,611 - log
set max_step: 2105 - 2025-05-28 13:15:21,519 - log
start to train the model................ 1 - 2025-05-28 13:15:21,520 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.32 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 13:15:24,852 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.05 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 13:15:27,858 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.87 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 13:15:30,846 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.06 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 13:15:33,852 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 13:15:34,464 - log
start to train the model................ 2 - 2025-05-28 13:15:36,048 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 24.26 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 13:15:38,474 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 30.08 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 13:15:41,482 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 30.04 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 13:15:44,487 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 30.19 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 13:15:47,507 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 13:15:48,777 - log
start to train the model................ 3 - 2025-05-28 13:15:50,597 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.69 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 13:15:52,366 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 29.81 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 13:15:55,348 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:15:55,348 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 29.86 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 13:15:58,334 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 29.89 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 13:16:01,324 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 13:16:03,195 - log
start to train the model................ 4 - 2025-05-28 13:16:04,879 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.67 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 13:16:06,046 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 29.97 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 13:16:09,044 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 29.98 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 13:16:12,042 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 29.96 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 13:16:15,039 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 13:16:17,547 - log
start to train the model................ 5 - 2025-05-28 13:16:19,300 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.20 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:16:19,820 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.29 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 13:16:22,849 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.23 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 13:16:25,873 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.19 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:16:28,892 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:16:28,892 - log
==================================================================================================== - 2025-05-28 13:17:14,008 - log
        - random_seed : 2025 - 2025-05-28 13:17:14,008 - log
        - lr : 0.0002 - 2025-05-28 13:17:14,008 - log
        - weight_decay : 0.01 - 2025-05-28 13:17:14,008 - log
        - correct_bias : False - 2025-05-28 13:17:14,008 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:17:14,008 - log
        - no_decay_bias : False - 2025-05-28 13:17:14,008 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:17:14,009 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:17:14,009 - log
        - scheduler : linear - 2025-05-28 13:17:14,009 - log
        - max_step : None - 2025-05-28 13:17:14,009 - log
        - max_epoch : 5 - 2025-05-28 13:17:14,009 - log
        - warmup_step : 500 - 2025-05-28 13:17:14,009 - log
        - i_steps : 0 - 2025-05-28 13:17:14,009 - log
        - i_lrs : 0.00025 - 2025-05-28 13:17:14,009 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:17:14,009 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:17:14,009 - log
        - train_batch_size : 2 - 2025-05-28 13:17:14,009 - log
        - valid_batch_size : 1 - 2025-05-28 13:17:14,009 - log
        - grad_acc : 2 - 2025-05-28 13:17:14,009 - log
        - clip : 0.0 - 2025-05-28 13:17:14,009 - log
        - seq_len : 64 - 2025-05-28 13:17:14,009 - log
        - model_card : gpt2.sm - 2025-05-28 13:17:14,009 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:17:14,009 - log
        - fp16 : False - 2025-05-28 13:17:14,009 - log
        - log_interval : 100 - 2025-05-28 13:17:14,009 - log
        - eval_interval : 2000 - 2025-05-28 13:17:14,009 - log
        - save_interval : 1000 - 2025-05-28 13:17:14,009 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:17:14,009 - log
        - lora_dim : 4 - 2025-05-28 13:17:14,009 - log
        - lora_alpha : 32 - 2025-05-28 13:17:14,009 - log
        - obj : clm - 2025-05-28 13:17:14,009 - log
        - lora_dropout : 0.1 - 2025-05-28 13:17:14,009 - log
        - label_smooth : 0.1 - 2025-05-28 13:17:14,009 - log
        - roll_interval : -1 - 2025-05-28 13:17:14,009 - log
        - roll_lr : 1e-05 - 2025-05-28 13:17:14,009 - log
        - roll_step : 100 - 2025-05-28 13:17:14,009 - log
        - eval_epoch : 1 - 2025-05-28 13:17:14,009 - log
        - device : cuda - 2025-05-28 13:17:14,009 - log
==================================================================================================== - 2025-05-28 13:17:14,009 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:17:14,009 - log
loading model pretrained weight. - 2025-05-28 13:17:14,108 - log
set max_step: 2105 - 2025-05-28 13:17:15,023 - log
start to train the model................ 1 - 2025-05-28 13:17:15,023 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.53 | loss  6.42 | avg loss  5.98 | ppl 395.37 - 2025-05-28 13:17:18,476 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.65 | loss  4.00 | avg loss  4.87 | ppl 130.46 - 2025-05-28 13:17:21,441 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.72 | loss  3.55 | avg loss  3.60 | ppl 36.47 - 2025-05-28 13:17:24,414 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.90 | loss  3.51 | avg loss  3.45 | ppl 31.41 - 2025-05-28 13:17:27,404 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.421.pkl - 2025-05-28 13:17:28,015 - log
start to train the model................ 2 - 2025-05-28 13:17:29,703 - log
| epoch   2 step      500 |     79 batches | lr 0.0002 | ms/batch 23.98 | loss  3.70 | avg loss  3.18 | ppl 24.07 - 2025-05-28 13:17:32,101 - log
| epoch   2 step      600 |    179 batches | lr 0.000188 | ms/batch 29.76 | loss  2.82 | avg loss  3.26 | ppl 26.10 - 2025-05-28 13:17:35,077 - log
| epoch   2 step      700 |    279 batches | lr 0.000175 | ms/batch 30.00 | loss  2.89 | avg loss  3.20 | ppl 24.49 - 2025-05-28 13:17:38,078 - log
| epoch   2 step      800 |    379 batches | lr 0.000163 | ms/batch 29.49 | loss  3.62 | avg loss  3.08 | ppl 21.73 - 2025-05-28 13:17:41,027 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.842.pkl - 2025-05-28 13:17:42,245 - log
start to train the model................ 3 - 2025-05-28 13:17:44,097 - log
| epoch   3 step      900 |     58 batches | lr 0.00015 | ms/batch 17.56 | loss  2.54 | avg loss  3.01 | ppl 20.20 - 2025-05-28 13:17:45,853 - log
| epoch   3 step     1000 |    158 batches | lr 0.000138 | ms/batch 29.71 | loss  3.04 | avg loss  2.99 | ppl 19.86 - 2025-05-28 13:17:48,824 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:17:48,825 - log
| epoch   3 step     1100 |    258 batches | lr 0.000125 | ms/batch 29.76 | loss  2.81 | avg loss  3.09 | ppl 21.87 - 2025-05-28 13:17:51,801 - log
| epoch   3 step     1200 |    358 batches | lr 0.000113 | ms/batch 29.77 | loss  2.98 | avg loss  3.12 | ppl 22.72 - 2025-05-28 13:17:54,778 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1263.pkl - 2025-05-28 13:17:56,667 - log
start to train the model................ 4 - 2025-05-28 13:17:58,433 - log
| epoch   4 step     1300 |     37 batches | lr 0.0001 | ms/batch 11.64 | loss  2.65 | avg loss  2.91 | ppl 18.36 - 2025-05-28 13:17:59,598 - log
| epoch   4 step     1400 |    137 batches | lr 8.79e-05 | ms/batch 30.05 | loss  2.27 | avg loss  3.03 | ppl 20.78 - 2025-05-28 13:18:02,602 - log
| epoch   4 step     1500 |    237 batches | lr 7.54e-05 | ms/batch 30.28 | loss  2.87 | avg loss  2.92 | ppl 18.59 - 2025-05-28 13:18:05,631 - log
| epoch   4 step     1600 |    337 batches | lr 6.29e-05 | ms/batch 30.70 | loss  3.17 | avg loss  3.03 | ppl 20.62 - 2025-05-28 13:18:08,701 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1684.pkl - 2025-05-28 13:18:11,216 - log
start to train the model................ 5 - 2025-05-28 13:18:12,985 - log
| epoch   5 step     1700 |     16 batches | lr 5.05e-05 | ms/batch  5.24 | loss  3.18 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:18:13,509 - log
| epoch   5 step     1800 |    116 batches | lr 3.8e-05 | ms/batch 30.36 | loss  2.78 | avg loss  2.98 | ppl 19.60 - 2025-05-28 13:18:16,545 - log
| epoch   5 step     1900 |    216 batches | lr 2.55e-05 | ms/batch 30.11 | loss  2.54 | avg loss  2.93 | ppl 18.79 - 2025-05-28 13:18:19,557 - log
| epoch   5 step     2000 |    316 batches | lr 1.31e-05 | ms/batch 30.17 | loss  2.59 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:18:22,574 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:18:22,574 - log
eval samples: 0, loss: 1.6393531560897827 - 2025-05-28 13:18:25,531 - log
average loss: +1.6130049036395164 - 2025-05-28 13:18:27,006 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:18:27,006 - log
| Eval   1 at step     2000 | time:  4.43s | valid loss  1.61 | valid ppl  5.02 | best ppl  5.02  - 2025-05-28 13:18:27,006 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:18:27,006 - log
| epoch   5 step     2100 |    416 batches | lr 6.23e-07 | ms/batch 74.24 | loss  3.10 | avg loss  2.95 | ppl 19.05 - 2025-05-28 13:18:29,998 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2105.pkl - 2025-05-28 13:18:30,137 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:18:31,654 - log
End of training - 2025-05-28 13:18:31,654 - log
ms/batch 29.13 - 2025-05-28 13:18:31,654 - log
==================================================================================================== - 2025-05-28 13:22:45,004 - log
        - random_seed : 2025 - 2025-05-28 13:22:45,005 - log
        - lr : 0.0002 - 2025-05-28 13:22:45,005 - log
        - weight_decay : 0.01 - 2025-05-28 13:22:45,005 - log
        - correct_bias : False - 2025-05-28 13:22:45,005 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:22:45,005 - log
        - no_decay_bias : False - 2025-05-28 13:22:45,005 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:22:45,005 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:22:45,005 - log
        - scheduler : linear - 2025-05-28 13:22:45,005 - log
        - max_step : None - 2025-05-28 13:22:45,005 - log
        - max_epoch : 5 - 2025-05-28 13:22:45,005 - log
        - warmup_step : 500 - 2025-05-28 13:22:45,005 - log
        - i_steps : 0 - 2025-05-28 13:22:45,005 - log
        - i_lrs : 0.00025 - 2025-05-28 13:22:45,005 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:22:45,005 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:22:45,005 - log
        - train_batch_size : 2 - 2025-05-28 13:22:45,005 - log
        - valid_batch_size : 1 - 2025-05-28 13:22:45,005 - log
        - grad_acc : 2 - 2025-05-28 13:22:45,005 - log
        - clip : 0.0 - 2025-05-28 13:22:45,005 - log
        - seq_len : 64 - 2025-05-28 13:22:45,005 - log
        - model_card : gpt2.sm - 2025-05-28 13:22:45,005 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:22:45,005 - log
        - fp16 : False - 2025-05-28 13:22:45,005 - log
        - log_interval : 100 - 2025-05-28 13:22:45,005 - log
        - eval_interval : 2000 - 2025-05-28 13:22:45,005 - log
        - save_interval : 1000 - 2025-05-28 13:22:45,005 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:22:45,005 - log
        - lora_dim : 4 - 2025-05-28 13:22:45,005 - log
        - lora_alpha : 32 - 2025-05-28 13:22:45,005 - log
        - obj : clm - 2025-05-28 13:22:45,005 - log
        - lora_dropout : 0.1 - 2025-05-28 13:22:45,005 - log
        - label_smooth : 0.1 - 2025-05-28 13:22:45,005 - log
        - roll_interval : -1 - 2025-05-28 13:22:45,005 - log
        - roll_lr : 1e-05 - 2025-05-28 13:22:45,005 - log
        - roll_step : 100 - 2025-05-28 13:22:45,005 - log
        - eval_epoch : 1 - 2025-05-28 13:22:45,005 - log
        - device : cuda - 2025-05-28 13:22:45,005 - log
==================================================================================================== - 2025-05-28 13:22:45,005 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:22:45,006 - log
loading model pretrained weight. - 2025-05-28 13:22:45,116 - log
set max_step: 1050 - 2025-05-28 13:22:46,066 - log
start to train the model................ 1 - 2025-05-28 13:22:46,066 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.35 | loss  5.51 | avg loss  5.99 | ppl 398.27 - 2025-05-28 13:22:49,501 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.56 | loss  4.03 | avg loss  4.89 | ppl 133.61 - 2025-05-28 13:22:52,558 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.210.pkl - 2025-05-28 13:22:52,838 - log
start to train the model................ 2 - 2025-05-28 13:22:54,643 - log
| epoch   2 step      300 |     90 batches | lr 0.00012 | ms/batch 27.75 | loss  3.23 | avg loss  3.57 | ppl 35.69 - 2025-05-28 13:22:57,418 - log
| epoch   2 step      400 |    190 batches | lr 0.00016 | ms/batch 30.64 | loss  4.11 | avg loss  3.40 | ppl 30.03 - 2025-05-28 13:23:00,483 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.420.pkl - 2025-05-28 13:23:01,067 - log
start to train the model................ 3 - 2025-05-28 13:23:02,812 - log
| epoch   3 step      500 |     80 batches | lr 0.0002 | ms/batch 24.65 | loss  3.38 | avg loss  3.23 | ppl 25.37 - 2025-05-28 13:23:05,277 - log
| epoch   3 step      600 |    180 batches | lr 0.000164 | ms/batch 30.46 | loss  3.04 | avg loss  3.20 | ppl 24.49 - 2025-05-28 13:23:08,324 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.630.pkl - 2025-05-28 13:23:09,216 - log
start to train the model................ 4 - 2025-05-28 13:23:11,017 - log
| epoch   4 step      700 |     70 batches | lr 0.000127 | ms/batch 21.52 | loss  3.46 | avg loss  3.15 | ppl 23.22 - 2025-05-28 13:23:13,170 - log
| epoch   4 step      800 |    170 batches | lr 9.09e-05 | ms/batch 30.53 | loss  3.04 | avg loss  3.01 | ppl 20.38 - 2025-05-28 13:23:16,223 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.840.pkl - 2025-05-28 13:23:17,409 - log
start to train the model................ 5 - 2025-05-28 13:23:19,155 - log
| epoch   5 step      900 |     60 batches | lr 5.45e-05 | ms/batch 18.50 | loss  2.47 | avg loss  3.04 | ppl 20.95 - 2025-05-28 13:23:21,005 - log
| epoch   5 step     1000 |    160 batches | lr 1.82e-05 | ms/batch 29.91 | loss  3.83 | avg loss  3.01 | ppl 20.36 - 2025-05-28 13:23:23,996 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:23:23,997 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1050.pkl - 2025-05-28 13:23:25,476 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:23:27,290 - log
End of training - 2025-05-28 13:23:27,290 - log
ms/batch 27.89 - 2025-05-28 13:23:27,290 - log
==================================================================================================== - 2025-05-28 13:25:02,356 - log
        - random_seed : 2025 - 2025-05-28 13:25:02,357 - log
        - lr : 0.0002 - 2025-05-28 13:25:02,357 - log
        - weight_decay : 0.01 - 2025-05-28 13:25:02,357 - log
        - correct_bias : False - 2025-05-28 13:25:02,357 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:25:02,357 - log
        - no_decay_bias : False - 2025-05-28 13:25:02,357 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:25:02,357 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:25:02,357 - log
        - scheduler : linear - 2025-05-28 13:25:02,357 - log
        - max_step : None - 2025-05-28 13:25:02,357 - log
        - max_epoch : 5 - 2025-05-28 13:25:02,357 - log
        - warmup_step : 500 - 2025-05-28 13:25:02,357 - log
        - i_steps : 0 - 2025-05-28 13:25:02,357 - log
        - i_lrs : 0.00025 - 2025-05-28 13:25:02,357 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:25:02,357 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:25:02,357 - log
        - train_batch_size : 2 - 2025-05-28 13:25:02,357 - log
        - valid_batch_size : 1 - 2025-05-28 13:25:02,357 - log
        - grad_acc : 2 - 2025-05-28 13:25:02,357 - log
        - clip : 0.0 - 2025-05-28 13:25:02,357 - log
        - seq_len : 64 - 2025-05-28 13:25:02,357 - log
        - model_card : gpt2.sm - 2025-05-28 13:25:02,357 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:25:02,357 - log
        - fp16 : False - 2025-05-28 13:25:02,357 - log
        - log_interval : 100 - 2025-05-28 13:25:02,357 - log
        - eval_interval : 2000 - 2025-05-28 13:25:02,357 - log
        - save_interval : 1000 - 2025-05-28 13:25:02,357 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:25:02,357 - log
        - lora_dim : 4 - 2025-05-28 13:25:02,357 - log
        - lora_alpha : 32 - 2025-05-28 13:25:02,357 - log
        - obj : clm - 2025-05-28 13:25:02,357 - log
        - lora_dropout : 0.1 - 2025-05-28 13:25:02,357 - log
        - label_smooth : 0.1 - 2025-05-28 13:25:02,357 - log
        - roll_interval : -1 - 2025-05-28 13:25:02,357 - log
        - roll_lr : 1e-05 - 2025-05-28 13:25:02,358 - log
        - roll_step : 100 - 2025-05-28 13:25:02,358 - log
        - eval_epoch : 1 - 2025-05-28 13:25:02,358 - log
        - device : cuda - 2025-05-28 13:25:02,358 - log
==================================================================================================== - 2025-05-28 13:25:02,358 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:25:02,358 - log
loading model pretrained weight. - 2025-05-28 13:25:02,454 - log
==================================================================================================== - 2025-05-28 13:25:13,540 - log
        - random_seed : 2025 - 2025-05-28 13:25:13,540 - log
        - lr : 0.0002 - 2025-05-28 13:25:13,540 - log
        - weight_decay : 0.01 - 2025-05-28 13:25:13,540 - log
        - correct_bias : False - 2025-05-28 13:25:13,540 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:25:13,540 - log
        - no_decay_bias : False - 2025-05-28 13:25:13,540 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:25:13,540 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:25:13,540 - log
        - scheduler : linear - 2025-05-28 13:25:13,540 - log
        - max_step : None - 2025-05-28 13:25:13,540 - log
        - max_epoch : 5 - 2025-05-28 13:25:13,540 - log
        - warmup_step : 500 - 2025-05-28 13:25:13,540 - log
        - i_steps : 0 - 2025-05-28 13:25:13,540 - log
        - i_lrs : 0.00025 - 2025-05-28 13:25:13,540 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:25:13,540 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:25:13,540 - log
        - train_batch_size : 2 - 2025-05-28 13:25:13,540 - log
        - valid_batch_size : 1 - 2025-05-28 13:25:13,540 - log
        - grad_acc : 2 - 2025-05-28 13:25:13,540 - log
        - clip : 0.0 - 2025-05-28 13:25:13,540 - log
        - seq_len : 64 - 2025-05-28 13:25:13,540 - log
        - model_card : gpt2.sm - 2025-05-28 13:25:13,540 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:25:13,541 - log
        - fp16 : False - 2025-05-28 13:25:13,541 - log
        - log_interval : 100 - 2025-05-28 13:25:13,541 - log
        - eval_interval : 2000 - 2025-05-28 13:25:13,541 - log
        - save_interval : 1000 - 2025-05-28 13:25:13,541 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:25:13,541 - log
        - lora_dim : 4 - 2025-05-28 13:25:13,541 - log
        - lora_alpha : 32 - 2025-05-28 13:25:13,541 - log
        - obj : clm - 2025-05-28 13:25:13,541 - log
        - lora_dropout : 0.1 - 2025-05-28 13:25:13,541 - log
        - label_smooth : 0.1 - 2025-05-28 13:25:13,541 - log
        - roll_interval : -1 - 2025-05-28 13:25:13,541 - log
        - roll_lr : 1e-05 - 2025-05-28 13:25:13,541 - log
        - roll_step : 100 - 2025-05-28 13:25:13,541 - log
        - eval_epoch : 1 - 2025-05-28 13:25:13,541 - log
        - device : cuda - 2025-05-28 13:25:13,541 - log
==================================================================================================== - 2025-05-28 13:25:13,541 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:25:13,541 - log
loading model pretrained weight. - 2025-05-28 13:25:13,664 - log
set max_step: 10515 - 2025-05-28 13:25:14,593 - log
start to train the model................ 1 - 2025-05-28 13:25:14,593 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.79 | loss  5.81 | avg loss  5.99 | ppl 398.74 - 2025-05-28 13:25:17,972 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.33 | loss  4.26 | avg loss  4.80 | ppl 121.75 - 2025-05-28 13:25:21,005 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.29 | loss  3.10 | avg loss  3.60 | ppl 36.58 - 2025-05-28 13:25:24,035 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.18 | loss  3.15 | avg loss  3.31 | ppl 27.43 - 2025-05-28 13:25:27,053 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.90 | loss  3.23 | avg loss  3.21 | ppl 24.89 - 2025-05-28 13:25:30,043 - log
| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 29.84 | loss  3.22 | avg loss  3.11 | ppl 22.31 - 2025-05-28 13:25:33,028 - log
| epoch   1 step      700 |    700 batches | lr 0.000196 | ms/batch 30.11 | loss  3.16 | avg loss  3.18 | ppl 24.02 - 2025-05-28 13:25:36,039 - log
| epoch   1 step      800 |    800 batches | lr 0.000194 | ms/batch 30.11 | loss  4.05 | avg loss  3.09 | ppl 21.98 - 2025-05-28 13:25:39,050 - log
| epoch   1 step      900 |    900 batches | lr 0.000192 | ms/batch 29.80 | loss  3.21 | avg loss  3.11 | ppl 22.39 - 2025-05-28 13:25:42,030 - log
| epoch   1 step     1000 |   1000 batches | lr 0.00019 | ms/batch 29.82 | loss  3.30 | avg loss  3.13 | ppl 22.98 - 2025-05-28 13:25:45,012 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:25:45,013 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000188 | ms/batch 29.92 | loss  3.24 | avg loss  3.04 | ppl 20.94 - 2025-05-28 13:25:48,005 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000186 | ms/batch 29.97 | loss  3.06 | avg loss  2.96 | ppl 19.38 - 2025-05-28 13:25:51,002 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000184 | ms/batch 29.98 | loss  2.99 | avg loss  3.01 | ppl 20.38 - 2025-05-28 13:25:54,000 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000182 | ms/batch 29.88 | loss  2.65 | avg loss  2.95 | ppl 19.15 - 2025-05-28 13:25:56,989 - log
| epoch   1 step     1500 |   1500 batches | lr 0.00018 | ms/batch 29.89 | loss  2.33 | avg loss  2.92 | ppl 18.60 - 2025-05-28 13:25:59,978 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000178 | ms/batch 29.78 | loss  2.48 | avg loss  2.94 | ppl 18.92 - 2025-05-28 13:26:02,956 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000176 | ms/batch 29.70 | loss  2.56 | avg loss  2.92 | ppl 18.49 - 2025-05-28 13:26:05,927 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000174 | ms/batch 30.18 | loss  2.48 | avg loss  2.98 | ppl 19.66 - 2025-05-28 13:26:08,945 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000172 | ms/batch 29.94 | loss  2.42 | avg loss  3.04 | ppl 20.81 - 2025-05-28 13:26:11,940 - log
| epoch   1 step     2000 |   2000 batches | lr 0.00017 | ms/batch 29.91 | loss  3.00 | avg loss  3.00 | ppl 20.16 - 2025-05-28 13:26:14,931 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:26:14,931 - log
eval samples: 0, loss: 1.3806864023208618 - 2025-05-28 13:26:14,952 - log
eval samples: 100, loss: 1.6813805103302002 - 2025-05-28 13:26:16,463 - log
eval samples: 200, loss: 1.1190105676651 - 2025-05-28 13:26:17,988 - log
eval samples: 300, loss: 1.9420855045318604 - 2025-05-28 13:26:19,496 - log
eval samples: 400, loss: 1.340158224105835 - 2025-05-28 13:26:21,018 - log
average loss: +1.5682519999765483 - 2025-05-28 13:26:22,022 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:26:22,022 - log
| Eval   1 at step     2000 | time:  7.09s | valid loss  1.57 | valid ppl  4.80 | best ppl  4.80  - 2025-05-28 13:26:22,022 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:26:22,022 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000168 | ms/batch 101.05 | loss  3.14 | avg loss  2.94 | ppl 18.91 - 2025-05-28 13:26:25,037 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2103.pkl - 2025-05-28 13:26:25,118 - log
start to train the model................ 2 - 2025-05-28 13:26:26,674 - log
| epoch   2 step     2200 |     97 batches | lr 0.000166 | ms/batch 29.73 | loss  2.42 | avg loss  2.86 | ppl 17.40 - 2025-05-28 13:26:29,648 - log
| epoch   2 step     2300 |    197 batches | lr 0.000164 | ms/batch 30.25 | loss  2.66 | avg loss  2.93 | ppl 18.64 - 2025-05-28 13:26:32,673 - log
| epoch   2 step     2400 |    297 batches | lr 0.000162 | ms/batch 30.21 | loss  2.98 | avg loss  2.97 | ppl 19.57 - 2025-05-28 13:26:35,694 - log
| epoch   2 step     2500 |    397 batches | lr 0.00016 | ms/batch 30.01 | loss  2.42 | avg loss  2.90 | ppl 18.16 - 2025-05-28 13:26:38,696 - log
| epoch   2 step     2600 |    497 batches | lr 0.000158 | ms/batch 30.13 | loss  2.51 | avg loss  2.87 | ppl 17.61 - 2025-05-28 13:26:41,709 - log
| epoch   2 step     2700 |    597 batches | lr 0.000156 | ms/batch 30.01 | loss  2.44 | avg loss  2.90 | ppl 18.23 - 2025-05-28 13:26:44,710 - log
| epoch   2 step     2800 |    697 batches | lr 0.000154 | ms/batch 30.05 | loss  2.95 | avg loss  2.86 | ppl 17.43 - 2025-05-28 13:26:47,716 - log
| epoch   2 step     2900 |    797 batches | lr 0.000152 | ms/batch 29.86 | loss  2.85 | avg loss  2.87 | ppl 17.57 - 2025-05-28 13:26:50,702 - log
| epoch   2 step     3000 |    897 batches | lr 0.00015 | ms/batch 30.58 | loss  2.49 | avg loss  2.86 | ppl 17.41 - 2025-05-28 13:26:53,760 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.3000.ckpt - 2025-05-28 13:26:53,761 - log
| epoch   2 step     3100 |    997 batches | lr 0.000148 | ms/batch 30.18 | loss  3.74 | avg loss  2.83 | ppl 16.94 - 2025-05-28 13:26:56,779 - log
| epoch   2 step     3200 |   1097 batches | lr 0.000146 | ms/batch 29.58 | loss  2.42 | avg loss  2.89 | ppl 17.97 - 2025-05-28 13:26:59,737 - log
| epoch   2 step     3300 |   1197 batches | lr 0.000144 | ms/batch 30.14 | loss  3.11 | avg loss  2.90 | ppl 18.15 - 2025-05-28 13:27:02,752 - log
| epoch   2 step     3400 |   1297 batches | lr 0.000142 | ms/batch 30.38 | loss  3.08 | avg loss  2.99 | ppl 19.80 - 2025-05-28 13:27:05,791 - log
| epoch   2 step     3500 |   1397 batches | lr 0.00014 | ms/batch 30.18 | loss  2.87 | avg loss  2.94 | ppl 18.94 - 2025-05-28 13:27:08,809 - log
| epoch   2 step     3600 |   1497 batches | lr 0.000138 | ms/batch 29.72 | loss  2.32 | avg loss  2.90 | ppl 18.19 - 2025-05-28 13:27:11,782 - log
| epoch   2 step     3700 |   1597 batches | lr 0.000136 | ms/batch 29.82 | loss  2.96 | avg loss  2.94 | ppl 18.82 - 2025-05-28 13:27:14,764 - log
| epoch   2 step     3800 |   1697 batches | lr 0.000134 | ms/batch 30.76 | loss  2.34 | avg loss  2.88 | ppl 17.77 - 2025-05-28 13:27:17,841 - log
| epoch   2 step     3900 |   1797 batches | lr 0.000132 | ms/batch 29.93 | loss  2.42 | avg loss  2.84 | ppl 17.17 - 2025-05-28 13:27:20,834 - log
| epoch   2 step     4000 |   1897 batches | lr 0.00013 | ms/batch 29.85 | loss  2.84 | avg loss  2.85 | ppl 17.34 - 2025-05-28 13:27:23,819 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4000.ckpt - 2025-05-28 13:27:23,819 - log
eval samples: 0, loss: 1.2568224668502808 - 2025-05-28 13:27:23,839 - log
eval samples: 100, loss: 1.1195513010025024 - 2025-05-28 13:27:25,341 - log
eval samples: 200, loss: 1.4956269264221191 - 2025-05-28 13:27:26,843 - log
eval samples: 300, loss: 0.7803529500961304 - 2025-05-28 13:27:28,344 - log
eval samples: 400, loss: 1.3677314519882202 - 2025-05-28 13:27:29,845 - log
average loss: +1.4779357777417854 - 2025-05-28 13:27:30,839 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:27:30,839 - log
| Eval   2 at step     4000 | time:  7.02s | valid loss  1.48 | valid ppl  4.38 | best ppl  4.38  - 2025-05-28 13:27:30,839 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:27:30,839 - log
| epoch   2 step     4100 |   1997 batches | lr 0.000128 | ms/batch 100.52 | loss  2.82 | avg loss  2.85 | ppl 17.33 - 2025-05-28 13:27:33,871 - log
| epoch   2 step     4200 |   2097 batches | lr 0.000126 | ms/batch 30.20 | loss  2.78 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:27:36,892 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4206.pkl - 2025-05-28 13:27:37,047 - log
start to train the model................ 3 - 2025-05-28 13:27:38,653 - log
| epoch   3 step     4300 |     94 batches | lr 0.000124 | ms/batch 28.48 | loss  3.08 | avg loss  2.88 | ppl 17.73 - 2025-05-28 13:27:41,500 - log
| epoch   3 step     4400 |    194 batches | lr 0.000122 | ms/batch 29.94 | loss  2.35 | avg loss  2.80 | ppl 16.38 - 2025-05-28 13:27:44,495 - log
| epoch   3 step     4500 |    294 batches | lr 0.00012 | ms/batch 29.85 | loss  2.73 | avg loss  2.82 | ppl 16.81 - 2025-05-28 13:27:47,479 - log
| epoch   3 step     4600 |    394 batches | lr 0.000118 | ms/batch 29.83 | loss  2.45 | avg loss  2.74 | ppl 15.51 - 2025-05-28 13:27:50,463 - log
| epoch   3 step     4700 |    494 batches | lr 0.000116 | ms/batch 30.08 | loss  2.71 | avg loss  2.84 | ppl 17.08 - 2025-05-28 13:27:53,471 - log
| epoch   3 step     4800 |    594 batches | lr 0.000114 | ms/batch 29.81 | loss  3.04 | avg loss  2.84 | ppl 17.04 - 2025-05-28 13:27:56,452 - log
| epoch   3 step     4900 |    694 batches | lr 0.000112 | ms/batch 29.63 | loss  3.04 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:27:59,415 - log
| epoch   3 step     5000 |    794 batches | lr 0.00011 | ms/batch 29.86 | loss  2.69 | avg loss  2.80 | ppl 16.43 - 2025-05-28 13:28:02,402 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5000.ckpt - 2025-05-28 13:28:02,402 - log
| epoch   3 step     5100 |    894 batches | lr 0.000108 | ms/batch 29.77 | loss  2.59 | avg loss  2.83 | ppl 16.91 - 2025-05-28 13:28:05,379 - log
| epoch   3 step     5200 |    994 batches | lr 0.000106 | ms/batch 29.71 | loss  3.60 | avg loss  2.84 | ppl 17.15 - 2025-05-28 13:28:08,350 - log
| epoch   3 step     5300 |   1094 batches | lr 0.000104 | ms/batch 29.66 | loss  2.31 | avg loss  2.84 | ppl 17.14 - 2025-05-28 13:28:11,317 - log
| epoch   3 step     5400 |   1194 batches | lr 0.000102 | ms/batch 29.61 | loss  2.71 | avg loss  2.83 | ppl 16.96 - 2025-05-28 13:28:14,278 - log
| epoch   3 step     5500 |   1294 batches | lr 0.0001 | ms/batch 29.77 | loss  2.44 | avg loss  2.84 | ppl 17.08 - 2025-05-28 13:28:17,256 - log
| epoch   3 step     5600 |   1394 batches | lr 9.82e-05 | ms/batch 29.68 | loss  2.82 | avg loss  2.78 | ppl 16.15 - 2025-05-28 13:28:20,224 - log
| epoch   3 step     5700 |   1494 batches | lr 9.62e-05 | ms/batch 30.13 | loss  2.67 | avg loss  2.91 | ppl 18.33 - 2025-05-28 13:28:23,237 - log
| epoch   3 step     5800 |   1594 batches | lr 9.42e-05 | ms/batch 30.65 | loss  2.85 | avg loss  2.76 | ppl 15.87 - 2025-05-28 13:28:26,303 - log
| epoch   3 step     5900 |   1694 batches | lr 9.22e-05 | ms/batch 31.11 | loss  2.93 | avg loss  2.74 | ppl 15.55 - 2025-05-28 13:28:29,415 - log
| epoch   3 step     6000 |   1794 batches | lr 9.02e-05 | ms/batch 30.21 | loss  2.32 | avg loss  2.85 | ppl 17.22 - 2025-05-28 13:28:32,436 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6000.ckpt - 2025-05-28 13:28:32,436 - log
eval samples: 0, loss: 1.528582215309143 - 2025-05-28 13:28:32,455 - log
eval samples: 100, loss: 2.212458610534668 - 2025-05-28 13:28:33,967 - log
eval samples: 200, loss: 1.4209142923355103 - 2025-05-28 13:28:35,480 - log
eval samples: 300, loss: 1.0029009580612183 - 2025-05-28 13:28:36,989 - log
eval samples: 400, loss: 1.7135777473449707 - 2025-05-28 13:28:38,498 - log
average loss: +1.4784412743958466 - 2025-05-28 13:28:39,495 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:28:39,496 - log
| Eval   3 at step     6000 | time:  7.06s | valid loss  1.48 | valid ppl  4.39 | best ppl  4.39  - 2025-05-28 13:28:39,496 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:28:39,496 - log
| epoch   3 step     6100 |   1894 batches | lr 8.82e-05 | ms/batch 100.63 | loss  2.58 | avg loss  2.72 | ppl 15.22 - 2025-05-28 13:28:42,499 - log
| epoch   3 step     6200 |   1994 batches | lr 8.62e-05 | ms/batch 30.10 | loss  2.55 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:28:45,509 - log
| epoch   3 step     6300 |   2094 batches | lr 8.42e-05 | ms/batch 30.36 | loss  2.81 | avg loss  2.89 | ppl 18.01 - 2025-05-28 13:28:48,545 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6309.pkl - 2025-05-28 13:28:48,803 - log
start to train the model................ 4 - 2025-05-28 13:28:50,318 - log
| epoch   4 step     6400 |     91 batches | lr 8.22e-05 | ms/batch 27.79 | loss  3.74 | avg loss  2.83 | ppl 16.90 - 2025-05-28 13:28:53,097 - log
| epoch   4 step     6500 |    191 batches | lr 8.02e-05 | ms/batch 30.47 | loss  2.74 | avg loss  2.79 | ppl 16.30 - 2025-05-28 13:28:56,145 - log
| epoch   4 step     6600 |    291 batches | lr 7.82e-05 | ms/batch 29.90 | loss  2.43 | avg loss  2.77 | ppl 15.98 - 2025-05-28 13:28:59,135 - log
| epoch   4 step     6700 |    391 batches | lr 7.62e-05 | ms/batch 29.96 | loss  2.83 | avg loss  2.81 | ppl 16.53 - 2025-05-28 13:29:02,132 - log
| epoch   4 step     6800 |    491 batches | lr 7.42e-05 | ms/batch 29.88 | loss  3.18 | avg loss  2.78 | ppl 16.06 - 2025-05-28 13:29:05,120 - log
| epoch   4 step     6900 |    591 batches | lr 7.22e-05 | ms/batch 29.72 | loss  2.21 | avg loss  2.78 | ppl 16.13 - 2025-05-28 13:29:08,093 - log
| epoch   4 step     7000 |    691 batches | lr 7.02e-05 | ms/batch 30.20 | loss  2.95 | avg loss  2.87 | ppl 17.61 - 2025-05-28 13:29:11,114 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.7000.ckpt - 2025-05-28 13:29:11,114 - log
| epoch   4 step     7100 |    791 batches | lr 6.82e-05 | ms/batch 29.86 | loss  2.36 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:29:14,100 - log
| epoch   4 step     7200 |    891 batches | lr 6.62e-05 | ms/batch 29.94 | loss  2.87 | avg loss  2.77 | ppl 16.03 - 2025-05-28 13:29:17,094 - log
| epoch   4 step     7300 |    991 batches | lr 6.42e-05 | ms/batch 29.86 | loss  2.85 | avg loss  2.77 | ppl 15.98 - 2025-05-28 13:29:20,080 - log
| epoch   4 step     7400 |   1091 batches | lr 6.22e-05 | ms/batch 30.29 | loss  2.71 | avg loss  2.75 | ppl 15.70 - 2025-05-28 13:29:23,109 - log
| epoch   4 step     7500 |   1191 batches | lr 6.02e-05 | ms/batch 30.01 | loss  2.82 | avg loss  2.75 | ppl 15.58 - 2025-05-28 13:29:26,111 - log
| epoch   4 step     7600 |   1291 batches | lr 5.82e-05 | ms/batch 29.84 | loss  2.27 | avg loss  2.73 | ppl 15.34 - 2025-05-28 13:29:29,095 - log
| epoch   4 step     7700 |   1391 batches | lr 5.62e-05 | ms/batch 30.35 | loss  3.30 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:29:32,130 - log
| epoch   4 step     7800 |   1491 batches | lr 5.42e-05 | ms/batch 30.65 | loss  2.75 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:29:35,195 - log
| epoch   4 step     7900 |   1591 batches | lr 5.22e-05 | ms/batch 30.20 | loss  3.20 | avg loss  2.81 | ppl 16.69 - 2025-05-28 13:29:38,216 - log
| epoch   4 step     8000 |   1691 batches | lr 5.02e-05 | ms/batch 30.35 | loss  2.26 | avg loss  2.76 | ppl 15.83 - 2025-05-28 13:29:41,251 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8000.ckpt - 2025-05-28 13:29:41,251 - log
eval samples: 0, loss: 1.3486098051071167 - 2025-05-28 13:29:41,271 - log
eval samples: 100, loss: 2.016256332397461 - 2025-05-28 13:29:42,775 - log
eval samples: 200, loss: 1.5685346126556396 - 2025-05-28 13:29:44,281 - log
eval samples: 300, loss: 1.8452339172363281 - 2025-05-28 13:29:45,786 - log
eval samples: 400, loss: 0.6121419072151184 - 2025-05-28 13:29:47,296 - log
average loss: +1.4211683249065146 - 2025-05-28 13:29:48,309 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:29:48,309 - log
| Eval   4 at step     8000 | time:  7.06s | valid loss  1.42 | valid ppl  4.14 | best ppl  4.14  - 2025-05-28 13:29:48,309 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:29:48,309 - log
| epoch   4 step     8100 |   1791 batches | lr 4.82e-05 | ms/batch 100.59 | loss  2.74 | avg loss  2.78 | ppl 16.19 - 2025-05-28 13:29:51,310 - log
| epoch   4 step     8200 |   1891 batches | lr 4.62e-05 | ms/batch 29.83 | loss  2.60 | avg loss  2.78 | ppl 16.12 - 2025-05-28 13:29:54,293 - log
| epoch   4 step     8300 |   1991 batches | lr 4.42e-05 | ms/batch 29.78 | loss  2.45 | avg loss  2.78 | ppl 16.05 - 2025-05-28 13:29:57,271 - log
| epoch   4 step     8400 |   2091 batches | lr 4.22e-05 | ms/batch 29.79 | loss  3.04 | avg loss  2.78 | ppl 16.06 - 2025-05-28 13:30:00,250 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8412.pkl - 2025-05-28 13:30:00,588 - log
==================================================================================================== - 2025-05-28 13:41:22,361 - log
        - random_seed : 2025 - 2025-05-28 13:41:22,362 - log
        - lr : 0.0002 - 2025-05-28 13:41:22,362 - log
        - weight_decay : 0.01 - 2025-05-28 13:41:22,362 - log
        - correct_bias : False - 2025-05-28 13:41:22,362 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:41:22,362 - log
        - no_decay_bias : False - 2025-05-28 13:41:22,362 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:41:22,362 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:41:22,362 - log
        - scheduler : linear - 2025-05-28 13:41:22,362 - log
        - max_step : None - 2025-05-28 13:41:22,362 - log
        - max_epoch : 5 - 2025-05-28 13:41:22,362 - log
        - warmup_step : 500 - 2025-05-28 13:41:22,362 - log
        - i_steps : 0 - 2025-05-28 13:41:22,362 - log
        - i_lrs : 0.00025 - 2025-05-28 13:41:22,362 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:41:22,362 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:41:22,362 - log
        - train_batch_size : 2 - 2025-05-28 13:41:22,362 - log
        - valid_batch_size : 1 - 2025-05-28 13:41:22,362 - log
        - grad_acc : 2 - 2025-05-28 13:41:22,362 - log
        - clip : 0.0 - 2025-05-28 13:41:22,362 - log
        - seq_len : 64 - 2025-05-28 13:41:22,362 - log
        - model_card : gpt2.sm - 2025-05-28 13:41:22,362 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:41:22,362 - log
        - fp16 : False - 2025-05-28 13:41:22,362 - log
        - log_interval : 100 - 2025-05-28 13:41:22,362 - log
        - eval_interval : 2000 - 2025-05-28 13:41:22,362 - log
        - save_interval : 1000 - 2025-05-28 13:41:22,362 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:41:22,362 - log
        - lora_dim : 4 - 2025-05-28 13:41:22,362 - log
        - lora_alpha : 32 - 2025-05-28 13:41:22,362 - log
        - obj : clm - 2025-05-28 13:41:22,362 - log
        - lora_dropout : 0.1 - 2025-05-28 13:41:22,362 - log
        - label_smooth : 0.1 - 2025-05-28 13:41:22,362 - log
        - roll_interval : -1 - 2025-05-28 13:41:22,362 - log
        - roll_lr : 1e-05 - 2025-05-28 13:41:22,362 - log
        - roll_step : 100 - 2025-05-28 13:41:22,362 - log
        - eval_epoch : 1 - 2025-05-28 13:41:22,362 - log
        - device : cuda - 2025-05-28 13:41:22,362 - log
==================================================================================================== - 2025-05-28 13:41:22,362 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:41:22,362 - log
loading model pretrained weight. - 2025-05-28 13:41:22,492 - log
set max_step: 10515 - 2025-05-28 13:41:23,390 - log
start to train the model................ 1 - 2025-05-28 13:41:23,390 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 34.74 | loss  5.81 | avg loss  5.99 | ppl 398.74 - 2025-05-28 13:41:26,865 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.01 | loss  4.26 | avg loss  4.80 | ppl 121.75 - 2025-05-28 13:41:29,867 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.18 | loss  3.10 | avg loss  3.60 | ppl 36.58 - 2025-05-28 13:41:32,885 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.94 | loss  3.15 | avg loss  3.31 | ppl 27.43 - 2025-05-28 13:41:35,879 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 30.36 | loss  3.23 | avg loss  3.21 | ppl 24.89 - 2025-05-28 13:41:38,915 - log
| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 30.21 | loss  3.22 | avg loss  3.11 | ppl 22.31 - 2025-05-28 13:41:41,937 - log
| epoch   1 step      700 |    700 batches | lr 0.000196 | ms/batch 30.25 | loss  3.16 | avg loss  3.18 | ppl 24.02 - 2025-05-28 13:41:44,962 - log
| epoch   1 step      800 |    800 batches | lr 0.000194 | ms/batch 30.28 | loss  4.05 | avg loss  3.09 | ppl 21.98 - 2025-05-28 13:41:47,990 - log
| epoch   1 step      900 |    900 batches | lr 0.000192 | ms/batch 30.55 | loss  3.21 | avg loss  3.11 | ppl 22.39 - 2025-05-28 13:41:51,045 - log
| epoch   1 step     1000 |   1000 batches | lr 0.00019 | ms/batch 30.29 | loss  3.30 | avg loss  3.13 | ppl 22.98 - 2025-05-28 13:41:54,075 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:41:54,075 - log
==================================================================================================== - 2025-05-28 13:43:51,977 - log
        - random_seed : 2025 - 2025-05-28 13:43:51,977 - log
        - lr : 0.0002 - 2025-05-28 13:43:51,977 - log
        - weight_decay : 0.01 - 2025-05-28 13:43:51,977 - log
        - correct_bias : False - 2025-05-28 13:43:51,977 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:43:51,977 - log
        - no_decay_bias : False - 2025-05-28 13:43:51,977 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:43:51,977 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:43:51,977 - log
        - scheduler : linear - 2025-05-28 13:43:51,977 - log
        - max_step : None - 2025-05-28 13:43:51,977 - log
        - max_epoch : 5 - 2025-05-28 13:43:51,977 - log
        - warmup_step : 500 - 2025-05-28 13:43:51,977 - log
        - i_steps : 0 - 2025-05-28 13:43:51,977 - log
        - i_lrs : 0.00025 - 2025-05-28 13:43:51,977 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:43:51,977 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:43:51,977 - log
        - train_batch_size : 2 - 2025-05-28 13:43:51,978 - log
        - valid_batch_size : 1 - 2025-05-28 13:43:51,978 - log
        - grad_acc : 2 - 2025-05-28 13:43:51,978 - log
        - clip : 0.0 - 2025-05-28 13:43:51,978 - log
        - seq_len : 64 - 2025-05-28 13:43:51,978 - log
        - model_card : gpt2.sm - 2025-05-28 13:43:51,978 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:43:51,978 - log
        - fp16 : False - 2025-05-28 13:43:51,978 - log
        - log_interval : 100 - 2025-05-28 13:43:51,978 - log
        - eval_interval : 2000 - 2025-05-28 13:43:51,978 - log
        - save_interval : 1000 - 2025-05-28 13:43:51,978 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:43:51,978 - log
        - lora_dim : 4 - 2025-05-28 13:43:51,978 - log
        - lora_alpha : 32 - 2025-05-28 13:43:51,978 - log
        - obj : clm - 2025-05-28 13:43:51,978 - log
        - lora_dropout : 0.1 - 2025-05-28 13:43:51,978 - log
        - label_smooth : 0.1 - 2025-05-28 13:43:51,978 - log
        - roll_interval : -1 - 2025-05-28 13:43:51,978 - log
        - roll_lr : 1e-05 - 2025-05-28 13:43:51,978 - log
        - roll_step : 100 - 2025-05-28 13:43:51,978 - log
        - eval_epoch : 1 - 2025-05-28 13:43:51,978 - log
        - device : cuda - 2025-05-28 13:43:51,978 - log
==================================================================================================== - 2025-05-28 13:43:51,978 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:43:51,978 - log
loading model pretrained weight. - 2025-05-28 13:43:52,104 - log
set max_step: 10515 - 2025-05-28 13:43:53,093 - log
start to train the model................ 1 - 2025-05-28 13:43:53,094 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.33 | loss  5.81 | avg loss  5.99 | ppl 398.74 - 2025-05-28 13:43:56,426 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.74 | loss  4.26 | avg loss  4.80 | ppl 121.75 - 2025-05-28 13:43:59,401 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.87 | loss  3.10 | avg loss  3.60 | ppl 36.58 - 2025-05-28 13:44:02,389 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.98 | loss  3.15 | avg loss  3.31 | ppl 27.43 - 2025-05-28 13:44:05,387 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.86 | loss  3.23 | avg loss  3.21 | ppl 24.89 - 2025-05-28 13:44:08,374 - log
| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 29.82 | loss  3.22 | avg loss  3.11 | ppl 22.31 - 2025-05-28 13:44:11,356 - log
| epoch   1 step      700 |    700 batches | lr 0.000196 | ms/batch 29.91 | loss  3.16 | avg loss  3.18 | ppl 24.02 - 2025-05-28 13:44:14,347 - log
| epoch   1 step      800 |    800 batches | lr 0.000194 | ms/batch 29.83 | loss  4.05 | avg loss  3.09 | ppl 21.98 - 2025-05-28 13:44:17,331 - log
| epoch   1 step      900 |    900 batches | lr 0.000192 | ms/batch 29.84 | loss  3.21 | avg loss  3.11 | ppl 22.39 - 2025-05-28 13:44:20,316 - log
| epoch   1 step     1000 |   1000 batches | lr 0.00019 | ms/batch 29.81 | loss  3.30 | avg loss  3.13 | ppl 22.98 - 2025-05-28 13:44:23,296 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:44:23,297 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000188 | ms/batch 29.65 | loss  3.24 | avg loss  3.04 | ppl 20.94 - 2025-05-28 13:44:26,262 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000186 | ms/batch 29.62 | loss  3.06 | avg loss  2.96 | ppl 19.38 - 2025-05-28 13:44:29,225 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000184 | ms/batch 29.62 | loss  2.99 | avg loss  3.01 | ppl 20.38 - 2025-05-28 13:44:32,187 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000182 | ms/batch 29.64 | loss  2.65 | avg loss  2.95 | ppl 19.15 - 2025-05-28 13:44:35,151 - log
| epoch   1 step     1500 |   1500 batches | lr 0.00018 | ms/batch 29.75 | loss  2.33 | avg loss  2.92 | ppl 18.60 - 2025-05-28 13:44:38,127 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000178 | ms/batch 30.17 | loss  2.48 | avg loss  2.94 | ppl 18.92 - 2025-05-28 13:44:41,144 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000176 | ms/batch 29.80 | loss  2.56 | avg loss  2.92 | ppl 18.49 - 2025-05-28 13:44:44,124 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000174 | ms/batch 29.77 | loss  2.48 | avg loss  2.98 | ppl 19.66 - 2025-05-28 13:44:47,101 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000172 | ms/batch 29.76 | loss  2.42 | avg loss  3.04 | ppl 20.81 - 2025-05-28 13:44:50,078 - log
| epoch   1 step     2000 |   2000 batches | lr 0.00017 | ms/batch 29.83 | loss  3.00 | avg loss  3.00 | ppl 20.16 - 2025-05-28 13:44:53,061 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:44:53,062 - log
eval samples: 0, loss: 1.3806884288787842 - 2025-05-28 13:44:53,082 - log
eval samples: 100, loss: 1.6813820600509644 - 2025-05-28 13:44:54,607 - log
eval samples: 200, loss: 1.1190109252929688 - 2025-05-28 13:44:56,125 - log
eval samples: 300, loss: 1.942086935043335 - 2025-05-28 13:44:57,649 - log
eval samples: 400, loss: 1.3401538133621216 - 2025-05-28 13:44:59,169 - log
average loss: +1.5682523364173269 - 2025-05-28 13:45:00,176 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:45:00,176 - log
| Eval   1 at step     2000 | time:  7.11s | valid loss  1.57 | valid ppl  4.80 | best ppl  4.80  - 2025-05-28 13:45:00,176 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:45:00,176 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000168 | ms/batch 101.08 | loss  3.14 | avg loss  2.94 | ppl 18.91 - 2025-05-28 13:45:03,170 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2103.pkl - 2025-05-28 13:45:03,248 - log
start to train the model................ 2 - 2025-05-28 13:45:04,895 - log
==================================================================================================== - 2025-05-28 13:45:47,629 - log
        - random_seed : 2025 - 2025-05-28 13:45:47,629 - log
        - lr : 0.0002 - 2025-05-28 13:45:47,629 - log
        - weight_decay : 0.01 - 2025-05-28 13:45:47,629 - log
        - correct_bias : False - 2025-05-28 13:45:47,629 - log
        - adam_epislon : 1e-06 - 2025-05-28 13:45:47,629 - log
        - no_decay_bias : False - 2025-05-28 13:45:47,629 - log
        - adam_beta1 : 0.9 - 2025-05-28 13:45:47,629 - log
        - adam_beta2 : 0.999 - 2025-05-28 13:45:47,629 - log
        - scheduler : linear - 2025-05-28 13:45:47,629 - log
        - max_step : None - 2025-05-28 13:45:47,629 - log
        - max_epoch : 5 - 2025-05-28 13:45:47,629 - log
        - warmup_step : 500 - 2025-05-28 13:45:47,629 - log
        - i_steps : 0 - 2025-05-28 13:45:47,629 - log
        - i_lrs : 0.00025 - 2025-05-28 13:45:47,629 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 13:45:47,629 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 13:45:47,629 - log
        - train_batch_size : 2 - 2025-05-28 13:45:47,629 - log
        - valid_batch_size : 1 - 2025-05-28 13:45:47,629 - log
        - grad_acc : 2 - 2025-05-28 13:45:47,629 - log
        - clip : 0.0 - 2025-05-28 13:45:47,629 - log
        - seq_len : 64 - 2025-05-28 13:45:47,629 - log
        - model_card : gpt2.sm - 2025-05-28 13:45:47,629 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 13:45:47,629 - log
        - fp16 : False - 2025-05-28 13:45:47,629 - log
        - log_interval : 100 - 2025-05-28 13:45:47,629 - log
        - eval_interval : 2000 - 2025-05-28 13:45:47,629 - log
        - save_interval : 1000 - 2025-05-28 13:45:47,630 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 13:45:47,630 - log
        - lora_dim : 4 - 2025-05-28 13:45:47,630 - log
        - lora_alpha : 32 - 2025-05-28 13:45:47,630 - log
        - obj : clm - 2025-05-28 13:45:47,630 - log
        - lora_dropout : 0.1 - 2025-05-28 13:45:47,630 - log
        - label_smooth : 0.1 - 2025-05-28 13:45:47,630 - log
        - roll_interval : -1 - 2025-05-28 13:45:47,630 - log
        - roll_lr : 1e-05 - 2025-05-28 13:45:47,630 - log
        - roll_step : 100 - 2025-05-28 13:45:47,630 - log
        - eval_epoch : 1 - 2025-05-28 13:45:47,630 - log
        - device : cuda - 2025-05-28 13:45:47,630 - log
==================================================================================================== - 2025-05-28 13:45:47,630 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 13:45:47,630 - log
loading model pretrained weight. - 2025-05-28 13:45:47,758 - log
set max_step: 10515 - 2025-05-28 13:45:48,730 - log
start to train the model................ 1 - 2025-05-28 13:45:48,730 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 33.66 | loss  5.81 | avg loss  5.99 | ppl 398.74 - 2025-05-28 13:45:52,096 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 29.66 | loss  4.26 | avg loss  4.80 | ppl 121.75 - 2025-05-28 13:45:55,062 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 29.75 | loss  3.10 | avg loss  3.60 | ppl 36.58 - 2025-05-28 13:45:58,038 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 29.58 | loss  3.15 | avg loss  3.31 | ppl 27.43 - 2025-05-28 13:46:00,996 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 29.48 | loss  3.23 | avg loss  3.21 | ppl 24.89 - 2025-05-28 13:46:03,945 - log
| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 29.36 | loss  3.22 | avg loss  3.11 | ppl 22.31 - 2025-05-28 13:46:06,881 - log
| epoch   1 step      700 |    700 batches | lr 0.000196 | ms/batch 29.41 | loss  3.16 | avg loss  3.18 | ppl 24.02 - 2025-05-28 13:46:09,822 - log
| epoch   1 step      800 |    800 batches | lr 0.000194 | ms/batch 29.56 | loss  4.05 | avg loss  3.09 | ppl 21.98 - 2025-05-28 13:46:12,778 - log
| epoch   1 step      900 |    900 batches | lr 0.000192 | ms/batch 29.31 | loss  3.21 | avg loss  3.11 | ppl 22.39 - 2025-05-28 13:46:15,709 - log
| epoch   1 step     1000 |   1000 batches | lr 0.00019 | ms/batch 29.45 | loss  3.30 | avg loss  3.13 | ppl 22.98 - 2025-05-28 13:46:18,655 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 13:46:18,655 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000188 | ms/batch 29.63 | loss  3.24 | avg loss  3.04 | ppl 20.94 - 2025-05-28 13:46:21,619 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000186 | ms/batch 29.44 | loss  3.06 | avg loss  2.96 | ppl 19.38 - 2025-05-28 13:46:24,563 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000184 | ms/batch 29.75 | loss  2.99 | avg loss  3.01 | ppl 20.38 - 2025-05-28 13:46:27,538 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000182 | ms/batch 29.51 | loss  2.65 | avg loss  2.95 | ppl 19.15 - 2025-05-28 13:46:30,489 - log
| epoch   1 step     1500 |   1500 batches | lr 0.00018 | ms/batch 29.71 | loss  2.33 | avg loss  2.92 | ppl 18.60 - 2025-05-28 13:46:33,461 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000178 | ms/batch 29.49 | loss  2.48 | avg loss  2.94 | ppl 18.92 - 2025-05-28 13:46:36,410 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000176 | ms/batch 29.51 | loss  2.56 | avg loss  2.92 | ppl 18.49 - 2025-05-28 13:46:39,362 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000174 | ms/batch 29.47 | loss  2.48 | avg loss  2.98 | ppl 19.66 - 2025-05-28 13:46:42,309 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000172 | ms/batch 29.51 | loss  2.42 | avg loss  3.04 | ppl 20.81 - 2025-05-28 13:46:45,260 - log
| epoch   1 step     2000 |   2000 batches | lr 0.00017 | ms/batch 29.62 | loss  3.00 | avg loss  3.00 | ppl 20.16 - 2025-05-28 13:46:48,222 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 13:46:48,223 - log
eval samples: 0, loss: 1.3806836605072021 - 2025-05-28 13:46:48,243 - log
eval samples: 100, loss: 1.6813900470733643 - 2025-05-28 13:46:49,721 - log
eval samples: 200, loss: 1.1190119981765747 - 2025-05-28 13:46:51,195 - log
eval samples: 300, loss: 1.9420926570892334 - 2025-05-28 13:46:52,672 - log
eval samples: 400, loss: 1.3401546478271484 - 2025-05-28 13:46:54,150 - log
average loss: +1.568253649761641 - 2025-05-28 13:46:55,124 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:46:55,125 - log
| Eval   1 at step     2000 | time:  6.90s | valid loss  1.57 | valid ppl  4.80 | best ppl  4.80  - 2025-05-28 13:46:55,125 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:46:55,125 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000168 | ms/batch 98.39 | loss  3.14 | avg loss  2.94 | ppl 18.91 - 2025-05-28 13:46:58,062 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2103.pkl - 2025-05-28 13:46:58,139 - log
start to train the model................ 2 - 2025-05-28 13:46:59,911 - log
| epoch   2 step     2200 |     97 batches | lr 0.000166 | ms/batch 29.40 | loss  2.42 | avg loss  2.86 | ppl 17.40 - 2025-05-28 13:47:02,851 - log
| epoch   2 step     2300 |    197 batches | lr 0.000164 | ms/batch 29.76 | loss  2.66 | avg loss  2.93 | ppl 18.64 - 2025-05-28 13:47:05,827 - log
| epoch   2 step     2400 |    297 batches | lr 0.000162 | ms/batch 29.67 | loss  2.98 | avg loss  2.97 | ppl 19.57 - 2025-05-28 13:47:08,794 - log
| epoch   2 step     2500 |    397 batches | lr 0.00016 | ms/batch 29.64 | loss  2.42 | avg loss  2.90 | ppl 18.16 - 2025-05-28 13:47:11,758 - log
| epoch   2 step     2600 |    497 batches | lr 0.000158 | ms/batch 29.94 | loss  2.51 | avg loss  2.87 | ppl 17.61 - 2025-05-28 13:47:14,753 - log
| epoch   2 step     2700 |    597 batches | lr 0.000156 | ms/batch 29.81 | loss  2.44 | avg loss  2.90 | ppl 18.23 - 2025-05-28 13:47:17,734 - log
| epoch   2 step     2800 |    697 batches | lr 0.000154 | ms/batch 29.82 | loss  2.95 | avg loss  2.86 | ppl 17.43 - 2025-05-28 13:47:20,716 - log
| epoch   2 step     2900 |    797 batches | lr 0.000152 | ms/batch 29.83 | loss  2.85 | avg loss  2.87 | ppl 17.57 - 2025-05-28 13:47:23,700 - log
| epoch   2 step     3000 |    897 batches | lr 0.00015 | ms/batch 29.68 | loss  2.49 | avg loss  2.86 | ppl 17.41 - 2025-05-28 13:47:26,668 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.3000.ckpt - 2025-05-28 13:47:26,669 - log
| epoch   2 step     3100 |    997 batches | lr 0.000148 | ms/batch 29.73 | loss  3.74 | avg loss  2.83 | ppl 16.94 - 2025-05-28 13:47:29,642 - log
| epoch   2 step     3200 |   1097 batches | lr 0.000146 | ms/batch 29.70 | loss  2.42 | avg loss  2.89 | ppl 17.97 - 2025-05-28 13:47:32,613 - log
| epoch   2 step     3300 |   1197 batches | lr 0.000144 | ms/batch 29.88 | loss  3.11 | avg loss  2.90 | ppl 18.15 - 2025-05-28 13:47:35,601 - log
| epoch   2 step     3400 |   1297 batches | lr 0.000142 | ms/batch 30.03 | loss  3.08 | avg loss  2.99 | ppl 19.80 - 2025-05-28 13:47:38,604 - log
| epoch   2 step     3500 |   1397 batches | lr 0.00014 | ms/batch 29.63 | loss  2.87 | avg loss  2.94 | ppl 18.94 - 2025-05-28 13:47:41,568 - log
| epoch   2 step     3600 |   1497 batches | lr 0.000138 | ms/batch 29.61 | loss  2.32 | avg loss  2.90 | ppl 18.19 - 2025-05-28 13:47:44,529 - log
| epoch   2 step     3700 |   1597 batches | lr 0.000136 | ms/batch 29.64 | loss  2.96 | avg loss  2.94 | ppl 18.82 - 2025-05-28 13:47:47,493 - log
| epoch   2 step     3800 |   1697 batches | lr 0.000134 | ms/batch 29.67 | loss  2.34 | avg loss  2.88 | ppl 17.77 - 2025-05-28 13:47:50,461 - log
| epoch   2 step     3900 |   1797 batches | lr 0.000132 | ms/batch 29.61 | loss  2.42 | avg loss  2.84 | ppl 17.17 - 2025-05-28 13:47:53,422 - log
| epoch   2 step     4000 |   1897 batches | lr 0.00013 | ms/batch 29.56 | loss  2.84 | avg loss  2.85 | ppl 17.34 - 2025-05-28 13:47:56,378 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4000.ckpt - 2025-05-28 13:47:56,378 - log
eval samples: 0, loss: 1.2568293809890747 - 2025-05-28 13:47:56,398 - log
eval samples: 100, loss: 1.1195480823516846 - 2025-05-28 13:47:57,885 - log
eval samples: 200, loss: 1.4956296682357788 - 2025-05-28 13:47:59,366 - log
eval samples: 300, loss: 0.7803559303283691 - 2025-05-28 13:48:00,857 - log
eval samples: 400, loss: 1.3677369356155396 - 2025-05-28 13:48:02,337 - log
average loss: +1.4779360499831287 - 2025-05-28 13:48:03,313 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:48:03,313 - log
| Eval   2 at step     4000 | time:  6.93s | valid loss  1.48 | valid ppl  4.38 | best ppl  4.38  - 2025-05-28 13:48:03,313 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:48:03,313 - log
| epoch   2 step     4100 |   1997 batches | lr 0.000128 | ms/batch 98.88 | loss  2.82 | avg loss  2.85 | ppl 17.33 - 2025-05-28 13:48:06,267 - log
| epoch   2 step     4200 |   2097 batches | lr 0.000126 | ms/batch 29.43 | loss  2.78 | avg loss  2.83 | ppl 16.95 - 2025-05-28 13:48:09,210 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4206.pkl - 2025-05-28 13:48:09,364 - log
start to train the model................ 3 - 2025-05-28 13:48:11,006 - log
| epoch   3 step     4300 |     94 batches | lr 0.000124 | ms/batch 28.06 | loss  3.08 | avg loss  2.88 | ppl 17.73 - 2025-05-28 13:48:13,812 - log
| epoch   3 step     4400 |    194 batches | lr 0.000122 | ms/batch 29.66 | loss  2.35 | avg loss  2.80 | ppl 16.38 - 2025-05-28 13:48:16,778 - log
| epoch   3 step     4500 |    294 batches | lr 0.00012 | ms/batch 29.82 | loss  2.73 | avg loss  2.82 | ppl 16.81 - 2025-05-28 13:48:19,761 - log
| epoch   3 step     4600 |    394 batches | lr 0.000118 | ms/batch 29.71 | loss  2.45 | avg loss  2.74 | ppl 15.51 - 2025-05-28 13:48:22,731 - log
| epoch   3 step     4700 |    494 batches | lr 0.000116 | ms/batch 29.68 | loss  2.71 | avg loss  2.84 | ppl 17.08 - 2025-05-28 13:48:25,700 - log
| epoch   3 step     4800 |    594 batches | lr 0.000114 | ms/batch 30.03 | loss  3.04 | avg loss  2.84 | ppl 17.04 - 2025-05-28 13:48:28,704 - log
| epoch   3 step     4900 |    694 batches | lr 0.000112 | ms/batch 30.14 | loss  3.04 | avg loss  2.93 | ppl 18.71 - 2025-05-28 13:48:31,719 - log
| epoch   3 step     5000 |    794 batches | lr 0.00011 | ms/batch 29.54 | loss  2.69 | avg loss  2.80 | ppl 16.43 - 2025-05-28 13:48:34,673 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5000.ckpt - 2025-05-28 13:48:34,673 - log
| epoch   3 step     5100 |    894 batches | lr 0.000108 | ms/batch 29.71 | loss  2.59 | avg loss  2.83 | ppl 16.91 - 2025-05-28 13:48:37,644 - log
| epoch   3 step     5200 |    994 batches | lr 0.000106 | ms/batch 29.71 | loss  3.60 | avg loss  2.84 | ppl 17.15 - 2025-05-28 13:48:40,615 - log
| epoch   3 step     5300 |   1094 batches | lr 0.000104 | ms/batch 29.63 | loss  2.31 | avg loss  2.84 | ppl 17.14 - 2025-05-28 13:48:43,578 - log
| epoch   3 step     5400 |   1194 batches | lr 0.000102 | ms/batch 29.73 | loss  2.71 | avg loss  2.83 | ppl 16.96 - 2025-05-28 13:48:46,551 - log
| epoch   3 step     5500 |   1294 batches | lr 0.0001 | ms/batch 29.96 | loss  2.44 | avg loss  2.84 | ppl 17.08 - 2025-05-28 13:48:49,548 - log
| epoch   3 step     5600 |   1394 batches | lr 9.82e-05 | ms/batch 29.70 | loss  2.82 | avg loss  2.78 | ppl 16.15 - 2025-05-28 13:48:52,517 - log
| epoch   3 step     5700 |   1494 batches | lr 9.62e-05 | ms/batch 29.57 | loss  2.67 | avg loss  2.91 | ppl 18.33 - 2025-05-28 13:48:55,475 - log
| epoch   3 step     5800 |   1594 batches | lr 9.42e-05 | ms/batch 29.60 | loss  2.85 | avg loss  2.76 | ppl 15.87 - 2025-05-28 13:48:58,435 - log
| epoch   3 step     5900 |   1694 batches | lr 9.22e-05 | ms/batch 29.67 | loss  2.93 | avg loss  2.74 | ppl 15.55 - 2025-05-28 13:49:01,402 - log
| epoch   3 step     6000 |   1794 batches | lr 9.02e-05 | ms/batch 30.07 | loss  2.32 | avg loss  2.85 | ppl 17.22 - 2025-05-28 13:49:04,409 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6000.ckpt - 2025-05-28 13:49:04,410 - log
eval samples: 0, loss: 1.5285892486572266 - 2025-05-28 13:49:04,428 - log
eval samples: 100, loss: 2.212453603744507 - 2025-05-28 13:49:05,924 - log
eval samples: 200, loss: 1.4209132194519043 - 2025-05-28 13:49:07,415 - log
eval samples: 300, loss: 1.0028976202011108 - 2025-05-28 13:49:08,903 - log
eval samples: 400, loss: 1.7135635614395142 - 2025-05-28 13:49:10,389 - log
average loss: +1.478440996028116 - 2025-05-28 13:49:11,374 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:49:11,374 - log
| Eval   3 at step     6000 | time:  6.96s | valid loss  1.48 | valid ppl  4.39 | best ppl  4.39  - 2025-05-28 13:49:11,374 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:49:11,374 - log
| epoch   3 step     6100 |   1894 batches | lr 8.82e-05 | ms/batch 99.38 | loss  2.58 | avg loss  2.72 | ppl 15.22 - 2025-05-28 13:49:14,348 - log
| epoch   3 step     6200 |   1994 batches | lr 8.62e-05 | ms/batch 29.61 | loss  2.55 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:49:17,309 - log
| epoch   3 step     6300 |   2094 batches | lr 8.42e-05 | ms/batch 29.81 | loss  2.81 | avg loss  2.89 | ppl 18.01 - 2025-05-28 13:49:20,291 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6309.pkl - 2025-05-28 13:49:20,549 - log
start to train the model................ 4 - 2025-05-28 13:49:22,107 - log
| epoch   4 step     6400 |     91 batches | lr 8.22e-05 | ms/batch 27.35 | loss  3.74 | avg loss  2.83 | ppl 16.90 - 2025-05-28 13:49:24,843 - log
| epoch   4 step     6500 |    191 batches | lr 8.02e-05 | ms/batch 29.47 | loss  2.74 | avg loss  2.79 | ppl 16.30 - 2025-05-28 13:49:27,790 - log
| epoch   4 step     6600 |    291 batches | lr 7.82e-05 | ms/batch 29.53 | loss  2.43 | avg loss  2.77 | ppl 15.98 - 2025-05-28 13:49:30,743 - log
| epoch   4 step     6700 |    391 batches | lr 7.62e-05 | ms/batch 29.58 | loss  2.83 | avg loss  2.81 | ppl 16.53 - 2025-05-28 13:49:33,701 - log
| epoch   4 step     6800 |    491 batches | lr 7.42e-05 | ms/batch 29.96 | loss  3.18 | avg loss  2.78 | ppl 16.06 - 2025-05-28 13:49:36,697 - log
| epoch   4 step     6900 |    591 batches | lr 7.22e-05 | ms/batch 29.70 | loss  2.21 | avg loss  2.78 | ppl 16.13 - 2025-05-28 13:49:39,668 - log
| epoch   4 step     7000 |    691 batches | lr 7.02e-05 | ms/batch 30.07 | loss  2.95 | avg loss  2.87 | ppl 17.61 - 2025-05-28 13:49:42,675 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.7000.ckpt - 2025-05-28 13:49:42,675 - log
| epoch   4 step     7100 |    791 batches | lr 6.82e-05 | ms/batch 29.93 | loss  2.36 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:49:45,668 - log
| epoch   4 step     7200 |    891 batches | lr 6.62e-05 | ms/batch 29.91 | loss  2.87 | avg loss  2.77 | ppl 16.03 - 2025-05-28 13:49:48,659 - log
| epoch   4 step     7300 |    991 batches | lr 6.42e-05 | ms/batch 30.03 | loss  2.85 | avg loss  2.77 | ppl 15.98 - 2025-05-28 13:49:51,662 - log
| epoch   4 step     7400 |   1091 batches | lr 6.22e-05 | ms/batch 29.81 | loss  2.71 | avg loss  2.75 | ppl 15.70 - 2025-05-28 13:49:54,644 - log
| epoch   4 step     7500 |   1191 batches | lr 6.02e-05 | ms/batch 29.84 | loss  2.81 | avg loss  2.75 | ppl 15.58 - 2025-05-28 13:49:57,628 - log
| epoch   4 step     7600 |   1291 batches | lr 5.82e-05 | ms/batch 30.05 | loss  2.27 | avg loss  2.73 | ppl 15.34 - 2025-05-28 13:50:00,634 - log
| epoch   4 step     7700 |   1391 batches | lr 5.62e-05 | ms/batch 30.02 | loss  3.30 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:50:03,637 - log
| epoch   4 step     7800 |   1491 batches | lr 5.42e-05 | ms/batch 29.57 | loss  2.75 | avg loss  2.79 | ppl 16.33 - 2025-05-28 13:50:06,593 - log
| epoch   4 step     7900 |   1591 batches | lr 5.22e-05 | ms/batch 29.58 | loss  3.20 | avg loss  2.81 | ppl 16.69 - 2025-05-28 13:50:09,551 - log
| epoch   4 step     8000 |   1691 batches | lr 5.02e-05 | ms/batch 29.78 | loss  2.26 | avg loss  2.76 | ppl 15.83 - 2025-05-28 13:50:12,529 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8000.ckpt - 2025-05-28 13:50:12,529 - log
eval samples: 0, loss: 1.3486045598983765 - 2025-05-28 13:50:12,548 - log
eval samples: 100, loss: 2.01627254486084 - 2025-05-28 13:50:14,035 - log
eval samples: 200, loss: 1.568535566329956 - 2025-05-28 13:50:15,520 - log
eval samples: 300, loss: 1.845246434211731 - 2025-05-28 13:50:17,025 - log
eval samples: 400, loss: 0.6121398210525513 - 2025-05-28 13:50:18,514 - log
average loss: +1.4211690285476424 - 2025-05-28 13:50:19,495 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:50:19,495 - log
| Eval   4 at step     8000 | time:  6.96s | valid loss  1.42 | valid ppl  4.14 | best ppl  4.14  - 2025-05-28 13:50:19,495 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:50:19,495 - log
| epoch   4 step     8100 |   1791 batches | lr 4.82e-05 | ms/batch 99.26 | loss  2.74 | avg loss  2.78 | ppl 16.19 - 2025-05-28 13:50:22,456 - log
| epoch   4 step     8200 |   1891 batches | lr 4.62e-05 | ms/batch 29.58 | loss  2.60 | avg loss  2.78 | ppl 16.12 - 2025-05-28 13:50:25,414 - log
| epoch   4 step     8300 |   1991 batches | lr 4.42e-05 | ms/batch 29.57 | loss  2.45 | avg loss  2.78 | ppl 16.05 - 2025-05-28 13:50:28,371 - log
| epoch   4 step     8400 |   2091 batches | lr 4.22e-05 | ms/batch 29.45 | loss  3.04 | avg loss  2.78 | ppl 16.06 - 2025-05-28 13:50:31,317 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8412.pkl - 2025-05-28 13:50:31,646 - log
start to train the model................ 5 - 2025-05-28 13:50:33,191 - log
| epoch   5 step     8500 |     88 batches | lr 4.02e-05 | ms/batch 26.19 | loss  2.61 | avg loss  2.68 | ppl 14.54 - 2025-05-28 13:50:35,810 - log
| epoch   5 step     8600 |    188 batches | lr 3.82e-05 | ms/batch 29.43 | loss  2.95 | avg loss  2.78 | ppl 16.19 - 2025-05-28 13:50:38,754 - log
| epoch   5 step     8700 |    288 batches | lr 3.62e-05 | ms/batch 29.52 | loss  2.64 | avg loss  2.74 | ppl 15.44 - 2025-05-28 13:50:41,706 - log
| epoch   5 step     8800 |    388 batches | lr 3.42e-05 | ms/batch 29.92 | loss  3.25 | avg loss  2.81 | ppl 16.65 - 2025-05-28 13:50:44,698 - log
| epoch   5 step     8900 |    488 batches | lr 3.23e-05 | ms/batch 29.80 | loss  3.06 | avg loss  2.77 | ppl 16.03 - 2025-05-28 13:50:47,678 - log
| epoch   5 step     9000 |    588 batches | lr 3.03e-05 | ms/batch 29.74 | loss  3.25 | avg loss  2.73 | ppl 15.36 - 2025-05-28 13:50:50,652 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.9000.ckpt - 2025-05-28 13:50:50,652 - log
| epoch   5 step     9100 |    688 batches | lr 2.83e-05 | ms/batch 29.72 | loss  3.27 | avg loss  2.78 | ppl 16.15 - 2025-05-28 13:50:53,624 - log
| epoch   5 step     9200 |    788 batches | lr 2.63e-05 | ms/batch 29.84 | loss  3.38 | avg loss  2.75 | ppl 15.57 - 2025-05-28 13:50:56,608 - log
| epoch   5 step     9300 |    888 batches | lr 2.43e-05 | ms/batch 29.97 | loss  2.50 | avg loss  2.72 | ppl 15.24 - 2025-05-28 13:50:59,606 - log
| epoch   5 step     9400 |    988 batches | lr 2.23e-05 | ms/batch 29.78 | loss  2.28 | avg loss  2.77 | ppl 15.93 - 2025-05-28 13:51:02,585 - log
| epoch   5 step     9500 |   1088 batches | lr 2.03e-05 | ms/batch 29.63 | loss  2.49 | avg loss  2.71 | ppl 14.99 - 2025-05-28 13:51:05,548 - log
| epoch   5 step     9600 |   1188 batches | lr 1.83e-05 | ms/batch 29.66 | loss  2.86 | avg loss  2.79 | ppl 16.31 - 2025-05-28 13:51:08,514 - log
| epoch   5 step     9700 |   1288 batches | lr 1.63e-05 | ms/batch 29.60 | loss  2.90 | avg loss  2.78 | ppl 16.11 - 2025-05-28 13:51:11,475 - log
| epoch   5 step     9800 |   1388 batches | lr 1.43e-05 | ms/batch 29.64 | loss  3.05 | avg loss  2.73 | ppl 15.26 - 2025-05-28 13:51:14,439 - log
| epoch   5 step     9900 |   1488 batches | lr 1.23e-05 | ms/batch 30.37 | loss  3.12 | avg loss  2.78 | ppl 16.12 - 2025-05-28 13:51:17,476 - log
| epoch   5 step    10000 |   1588 batches | lr 1.03e-05 | ms/batch 30.25 | loss  2.99 | avg loss  2.73 | ppl 15.36 - 2025-05-28 13:51:20,501 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10000.ckpt - 2025-05-28 13:51:20,502 - log
eval samples: 0, loss: 0.7234548330307007 - 2025-05-28 13:51:20,520 - log
eval samples: 100, loss: 1.7701244354248047 - 2025-05-28 13:51:22,051 - log
eval samples: 200, loss: 1.37086820602417 - 2025-05-28 13:51:23,556 - log
eval samples: 300, loss: 1.7092915773391724 - 2025-05-28 13:51:25,063 - log
eval samples: 400, loss: 1.8187055587768555 - 2025-05-28 13:51:26,588 - log
average loss: +1.4377201140820342 - 2025-05-28 13:51:27,563 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:51:27,563 - log
| Eval   5 at step    10000 | time:  7.06s | valid loss  1.44 | valid ppl  4.21 | best ppl  4.21  - 2025-05-28 13:51:27,563 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:51:27,563 - log
| epoch   5 step    10100 |   1688 batches | lr 8.29e-06 | ms/batch 100.45 | loss  2.46 | avg loss  2.77 | ppl 15.95 - 2025-05-28 13:51:30,547 - log
| epoch   5 step    10200 |   1788 batches | lr 6.29e-06 | ms/batch 29.79 | loss  2.34 | avg loss  2.82 | ppl 16.86 - 2025-05-28 13:51:33,526 - log
| epoch   5 step    10300 |   1888 batches | lr 4.29e-06 | ms/batch 29.62 | loss  2.76 | avg loss  2.73 | ppl 15.33 - 2025-05-28 13:51:36,488 - log
| epoch   5 step    10400 |   1988 batches | lr 2.3e-06 | ms/batch 29.57 | loss  2.14 | avg loss  2.79 | ppl 16.34 - 2025-05-28 13:51:39,445 - log
| epoch   5 step    10500 |   2088 batches | lr 3e-07 | ms/batch 29.53 | loss  2.91 | avg loss  2.72 | ppl 15.24 - 2025-05-28 13:51:42,398 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10515.pkl - 2025-05-28 13:51:42,828 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 13:51:44,318 - log
End of training - 2025-05-28 13:51:44,318 - log
ms/batch 32.98 - 2025-05-28 13:51:44,318 - log
==================================================================================================== - 2025-05-28 19:18:40,516 - log
        - random_seed : 2025 - 2025-05-28 19:18:40,516 - log
        - lr : 0.0002 - 2025-05-28 19:18:40,516 - log
        - weight_decay : 0.01 - 2025-05-28 19:18:40,516 - log
        - correct_bias : False - 2025-05-28 19:18:40,516 - log
        - adam_epislon : 1e-06 - 2025-05-28 19:18:40,516 - log
        - no_decay_bias : False - 2025-05-28 19:18:40,516 - log
        - adam_beta1 : 0.9 - 2025-05-28 19:18:40,516 - log
        - adam_beta2 : 0.999 - 2025-05-28 19:18:40,516 - log
        - scheduler : linear - 2025-05-28 19:18:40,516 - log
        - max_step : None - 2025-05-28 19:18:40,516 - log
        - max_epoch : 5 - 2025-05-28 19:18:40,516 - log
        - warmup_step : 500 - 2025-05-28 19:18:40,516 - log
        - i_steps : 0 - 2025-05-28 19:18:40,516 - log
        - i_lrs : 0.00025 - 2025-05-28 19:18:40,516 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 19:18:40,516 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 19:18:40,516 - log
        - train_batch_size : 2 - 2025-05-28 19:18:40,516 - log
        - valid_batch_size : 1 - 2025-05-28 19:18:40,516 - log
        - grad_acc : 2 - 2025-05-28 19:18:40,516 - log
        - clip : 0.0 - 2025-05-28 19:18:40,516 - log
        - seq_len : 64 - 2025-05-28 19:18:40,516 - log
        - model_card : gpt2.sm - 2025-05-28 19:18:40,516 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 19:18:40,516 - log
        - fp16 : False - 2025-05-28 19:18:40,516 - log
        - log_interval : 100 - 2025-05-28 19:18:40,517 - log
        - eval_interval : 2000 - 2025-05-28 19:18:40,517 - log
        - save_interval : 1000 - 2025-05-28 19:18:40,517 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 19:18:40,517 - log
        - lora_dim : 4 - 2025-05-28 19:18:40,517 - log
        - lora_alpha : 32 - 2025-05-28 19:18:40,517 - log
        - obj : clm - 2025-05-28 19:18:40,517 - log
        - lora_dropout : 0.1 - 2025-05-28 19:18:40,517 - log
        - label_smooth : 0.1 - 2025-05-28 19:18:40,517 - log
        - roll_interval : -1 - 2025-05-28 19:18:40,517 - log
        - roll_lr : 1e-05 - 2025-05-28 19:18:40,517 - log
        - roll_step : 100 - 2025-05-28 19:18:40,517 - log
        - eval_epoch : 1 - 2025-05-28 19:18:40,517 - log
        - device : cuda - 2025-05-28 19:18:40,517 - log
==================================================================================================== - 2025-05-28 19:18:40,517 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 19:18:40,517 - log
loading model pretrained weight. - 2025-05-28 19:18:40,946 - log
set max_step: 105155 - 2025-05-28 19:18:42,488 - log
start to train the model................ 1 - 2025-05-28 19:18:42,488 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 37.22 | loss  5.34 | avg loss  5.90 | ppl 363.80 - 2025-05-28 19:18:46,211 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 30.99 | loss  3.76 | avg loss  4.82 | ppl 124.58 - 2025-05-28 19:18:49,311 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 30.73 | loss  3.23 | avg loss  3.60 | ppl 36.42 - 2025-05-28 19:18:52,384 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 30.84 | loss  2.83 | avg loss  3.46 | ppl 31.68 - 2025-05-28 19:18:55,468 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 30.65 | loss  3.22 | avg loss  3.27 | ppl 26.28 - 2025-05-28 19:18:58,534 - log
| epoch   1 step      600 |    600 batches | lr 0.0002 | ms/batch 30.51 | loss  2.93 | avg loss  3.18 | ppl 24.13 - 2025-05-28 19:19:01,585 - log
| epoch   1 step      700 |    700 batches | lr 0.0002 | ms/batch 30.34 | loss  3.51 | avg loss  3.17 | ppl 23.74 - 2025-05-28 19:19:04,619 - log
| epoch   1 step      800 |    800 batches | lr 0.000199 | ms/batch 30.14 | loss  2.95 | avg loss  3.14 | ppl 23.15 - 2025-05-28 19:19:07,634 - log
| epoch   1 step      900 |    900 batches | lr 0.000199 | ms/batch 30.00 | loss  4.15 | avg loss  3.03 | ppl 20.74 - 2025-05-28 19:19:10,634 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000199 | ms/batch 30.43 | loss  3.75 | avg loss  3.06 | ppl 21.42 - 2025-05-28 19:19:13,678 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 19:19:13,678 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000199 | ms/batch 30.65 | loss  2.85 | avg loss  3.02 | ppl 20.47 - 2025-05-28 19:19:16,743 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000199 | ms/batch 30.62 | loss  3.04 | avg loss  3.05 | ppl 21.06 - 2025-05-28 19:19:19,806 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000198 | ms/batch 30.99 | loss  3.55 | avg loss  3.09 | ppl 21.90 - 2025-05-28 19:19:22,905 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000198 | ms/batch 31.06 | loss  3.40 | avg loss  3.00 | ppl 20.13 - 2025-05-28 19:19:26,011 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000198 | ms/batch 30.91 | loss  2.97 | avg loss  3.00 | ppl 20.18 - 2025-05-28 19:19:29,103 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000198 | ms/batch 30.87 | loss  2.73 | avg loss  3.00 | ppl 20.16 - 2025-05-28 19:19:32,190 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000198 | ms/batch 31.08 | loss  2.84 | avg loss  2.95 | ppl 19.19 - 2025-05-28 19:19:35,299 - log
| epoch   1 step     1800 |   1800 batches | lr 0.000198 | ms/batch 30.73 | loss  2.42 | avg loss  2.96 | ppl 19.28 - 2025-05-28 19:19:38,372 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000197 | ms/batch 30.71 | loss  2.65 | avg loss  2.94 | ppl 18.91 - 2025-05-28 19:19:41,443 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000197 | ms/batch 30.70 | loss  2.68 | avg loss  2.93 | ppl 18.77 - 2025-05-28 19:19:44,514 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 19:19:44,514 - log
eval samples: 0, loss: 1.3822596073150635 - 2025-05-28 19:19:44,536 - log
eval samples: 100, loss: 1.5121967792510986 - 2025-05-28 19:19:46,070 - log
eval samples: 200, loss: 2.9101884365081787 - 2025-05-28 19:19:47,606 - log
eval samples: 300, loss: 2.820584297180176 - 2025-05-28 19:19:49,140 - log
eval samples: 400, loss: 0.9913057684898376 - 2025-05-28 19:19:50,668 - log
eval samples: 500, loss: 1.1096926927566528 - 2025-05-28 19:19:52,204 - log
eval samples: 600, loss: 1.3458002805709839 - 2025-05-28 19:19:53,735 - log
eval samples: 700, loss: 1.1560392379760742 - 2025-05-28 19:19:55,265 - log
eval samples: 800, loss: 1.4899373054504395 - 2025-05-28 19:19:56,795 - log
eval samples: 900, loss: 1.8548023700714111 - 2025-05-28 19:19:58,330 - log
eval samples: 1000, loss: 1.792708396911621 - 2025-05-28 19:19:59,864 - log
eval samples: 1100, loss: 1.5347213745117188 - 2025-05-28 19:20:01,488 - log
eval samples: 1200, loss: 0.9825953245162964 - 2025-05-28 19:20:03,022 - log
eval samples: 1300, loss: 1.4695066213607788 - 2025-05-28 19:20:04,575 - log
eval samples: 1400, loss: 1.7279644012451172 - 2025-05-28 19:20:06,129 - log
eval samples: 1500, loss: 1.2965810298919678 - 2025-05-28 19:20:07,665 - log
eval samples: 1600, loss: 1.2556654214859009 - 2025-05-28 19:20:09,198 - log
eval samples: 1700, loss: 1.5123343467712402 - 2025-05-28 19:20:10,732 - log
eval samples: 1800, loss: 1.103352665901184 - 2025-05-28 19:20:12,258 - log
eval samples: 1900, loss: 1.301106572151184 - 2025-05-28 19:20:13,796 - log
eval samples: 2000, loss: 1.9816920757293701 - 2025-05-28 19:20:15,321 - log
eval samples: 2100, loss: 1.654858112335205 - 2025-05-28 19:20:16,866 - log
eval samples: 2200, loss: 1.1307365894317627 - 2025-05-28 19:20:18,396 - log
eval samples: 2300, loss: 1.1154261827468872 - 2025-05-28 19:20:19,922 - log
eval samples: 2400, loss: 1.7295726537704468 - 2025-05-28 19:20:21,447 - log
eval samples: 2500, loss: 1.8547221422195435 - 2025-05-28 19:20:22,968 - log
eval samples: 2600, loss: 1.7775551080703735 - 2025-05-28 19:20:24,496 - log
eval samples: 2700, loss: 1.1659530401229858 - 2025-05-28 19:20:26,016 - log
eval samples: 2800, loss: 1.3570083379745483 - 2025-05-28 19:20:27,533 - log
==================================================================================================== - 2025-05-28 19:21:26,599 - log
        - random_seed : 2025 - 2025-05-28 19:21:26,599 - log
        - lr : 0.0002 - 2025-05-28 19:21:26,599 - log
        - weight_decay : 0.01 - 2025-05-28 19:21:26,599 - log
        - correct_bias : False - 2025-05-28 19:21:26,599 - log
        - adam_epislon : 1e-06 - 2025-05-28 19:21:26,599 - log
        - no_decay_bias : False - 2025-05-28 19:21:26,599 - log
        - adam_beta1 : 0.9 - 2025-05-28 19:21:26,599 - log
        - adam_beta2 : 0.999 - 2025-05-28 19:21:26,599 - log
        - scheduler : linear - 2025-05-28 19:21:26,599 - log
        - max_step : None - 2025-05-28 19:21:26,599 - log
        - max_epoch : 5 - 2025-05-28 19:21:26,599 - log
        - warmup_step : 500 - 2025-05-28 19:21:26,599 - log
        - i_steps : 0 - 2025-05-28 19:21:26,599 - log
        - i_lrs : 0.00025 - 2025-05-28 19:21:26,599 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 19:21:26,599 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 19:21:26,599 - log
        - train_batch_size : 8 - 2025-05-28 19:21:26,599 - log
        - valid_batch_size : 4 - 2025-05-28 19:21:26,599 - log
        - grad_acc : 2 - 2025-05-28 19:21:26,599 - log
        - clip : 0.0 - 2025-05-28 19:21:26,599 - log
        - seq_len : 64 - 2025-05-28 19:21:26,599 - log
        - model_card : gpt2.sm - 2025-05-28 19:21:26,599 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 19:21:26,599 - log
        - fp16 : False - 2025-05-28 19:21:26,599 - log
        - log_interval : 100 - 2025-05-28 19:21:26,599 - log
        - eval_interval : 2000 - 2025-05-28 19:21:26,599 - log
        - save_interval : 1000 - 2025-05-28 19:21:26,599 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 19:21:26,599 - log
        - lora_dim : 4 - 2025-05-28 19:21:26,600 - log
        - lora_alpha : 32 - 2025-05-28 19:21:26,600 - log
        - obj : clm - 2025-05-28 19:21:26,600 - log
        - lora_dropout : 0.1 - 2025-05-28 19:21:26,600 - log
        - label_smooth : 0.1 - 2025-05-28 19:21:26,600 - log
        - roll_interval : -1 - 2025-05-28 19:21:26,600 - log
        - roll_lr : 1e-05 - 2025-05-28 19:21:26,600 - log
        - roll_step : 100 - 2025-05-28 19:21:26,600 - log
        - eval_epoch : 1 - 2025-05-28 19:21:26,600 - log
        - device : cuda - 2025-05-28 19:21:26,600 - log
==================================================================================================== - 2025-05-28 19:21:26,600 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 19:21:26,600 - log
loading model pretrained weight. - 2025-05-28 19:21:27,005 - log
set max_step: 26290 - 2025-05-28 19:21:27,972 - log
start to train the model................ 1 - 2025-05-28 19:21:27,972 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 56.65 | loss  5.46 | avg loss  5.90 | ppl 365.77 - 2025-05-28 19:21:33,638 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 50.48 | loss  4.06 | avg loss  4.61 | ppl 100.71 - 2025-05-28 19:21:38,686 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 50.30 | loss  3.63 | avg loss  3.47 | ppl 32.06 - 2025-05-28 19:21:43,716 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 50.42 | loss  3.31 | avg loss  3.25 | ppl 25.80 - 2025-05-28 19:21:48,759 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 50.21 | loss  2.97 | avg loss  3.14 | ppl 23.01 - 2025-05-28 19:21:53,781 - log
| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 50.16 | loss  3.62 | avg loss  3.11 | ppl 22.36 - 2025-05-28 19:21:58,797 - log
| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 50.59 | loss  3.22 | avg loss  3.05 | ppl 21.18 - 2025-05-28 19:22:03,856 - log
| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 50.46 | loss  3.08 | avg loss  2.98 | ppl 19.71 - 2025-05-28 19:22:08,902 - log
| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 50.29 | loss  2.82 | avg loss  2.98 | ppl 19.76 - 2025-05-28 19:22:13,932 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 50.64 | loss  2.89 | avg loss  3.00 | ppl 20.17 - 2025-05-28 19:22:18,996 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 19:22:18,997 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 51.30 | loss  3.02 | avg loss  2.97 | ppl 19.52 - 2025-05-28 19:22:24,127 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 51.11 | loss  2.61 | avg loss  2.99 | ppl 19.82 - 2025-05-28 19:22:29,239 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 51.13 | loss  3.27 | avg loss  2.92 | ppl 18.49 - 2025-05-28 19:22:34,352 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 51.20 | loss  2.86 | avg loss  2.89 | ppl 17.92 - 2025-05-28 19:22:39,472 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 50.85 | loss  2.63 | avg loss  2.89 | ppl 18.07 - 2025-05-28 19:22:44,557 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 50.51 | loss  2.60 | avg loss  2.92 | ppl 18.60 - 2025-05-28 19:22:49,608 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 50.69 | loss  3.14 | avg loss  2.89 | ppl 17.91 - 2025-05-28 19:22:54,678 - log
| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 50.83 | loss  3.04 | avg loss  2.89 | ppl 17.95 - 2025-05-28 19:22:59,761 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 50.71 | loss  3.03 | avg loss  2.89 | ppl 17.91 - 2025-05-28 19:23:04,832 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 50.64 | loss  2.90 | avg loss  2.87 | ppl 17.58 - 2025-05-28 19:23:09,897 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 19:23:09,897 - log
eval samples: 0, loss: 1.3926268815994263 - 2025-05-28 19:23:09,928 - log
eval samples: 100, loss: 1.6503539085388184 - 2025-05-28 19:23:12,362 - log
eval samples: 200, loss: 1.3838289976119995 - 2025-05-28 19:23:14,794 - log
eval samples: 300, loss: 1.0273760557174683 - 2025-05-28 19:23:17,222 - log
eval samples: 400, loss: 1.2176626920700073 - 2025-05-28 19:23:19,657 - log
eval samples: 500, loss: 1.4640138149261475 - 2025-05-28 19:23:22,103 - log
eval samples: 600, loss: 1.8103233575820923 - 2025-05-28 19:23:24,568 - log
eval samples: 700, loss: 1.4132297039031982 - 2025-05-28 19:23:27,024 - log
eval samples: 800, loss: 1.2258838415145874 - 2025-05-28 19:23:29,483 - log
eval samples: 900, loss: 1.2368122339248657 - 2025-05-28 19:23:31,945 - log
eval samples: 1000, loss: 1.3854267597198486 - 2025-05-28 19:23:34,405 - log
eval samples: 1100, loss: 1.8193624019622803 - 2025-05-28 19:23:36,860 - log
average loss: +1.4883607527981066 - 2025-05-28 19:23:38,517 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:23:38,517 - log
| Eval   1 at step     2000 | time: 28.62s | valid loss  1.49 | valid ppl  4.43 | best ppl  4.43  - 2025-05-28 19:23:38,518 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:23:38,518 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 337.20 | loss  3.16 | avg loss  2.87 | ppl 17.68 - 2025-05-28 19:23:43,617 - log
| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 51.06 | loss  2.76 | avg loss  2.86 | ppl 17.50 - 2025-05-28 19:23:48,723 - log
| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 51.19 | loss  3.29 | avg loss  2.88 | ppl 17.73 - 2025-05-28 19:23:53,843 - log
| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 51.14 | loss  2.56 | avg loss  2.83 | ppl 17.02 - 2025-05-28 19:23:58,957 - log
| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 51.03 | loss  3.35 | avg loss  2.88 | ppl 17.81 - 2025-05-28 19:24:04,061 - log
| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 50.89 | loss  2.60 | avg loss  2.85 | ppl 17.22 - 2025-05-28 19:24:09,151 - log
| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 50.43 | loss  2.82 | avg loss  2.83 | ppl 17.01 - 2025-05-28 19:24:14,194 - log
| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 50.94 | loss  2.85 | avg loss  2.84 | ppl 17.19 - 2025-05-28 19:24:19,288 - log
| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 50.95 | loss  2.77 | avg loss  2.83 | ppl 16.96 - 2025-05-28 19:24:24,383 - log
| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 51.07 | loss  2.86 | avg loss  2.81 | ppl 16.60 - 2025-05-28 19:24:29,490 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.3000.ckpt - 2025-05-28 19:24:29,491 - log
| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 50.70 | loss  2.77 | avg loss  2.85 | ppl 17.35 - 2025-05-28 19:24:34,560 - log
| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 50.32 | loss  3.05 | avg loss  2.84 | ppl 17.10 - 2025-05-28 19:24:39,592 - log
| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 50.36 | loss  2.92 | avg loss  2.82 | ppl 16.80 - 2025-05-28 19:24:44,629 - log
| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 50.63 | loss  2.67 | avg loss  2.80 | ppl 16.44 - 2025-05-28 19:24:49,692 - log
| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 50.74 | loss  3.24 | avg loss  2.81 | ppl 16.55 - 2025-05-28 19:24:54,766 - log
| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 50.79 | loss  2.80 | avg loss  2.78 | ppl 16.18 - 2025-05-28 19:24:59,846 - log
| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 50.76 | loss  2.61 | avg loss  2.79 | ppl 16.21 - 2025-05-28 19:25:04,922 - log
| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 50.80 | loss  2.93 | avg loss  2.81 | ppl 16.55 - 2025-05-28 19:25:10,002 - log
| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 51.13 | loss  2.78 | avg loss  2.81 | ppl 16.58 - 2025-05-28 19:25:15,115 - log
| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 50.45 | loss  2.63 | avg loss  2.81 | ppl 16.60 - 2025-05-28 19:25:20,161 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4000.ckpt - 2025-05-28 19:25:20,161 - log
eval samples: 0, loss: 1.570698618888855 - 2025-05-28 19:25:20,191 - log
eval samples: 100, loss: 1.4293571710586548 - 2025-05-28 19:25:22,651 - log
eval samples: 200, loss: 1.0918924808502197 - 2025-05-28 19:25:25,102 - log
eval samples: 300, loss: 1.238788366317749 - 2025-05-28 19:25:27,571 - log
eval samples: 400, loss: 1.76638925075531 - 2025-05-28 19:25:30,022 - log
eval samples: 500, loss: 1.1833823919296265 - 2025-05-28 19:25:32,478 - log
eval samples: 600, loss: 1.521405577659607 - 2025-05-28 19:25:34,925 - log
eval samples: 700, loss: 1.5805954933166504 - 2025-05-28 19:25:37,381 - log
eval samples: 800, loss: 1.7680714130401611 - 2025-05-28 19:25:39,836 - log
eval samples: 900, loss: 1.6501818895339966 - 2025-05-28 19:25:42,294 - log
eval samples: 1000, loss: 1.4501891136169434 - 2025-05-28 19:25:44,749 - log
eval samples: 1100, loss: 1.2959229946136475 - 2025-05-28 19:25:47,197 - log
average loss: +1.4161751587709335 - 2025-05-28 19:25:48,834 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:25:48,834 - log
| Eval   2 at step     4000 | time: 28.67s | valid loss  1.42 | valid ppl  4.12 | best ppl  4.12  - 2025-05-28 19:25:48,834 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:25:48,834 - log
| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 337.60 | loss  2.69 | avg loss  2.82 | ppl 16.86 - 2025-05-28 19:25:53,921 - log
| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 50.46 | loss  2.47 | avg loss  2.79 | ppl 16.34 - 2025-05-28 19:25:58,968 - log
| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 50.37 | loss  2.88 | avg loss  2.75 | ppl 15.70 - 2025-05-28 19:26:04,005 - log
| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 50.40 | loss  2.51 | avg loss  2.80 | ppl 16.44 - 2025-05-28 19:26:09,046 - log
| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 50.80 | loss  3.27 | avg loss  2.80 | ppl 16.42 - 2025-05-28 19:26:14,126 - log
| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 50.76 | loss  2.80 | avg loss  2.79 | ppl 16.28 - 2025-05-28 19:26:19,203 - log
| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 50.70 | loss  2.83 | avg loss  2.77 | ppl 15.95 - 2025-05-28 19:26:24,273 - log
| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 50.51 | loss  2.67 | avg loss  2.77 | ppl 15.91 - 2025-05-28 19:26:29,324 - log
| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 51.05 | loss  2.70 | avg loss  2.78 | ppl 16.08 - 2025-05-28 19:26:34,429 - log
| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 50.97 | loss  2.83 | avg loss  2.77 | ppl 16.00 - 2025-05-28 19:26:39,527 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5000.ckpt - 2025-05-28 19:26:39,527 - log
| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 50.74 | loss  2.84 | avg loss  2.73 | ppl 15.39 - 2025-05-28 19:26:44,601 - log
| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 50.97 | loss  2.74 | avg loss  2.79 | ppl 16.30 - 2025-05-28 19:26:49,699 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5258.pkl - 2025-05-28 19:26:52,611 - log
start to train the model................ 2 - 2025-05-28 19:26:54,445 - log
| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 21.78 | loss  2.57 | avg loss  2.78 | ppl 16.10 - 2025-05-28 19:26:56,624 - log
| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 51.12 | loss  2.59 | avg loss  2.74 | ppl 15.41 - 2025-05-28 19:27:01,736 - log
| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 51.37 | loss  2.54 | avg loss  2.77 | ppl 15.92 - 2025-05-28 19:27:06,873 - log
| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 51.06 | loss  2.62 | avg loss  2.74 | ppl 15.55 - 2025-05-28 19:27:11,980 - log
| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 51.18 | loss  2.45 | avg loss  2.73 | ppl 15.35 - 2025-05-28 19:27:17,098 - log
| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 50.87 | loss  2.53 | avg loss  2.72 | ppl 15.23 - 2025-05-28 19:27:22,186 - log
| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 50.54 | loss  2.92 | avg loss  2.77 | ppl 16.03 - 2025-05-28 19:27:27,240 - log
| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 50.52 | loss  2.74 | avg loss  2.77 | ppl 15.94 - 2025-05-28 19:27:32,293 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6000.ckpt - 2025-05-28 19:27:32,293 - log
eval samples: 0, loss: 1.1511974334716797 - 2025-05-28 19:27:32,323 - log
eval samples: 100, loss: 1.4397790431976318 - 2025-05-28 19:27:34,775 - log
eval samples: 200, loss: 1.4672889709472656 - 2025-05-28 19:27:37,234 - log
eval samples: 300, loss: 1.5839797258377075 - 2025-05-28 19:27:39,679 - log
eval samples: 400, loss: 1.2455517053604126 - 2025-05-28 19:27:42,133 - log
eval samples: 500, loss: 1.3787482976913452 - 2025-05-28 19:27:44,599 - log
eval samples: 600, loss: 1.687854528427124 - 2025-05-28 19:27:47,051 - log
eval samples: 700, loss: 1.3891116380691528 - 2025-05-28 19:27:49,494 - log
eval samples: 800, loss: 1.0374505519866943 - 2025-05-28 19:27:51,950 - log
eval samples: 900, loss: 1.4449728727340698 - 2025-05-28 19:27:54,410 - log
eval samples: 1000, loss: 1.4413506984710693 - 2025-05-28 19:27:56,863 - log
eval samples: 1100, loss: 1.1835432052612305 - 2025-05-28 19:27:59,308 - log
average loss: +1.3751063819413316 - 2025-05-28 19:28:00,949 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:28:00,949 - log
| Eval   3 at step     6000 | time: 28.65s | valid loss  1.38 | valid ppl  3.96 | best ppl  3.96  - 2025-05-28 19:28:00,949 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:28:00,949 - log
| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 337.56 | loss  2.67 | avg loss  2.75 | ppl 15.60 - 2025-05-28 19:28:06,049 - log
| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 51.17 | loss  2.65 | avg loss  2.71 | ppl 15.06 - 2025-05-28 19:28:11,166 - log
| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 51.31 | loss  2.76 | avg loss  2.75 | ppl 15.64 - 2025-05-28 19:28:16,298 - log
| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 51.27 | loss  3.09 | avg loss  2.75 | ppl 15.72 - 2025-05-28 19:28:21,425 - log
| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 51.34 | loss  2.82 | avg loss  2.75 | ppl 15.69 - 2025-05-28 19:28:26,560 - log
| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 51.33 | loss  2.96 | avg loss  2.70 | ppl 14.89 - 2025-05-28 19:28:31,693 - log
| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 50.93 | loss  2.64 | avg loss  2.75 | ppl 15.70 - 2025-05-28 19:28:36,787 - log
| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 50.91 | loss  2.90 | avg loss  2.73 | ppl 15.37 - 2025-05-28 19:28:41,878 - log
| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 50.83 | loss  2.80 | avg loss  2.72 | ppl 15.15 - 2025-05-28 19:28:46,962 - log
| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 50.73 | loss  2.62 | avg loss  2.75 | ppl 15.68 - 2025-05-28 19:28:52,036 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.7000.ckpt - 2025-05-28 19:28:52,036 - log
| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 50.91 | loss  2.66 | avg loss  2.73 | ppl 15.33 - 2025-05-28 19:28:57,128 - log
| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 50.89 | loss  3.10 | avg loss  2.72 | ppl 15.21 - 2025-05-28 19:29:02,217 - log
| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 51.22 | loss  2.61 | avg loss  2.75 | ppl 15.62 - 2025-05-28 19:29:07,340 - log
| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 51.86 | loss  2.64 | avg loss  2.77 | ppl 15.92 - 2025-05-28 19:29:12,526 - log
| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 51.14 | loss  2.79 | avg loss  2.74 | ppl 15.56 - 2025-05-28 19:29:17,640 - log
| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 51.03 | loss  2.39 | avg loss  2.70 | ppl 14.92 - 2025-05-28 19:29:22,743 - log
| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 50.96 | loss  2.68 | avg loss  2.73 | ppl 15.40 - 2025-05-28 19:29:27,839 - log
| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 50.96 | loss  2.71 | avg loss  2.75 | ppl 15.57 - 2025-05-28 19:29:32,936 - log
| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 51.13 | loss  2.68 | avg loss  2.73 | ppl 15.39 - 2025-05-28 19:29:38,050 - log
| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 50.88 | loss  2.74 | avg loss  2.76 | ppl 15.80 - 2025-05-28 19:29:43,138 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8000.ckpt - 2025-05-28 19:29:43,139 - log
eval samples: 0, loss: 1.3718620538711548 - 2025-05-28 19:29:43,169 - log
eval samples: 100, loss: 1.4771904945373535 - 2025-05-28 19:29:45,618 - log
eval samples: 200, loss: 1.0191090106964111 - 2025-05-28 19:29:48,062 - log
eval samples: 300, loss: 1.2659423351287842 - 2025-05-28 19:29:50,508 - log
eval samples: 400, loss: 1.1190104484558105 - 2025-05-28 19:29:52,976 - log
eval samples: 500, loss: 1.3339935541152954 - 2025-05-28 19:29:55,431 - log
eval samples: 600, loss: 1.8453547954559326 - 2025-05-28 19:29:57,878 - log
eval samples: 700, loss: 1.3890070915222168 - 2025-05-28 19:30:00,345 - log
eval samples: 800, loss: 1.5352460145950317 - 2025-05-28 19:30:02,792 - log
eval samples: 900, loss: 1.3466159105300903 - 2025-05-28 19:30:05,243 - log
eval samples: 1000, loss: 1.5590624809265137 - 2025-05-28 19:30:07,683 - log
eval samples: 1100, loss: 1.1091822385787964 - 2025-05-28 19:30:10,121 - log
average loss: +1.3563496251016447 - 2025-05-28 19:30:11,754 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:30:11,754 - log
| Eval   4 at step     8000 | time: 28.61s | valid loss  1.36 | valid ppl  3.88 | best ppl  3.88  - 2025-05-28 19:30:11,754 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:30:11,754 - log
| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 336.76 | loss  2.91 | avg loss  2.71 | ppl 15.01 - 2025-05-28 19:30:16,815 - log
| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 50.68 | loss  2.75 | avg loss  2.71 | ppl 15.04 - 2025-05-28 19:30:21,884 - log
| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 50.81 | loss  2.87 | avg loss  2.72 | ppl 15.22 - 2025-05-28 19:30:26,965 - log
| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 51.15 | loss  2.36 | avg loss  2.72 | ppl 15.12 - 2025-05-28 19:30:32,080 - log
| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 50.99 | loss  2.87 | avg loss  2.76 | ppl 15.78 - 2025-05-28 19:30:37,179 - log
| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 50.93 | loss  2.62 | avg loss  2.73 | ppl 15.38 - 2025-05-28 19:30:42,272 - log
| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 51.06 | loss  2.82 | avg loss  2.72 | ppl 15.23 - 2025-05-28 19:30:47,378 - log
| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 50.89 | loss  2.66 | avg loss  2.69 | ppl 14.73 - 2025-05-28 19:30:52,468 - log
| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 50.82 | loss  2.42 | avg loss  2.70 | ppl 14.91 - 2025-05-28 19:30:57,550 - log
| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 50.83 | loss  2.82 | avg loss  2.72 | ppl 15.23 - 2025-05-28 19:31:02,633 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.9000.ckpt - 2025-05-28 19:31:02,634 - log
| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 51.03 | loss  2.75 | avg loss  2.68 | ppl 14.60 - 2025-05-28 19:31:07,737 - log
| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 51.03 | loss  2.26 | avg loss  2.73 | ppl 15.29 - 2025-05-28 19:31:12,840 - log
| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 51.08 | loss  2.60 | avg loss  2.72 | ppl 15.11 - 2025-05-28 19:31:17,948 - log
| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 51.11 | loss  2.65 | avg loss  2.71 | ppl 14.98 - 2025-05-28 19:31:23,060 - log
| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 51.18 | loss  2.83 | avg loss  2.75 | ppl 15.63 - 2025-05-28 19:31:28,178 - log
| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 50.63 | loss  2.49 | avg loss  2.73 | ppl 15.36 - 2025-05-28 19:31:33,242 - log
| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 50.79 | loss  2.35 | avg loss  2.72 | ppl 15.25 - 2025-05-28 19:31:38,320 - log
| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 50.63 | loss  2.53 | avg loss  2.68 | ppl 14.55 - 2025-05-28 19:31:43,384 - log
| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 51.34 | loss  3.05 | avg loss  2.74 | ppl 15.53 - 2025-05-28 19:31:48,517 - log
| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 51.11 | loss  2.55 | avg loss  2.71 | ppl 15.10 - 2025-05-28 19:31:53,628 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10000.ckpt - 2025-05-28 19:31:53,629 - log
eval samples: 0, loss: 1.1700328588485718 - 2025-05-28 19:31:53,659 - log
eval samples: 100, loss: 1.3764581680297852 - 2025-05-28 19:31:56,124 - log
eval samples: 200, loss: 1.7952240705490112 - 2025-05-28 19:31:58,582 - log
eval samples: 300, loss: 1.4114738702774048 - 2025-05-28 19:32:01,067 - log
eval samples: 400, loss: 1.3160778284072876 - 2025-05-28 19:32:03,524 - log
eval samples: 500, loss: 1.4877467155456543 - 2025-05-28 19:32:05,994 - log
eval samples: 600, loss: 1.5561617612838745 - 2025-05-28 19:32:08,460 - log
eval samples: 700, loss: 1.6633015871047974 - 2025-05-28 19:32:10,907 - log
eval samples: 800, loss: 1.465171217918396 - 2025-05-28 19:32:13,355 - log
eval samples: 900, loss: 1.2829045057296753 - 2025-05-28 19:32:15,806 - log
eval samples: 1000, loss: 1.27717125415802 - 2025-05-28 19:32:18,265 - log
eval samples: 1100, loss: 1.2594629526138306 - 2025-05-28 19:32:20,735 - log
average loss: +1.3410221221410248 - 2025-05-28 19:32:22,385 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:32:22,385 - log
| Eval   5 at step    10000 | time: 28.75s | valid loss  1.34 | valid ppl  3.82 | best ppl  3.82  - 2025-05-28 19:32:22,385 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:32:22,385 - log
| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 338.76 | loss  2.56 | avg loss  2.69 | ppl 14.72 - 2025-05-28 19:32:27,505 - log
| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 51.19 | loss  2.84 | avg loss  2.68 | ppl 14.58 - 2025-05-28 19:32:32,625 - log
| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 51.13 | loss  2.73 | avg loss  2.74 | ppl 15.52 - 2025-05-28 19:32:37,738 - log
| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 51.00 | loss  2.88 | avg loss  2.71 | ppl 15.06 - 2025-05-28 19:32:42,838 - log
| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 51.08 | loss  2.56 | avg loss  2.69 | ppl 14.80 - 2025-05-28 19:32:47,947 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10516.pkl - 2025-05-28 19:32:48,716 - log
start to train the model................ 3 - 2025-05-28 19:32:50,498 - log
| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 42.93 | loss  2.26 | avg loss  2.70 | ppl 14.93 - 2025-05-28 19:32:54,791 - log
| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 50.66 | loss  2.49 | avg loss  2.69 | ppl 14.69 - 2025-05-28 19:32:59,857 - log
| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 50.72 | loss  2.76 | avg loss  2.66 | ppl 14.23 - 2025-05-28 19:33:04,930 - log
| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 50.70 | loss  2.57 | avg loss  2.69 | ppl 14.75 - 2025-05-28 19:33:10,001 - log
| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 51.08 | loss  2.75 | avg loss  2.71 | ppl 14.97 - 2025-05-28 19:33:15,109 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.11000.ckpt - 2025-05-28 19:33:15,109 - log
| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 51.26 | loss  2.87 | avg loss  2.71 | ppl 15.03 - 2025-05-28 19:33:20,235 - log
| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 50.80 | loss  2.56 | avg loss  2.69 | ppl 14.67 - 2025-05-28 19:33:25,316 - log
| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 51.09 | loss  2.64 | avg loss  2.69 | ppl 14.77 - 2025-05-28 19:33:30,425 - log
| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 51.06 | loss  2.79 | avg loss  2.68 | ppl 14.53 - 2025-05-28 19:33:35,531 - log
| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 51.22 | loss  2.82 | avg loss  2.67 | ppl 14.42 - 2025-05-28 19:33:40,653 - log
| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 51.05 | loss  2.87 | avg loss  2.70 | ppl 14.85 - 2025-05-28 19:33:45,759 - log
| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 51.05 | loss  3.31 | avg loss  2.69 | ppl 14.71 - 2025-05-28 19:33:50,864 - log
| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 50.98 | loss  2.55 | avg loss  2.70 | ppl 14.82 - 2025-05-28 19:33:55,962 - log
| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 51.29 | loss  2.64 | avg loss  2.65 | ppl 14.22 - 2025-05-28 19:34:01,091 - log
| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 51.44 | loss  2.90 | avg loss  2.68 | ppl 14.54 - 2025-05-28 19:34:06,234 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.12000.ckpt - 2025-05-28 19:34:06,235 - log
eval samples: 0, loss: 1.2633315324783325 - 2025-05-28 19:34:06,265 - log
eval samples: 100, loss: 1.4668744802474976 - 2025-05-28 19:34:08,727 - log
eval samples: 200, loss: 1.0453187227249146 - 2025-05-28 19:34:11,192 - log
eval samples: 300, loss: 1.5563288927078247 - 2025-05-28 19:34:13,650 - log
eval samples: 400, loss: 1.425518274307251 - 2025-05-28 19:34:16,113 - log
eval samples: 500, loss: 1.280185341835022 - 2025-05-28 19:34:18,568 - log
eval samples: 600, loss: 1.2796087265014648 - 2025-05-28 19:34:21,023 - log
eval samples: 700, loss: 1.1705528497695923 - 2025-05-28 19:34:23,475 - log
eval samples: 800, loss: 1.258530855178833 - 2025-05-28 19:34:25,927 - log
eval samples: 900, loss: 1.4789248704910278 - 2025-05-28 19:34:28,381 - log
eval samples: 1000, loss: 0.8809981346130371 - 2025-05-28 19:34:30,843 - log
eval samples: 1100, loss: 1.2408955097198486 - 2025-05-28 19:34:33,315 - log
average loss: +1.3228591492220962 - 2025-05-28 19:34:34,981 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:34:34,981 - log
| Eval   6 at step    12000 | time: 28.74s | valid loss  1.32 | valid ppl  3.75 | best ppl  3.75  - 2025-05-28 19:34:34,981 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:34:34,981 - log
| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 338.70 | loss  2.36 | avg loss  2.68 | ppl 14.52 - 2025-05-28 19:34:40,105 - log
| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 51.04 | loss  2.45 | avg loss  2.72 | ppl 15.20 - 2025-05-28 19:34:45,210 - log
| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 51.14 | loss  2.89 | avg loss  2.67 | ppl 14.42 - 2025-05-28 19:34:50,325 - log
| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 50.95 | loss  2.93 | avg loss  2.70 | ppl 14.82 - 2025-05-28 19:34:55,420 - log
| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 51.00 | loss  2.81 | avg loss  2.71 | ppl 15.01 - 2025-05-28 19:35:00,521 - log
| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 51.18 | loss  2.84 | avg loss  2.74 | ppl 15.43 - 2025-05-28 19:35:05,639 - log
| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 51.17 | loss  2.81 | avg loss  2.67 | ppl 14.42 - 2025-05-28 19:35:10,757 - log
| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 51.42 | loss  2.46 | avg loss  2.69 | ppl 14.76 - 2025-05-28 19:35:15,899 - log
| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 51.18 | loss  2.67 | avg loss  2.72 | ppl 15.18 - 2025-05-28 19:35:21,018 - log
| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 50.91 | loss  2.39 | avg loss  2.69 | ppl 14.66 - 2025-05-28 19:35:26,109 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.13000.ckpt - 2025-05-28 19:35:26,110 - log
| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 51.02 | loss  2.66 | avg loss  2.68 | ppl 14.51 - 2025-05-28 19:35:31,212 - log
| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 51.34 | loss  2.73 | avg loss  2.70 | ppl 14.83 - 2025-05-28 19:35:36,346 - log
| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 51.16 | loss  3.07 | avg loss  2.69 | ppl 14.79 - 2025-05-28 19:35:41,463 - log
| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 50.89 | loss  2.55 | avg loss  2.68 | ppl 14.60 - 2025-05-28 19:35:46,552 - log
| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 50.98 | loss  2.43 | avg loss  2.67 | ppl 14.50 - 2025-05-28 19:35:51,651 - log
| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 51.28 | loss  2.90 | avg loss  2.72 | ppl 15.19 - 2025-05-28 19:35:56,780 - log
| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 51.16 | loss  2.61 | avg loss  2.66 | ppl 14.36 - 2025-05-28 19:36:01,896 - log
| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 51.23 | loss  2.68 | avg loss  2.65 | ppl 14.14 - 2025-05-28 19:36:07,020 - log
| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 51.09 | loss  2.89 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:36:12,130 - log
| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 50.94 | loss  2.84 | avg loss  2.65 | ppl 14.18 - 2025-05-28 19:36:17,225 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.14000.ckpt - 2025-05-28 19:36:17,225 - log
eval samples: 0, loss: 1.357513666152954 - 2025-05-28 19:36:17,257 - log
eval samples: 100, loss: 1.0451284646987915 - 2025-05-28 19:36:19,719 - log
eval samples: 200, loss: 1.9112664461135864 - 2025-05-28 19:36:22,165 - log
eval samples: 300, loss: 1.0744510889053345 - 2025-05-28 19:36:24,615 - log
eval samples: 400, loss: 1.2073099613189697 - 2025-05-28 19:36:27,082 - log
eval samples: 500, loss: 1.2872084379196167 - 2025-05-28 19:36:29,558 - log
eval samples: 600, loss: 1.0042425394058228 - 2025-05-28 19:36:32,007 - log
eval samples: 700, loss: 0.8458223342895508 - 2025-05-28 19:36:34,458 - log
eval samples: 800, loss: 1.5183669328689575 - 2025-05-28 19:36:36,924 - log
eval samples: 900, loss: 1.1566377878189087 - 2025-05-28 19:36:39,380 - log
eval samples: 1000, loss: 1.380252718925476 - 2025-05-28 19:36:41,844 - log
eval samples: 1100, loss: 1.328174352645874 - 2025-05-28 19:36:44,303 - log
average loss: +1.30555330267916 - 2025-05-28 19:36:45,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:36:45,955 - log
| Eval   7 at step    14000 | time: 28.73s | valid loss  1.31 | valid ppl  3.69 | best ppl  3.69  - 2025-05-28 19:36:45,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:36:45,956 - log
| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 338.21 | loss  2.35 | avg loss  2.70 | ppl 14.85 - 2025-05-28 19:36:51,046 - log
| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 51.07 | loss  2.72 | avg loss  2.67 | ppl 14.42 - 2025-05-28 19:36:56,154 - log
| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 51.10 | loss  2.36 | avg loss  2.66 | ppl 14.32 - 2025-05-28 19:37:01,265 - log
| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 50.58 | loss  2.79 | avg loss  2.62 | ppl 13.72 - 2025-05-28 19:37:06,323 - log
| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 50.53 | loss  2.66 | avg loss  2.69 | ppl 14.77 - 2025-05-28 19:37:11,377 - log
| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 51.12 | loss  3.01 | avg loss  2.69 | ppl 14.78 - 2025-05-28 19:37:16,489 - log
| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 50.40 | loss  2.49 | avg loss  2.66 | ppl 14.26 - 2025-05-28 19:37:21,530 - log
| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 50.87 | loss  2.87 | avg loss  2.69 | ppl 14.72 - 2025-05-28 19:37:26,617 - log
| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 50.86 | loss  2.72 | avg loss  2.69 | ppl 14.80 - 2025-05-28 19:37:31,704 - log
| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 50.76 | loss  2.80 | avg loss  2.68 | ppl 14.51 - 2025-05-28 19:37:36,780 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15000.ckpt - 2025-05-28 19:37:36,780 - log
| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 50.90 | loss  2.73 | avg loss  2.67 | ppl 14.40 - 2025-05-28 19:37:41,870 - log
| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 50.79 | loss  2.73 | avg loss  2.69 | ppl 14.77 - 2025-05-28 19:37:46,949 - log
| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 50.83 | loss  2.66 | avg loss  2.66 | ppl 14.24 - 2025-05-28 19:37:52,033 - log
| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 50.85 | loss  2.49 | avg loss  2.71 | ppl 15.03 - 2025-05-28 19:37:57,118 - log
| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 50.76 | loss  2.80 | avg loss  2.73 | ppl 15.27 - 2025-05-28 19:38:02,195 - log
| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 50.64 | loss  2.39 | avg loss  2.66 | ppl 14.29 - 2025-05-28 19:38:07,260 - log
| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 50.70 | loss  2.70 | avg loss  2.69 | ppl 14.73 - 2025-05-28 19:38:12,330 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15774.pkl - 2025-05-28 19:38:16,026 - log
start to train the model................ 4 - 2025-05-28 19:38:17,796 - log
| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 13.49 | loss  2.68 | avg loss  2.71 | ppl 14.96 - 2025-05-28 19:38:19,145 - log
| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 50.92 | loss  2.61 | avg loss  2.66 | ppl 14.27 - 2025-05-28 19:38:24,237 - log
| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 50.80 | loss  2.83 | avg loss  2.64 | ppl 13.99 - 2025-05-28 19:38:29,318 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.16000.ckpt - 2025-05-28 19:38:29,318 - log
eval samples: 0, loss: 0.9353418350219727 - 2025-05-28 19:38:29,349 - log
eval samples: 100, loss: 1.437267541885376 - 2025-05-28 19:38:31,797 - log
eval samples: 200, loss: 1.3076859712600708 - 2025-05-28 19:38:34,244 - log
eval samples: 300, loss: 1.3381588459014893 - 2025-05-28 19:38:36,698 - log
eval samples: 400, loss: 1.4178297519683838 - 2025-05-28 19:38:39,163 - log
eval samples: 500, loss: 1.5683743953704834 - 2025-05-28 19:38:41,625 - log
eval samples: 600, loss: 1.5771623849868774 - 2025-05-28 19:38:44,101 - log
eval samples: 700, loss: 1.2509164810180664 - 2025-05-28 19:38:46,549 - log
eval samples: 800, loss: 1.7620576620101929 - 2025-05-28 19:38:48,998 - log
eval samples: 900, loss: 1.2213748693466187 - 2025-05-28 19:38:51,446 - log
eval samples: 1000, loss: 1.25296950340271 - 2025-05-28 19:38:53,892 - log
eval samples: 1100, loss: 1.1785836219787598 - 2025-05-28 19:38:56,343 - log
average loss: +1.29694511527068 - 2025-05-28 19:38:57,982 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:38:57,982 - log
| Eval   8 at step    16000 | time: 28.66s | valid loss  1.30 | valid ppl  3.66 | best ppl  3.66  - 2025-05-28 19:38:57,983 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:38:57,983 - log
| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 337.49 | loss  2.66 | avg loss  2.66 | ppl 14.29 - 2025-05-28 19:39:03,068 - log
| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 51.05 | loss  2.75 | avg loss  2.64 | ppl 14.07 - 2025-05-28 19:39:08,173 - log
| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 50.83 | loss  2.34 | avg loss  2.68 | ppl 14.61 - 2025-05-28 19:39:13,256 - log
| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 51.04 | loss  2.47 | avg loss  2.64 | ppl 14.05 - 2025-05-28 19:39:18,360 - log
| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 51.16 | loss  2.94 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:39:23,477 - log
| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 51.22 | loss  2.89 | avg loss  2.66 | ppl 14.34 - 2025-05-28 19:39:28,599 - log
| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 51.04 | loss  2.65 | avg loss  2.67 | ppl 14.47 - 2025-05-28 19:39:33,704 - log
| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 51.07 | loss  2.93 | avg loss  2.68 | ppl 14.53 - 2025-05-28 19:39:38,811 - log
| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 50.99 | loss  2.84 | avg loss  2.70 | ppl 14.95 - 2025-05-28 19:39:43,910 - log
| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 50.96 | loss  2.52 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:39:49,007 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.17000.ckpt - 2025-05-28 19:39:49,007 - log
| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 50.82 | loss  2.70 | avg loss  2.67 | ppl 14.38 - 2025-05-28 19:39:54,089 - log
| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 50.72 | loss  2.60 | avg loss  2.65 | ppl 14.22 - 2025-05-28 19:39:59,161 - log
| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 50.92 | loss  3.05 | avg loss  2.65 | ppl 14.20 - 2025-05-28 19:40:04,253 - log
| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 51.15 | loss  2.51 | avg loss  2.67 | ppl 14.45 - 2025-05-28 19:40:09,368 - log
| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 51.20 | loss  2.75 | avg loss  2.66 | ppl 14.29 - 2025-05-28 19:40:14,489 - log
| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 50.64 | loss  2.71 | avg loss  2.65 | ppl 14.11 - 2025-05-28 19:40:19,554 - log
| epoch   4 step    17700 |   1926 batches | lr 6.66e-05 | ms/batch 51.12 | loss  2.33 | avg loss  2.64 | ppl 14.07 - 2025-05-28 19:40:24,666 - log
| epoch   4 step    17800 |   2026 batches | lr 6.58e-05 | ms/batch 50.92 | loss  2.40 | avg loss  2.68 | ppl 14.55 - 2025-05-28 19:40:29,759 - log
| epoch   4 step    17900 |   2126 batches | lr 6.51e-05 | ms/batch 51.14 | loss  2.42 | avg loss  2.67 | ppl 14.43 - 2025-05-28 19:40:34,873 - log
| epoch   4 step    18000 |   2226 batches | lr 6.43e-05 | ms/batch 51.20 | loss  2.71 | avg loss  2.65 | ppl 14.14 - 2025-05-28 19:40:39,994 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.18000.ckpt - 2025-05-28 19:40:39,994 - log
eval samples: 0, loss: 1.6344207525253296 - 2025-05-28 19:40:40,026 - log
eval samples: 100, loss: 1.342889428138733 - 2025-05-28 19:40:42,494 - log
eval samples: 200, loss: 1.0921032428741455 - 2025-05-28 19:40:44,972 - log
eval samples: 300, loss: 1.322641372680664 - 2025-05-28 19:40:47,437 - log
eval samples: 400, loss: 1.217519760131836 - 2025-05-28 19:40:49,904 - log
eval samples: 500, loss: 1.2165040969848633 - 2025-05-28 19:40:52,369 - log
eval samples: 600, loss: 1.2928768396377563 - 2025-05-28 19:40:54,836 - log
eval samples: 700, loss: 1.2887195348739624 - 2025-05-28 19:40:57,300 - log
eval samples: 800, loss: 1.429925799369812 - 2025-05-28 19:40:59,759 - log
eval samples: 900, loss: 0.9430123567581177 - 2025-05-28 19:41:02,222 - log
eval samples: 1000, loss: 1.4941818714141846 - 2025-05-28 19:41:04,680 - log
eval samples: 1100, loss: 1.2067198753356934 - 2025-05-28 19:41:07,127 - log
average loss: +1.30454063328774 - 2025-05-28 19:41:08,764 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:41:08,765 - log
| Eval   9 at step    18000 | time: 28.77s | valid loss  1.30 | valid ppl  3.69 | best ppl  3.66  - 2025-05-28 19:41:08,765 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:41:08,765 - log
| epoch   4 step    18100 |   2326 batches | lr 6.35e-05 | ms/batch 338.05 | loss  2.66 | avg loss  2.68 | ppl 14.60 - 2025-05-28 19:41:13,799 - log
| epoch   4 step    18200 |   2426 batches | lr 6.27e-05 | ms/batch 50.48 | loss  2.62 | avg loss  2.64 | ppl 14.07 - 2025-05-28 19:41:18,848 - log
| epoch   4 step    18300 |   2526 batches | lr 6.2e-05 | ms/batch 50.57 | loss  2.58 | avg loss  2.66 | ppl 14.28 - 2025-05-28 19:41:23,905 - log
| epoch   4 step    18400 |   2626 batches | lr 6.12e-05 | ms/batch 50.73 | loss  2.49 | avg loss  2.69 | ppl 14.79 - 2025-05-28 19:41:28,979 - log
| epoch   4 step    18500 |   2726 batches | lr 6.04e-05 | ms/batch 50.76 | loss  2.63 | avg loss  2.69 | ppl 14.77 - 2025-05-28 19:41:34,055 - log
| epoch   4 step    18600 |   2826 batches | lr 5.96e-05 | ms/batch 51.04 | loss  2.70 | avg loss  2.64 | ppl 14.04 - 2025-05-28 19:41:39,160 - log
| epoch   4 step    18700 |   2926 batches | lr 5.89e-05 | ms/batch 51.08 | loss  2.40 | avg loss  2.64 | ppl 14.05 - 2025-05-28 19:41:44,268 - log
| epoch   4 step    18800 |   3026 batches | lr 5.81e-05 | ms/batch 50.93 | loss  2.79 | avg loss  2.64 | ppl 14.04 - 2025-05-28 19:41:49,362 - log
| epoch   4 step    18900 |   3126 batches | lr 5.73e-05 | ms/batch 50.94 | loss  2.68 | avg loss  2.64 | ppl 13.99 - 2025-05-28 19:41:54,457 - log
| epoch   4 step    19000 |   3226 batches | lr 5.65e-05 | ms/batch 50.93 | loss  2.68 | avg loss  2.66 | ppl 14.23 - 2025-05-28 19:41:59,550 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.19000.ckpt - 2025-05-28 19:41:59,550 - log
| epoch   4 step    19100 |   3326 batches | lr 5.58e-05 | ms/batch 50.94 | loss  2.57 | avg loss  2.64 | ppl 13.96 - 2025-05-28 19:42:04,645 - log
| epoch   4 step    19200 |   3426 batches | lr 5.5e-05 | ms/batch 50.98 | loss  2.67 | avg loss  2.69 | ppl 14.79 - 2025-05-28 19:42:09,743 - log
| epoch   4 step    19300 |   3526 batches | lr 5.42e-05 | ms/batch 50.67 | loss  2.66 | avg loss  2.66 | ppl 14.27 - 2025-05-28 19:42:14,810 - log
| epoch   4 step    19400 |   3626 batches | lr 5.34e-05 | ms/batch 50.56 | loss  2.72 | avg loss  2.67 | ppl 14.43 - 2025-05-28 19:42:19,867 - log
| epoch   4 step    19500 |   3726 batches | lr 5.27e-05 | ms/batch 50.96 | loss  2.42 | avg loss  2.64 | ppl 13.99 - 2025-05-28 19:42:24,963 - log
| epoch   4 step    19600 |   3826 batches | lr 5.19e-05 | ms/batch 50.95 | loss  2.57 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:42:30,058 - log
| epoch   4 step    19700 |   3926 batches | lr 5.11e-05 | ms/batch 50.78 | loss  2.62 | avg loss  2.68 | ppl 14.51 - 2025-05-28 19:42:35,137 - log
| epoch   4 step    19800 |   4026 batches | lr 5.03e-05 | ms/batch 50.59 | loss  2.23 | avg loss  2.66 | ppl 14.28 - 2025-05-28 19:42:40,196 - log
| epoch   4 step    19900 |   4126 batches | lr 4.96e-05 | ms/batch 50.53 | loss  2.66 | avg loss  2.66 | ppl 14.30 - 2025-05-28 19:42:45,249 - log
| epoch   4 step    20000 |   4226 batches | lr 4.88e-05 | ms/batch 50.81 | loss  2.71 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:42:50,330 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.20000.ckpt - 2025-05-28 19:42:50,330 - log
eval samples: 0, loss: 1.4803208112716675 - 2025-05-28 19:42:50,360 - log
eval samples: 100, loss: 1.2850626707077026 - 2025-05-28 19:42:52,804 - log
eval samples: 200, loss: 1.15731942653656 - 2025-05-28 19:42:55,250 - log
eval samples: 300, loss: 1.4986344575881958 - 2025-05-28 19:42:57,697 - log
eval samples: 400, loss: 1.1116288900375366 - 2025-05-28 19:43:00,149 - log
eval samples: 500, loss: 1.2143479585647583 - 2025-05-28 19:43:02,616 - log
eval samples: 600, loss: 1.4153852462768555 - 2025-05-28 19:43:05,062 - log
eval samples: 700, loss: 0.9058754444122314 - 2025-05-28 19:43:07,505 - log
eval samples: 800, loss: 1.3802525997161865 - 2025-05-28 19:43:09,949 - log
eval samples: 900, loss: 1.2564935684204102 - 2025-05-28 19:43:12,404 - log
eval samples: 1000, loss: 1.274799108505249 - 2025-05-28 19:43:14,864 - log
eval samples: 1100, loss: 1.1625906229019165 - 2025-05-28 19:43:17,316 - log
average loss: +1.3028206563975713 - 2025-05-28 19:43:18,956 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:43:18,956 - log
| Eval  10 at step    20000 | time: 28.62s | valid loss  1.30 | valid ppl  3.68 | best ppl  3.66  - 2025-05-28 19:43:18,956 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:43:18,956 - log
| epoch   4 step    20100 |   4326 batches | lr 4.8e-05 | ms/batch 337.36 | loss  2.63 | avg loss  2.64 | ppl 14.02 - 2025-05-28 19:43:24,067 - log
| epoch   4 step    20200 |   4426 batches | lr 4.72e-05 | ms/batch 51.34 | loss  2.51 | avg loss  2.64 | ppl 13.96 - 2025-05-28 19:43:29,201 - log
| epoch   4 step    20300 |   4526 batches | lr 4.65e-05 | ms/batch 50.83 | loss  2.40 | avg loss  2.68 | ppl 14.60 - 2025-05-28 19:43:34,285 - log
| epoch   4 step    20400 |   4626 batches | lr 4.57e-05 | ms/batch 51.31 | loss  2.76 | avg loss  2.69 | ppl 14.66 - 2025-05-28 19:43:39,416 - log
| epoch   4 step    20500 |   4726 batches | lr 4.49e-05 | ms/batch 51.17 | loss  2.61 | avg loss  2.64 | ppl 14.08 - 2025-05-28 19:43:44,534 - log
| epoch   4 step    20600 |   4826 batches | lr 4.41e-05 | ms/batch 51.01 | loss  2.75 | avg loss  2.65 | ppl 14.18 - 2025-05-28 19:43:49,635 - log
| epoch   4 step    20700 |   4926 batches | lr 4.34e-05 | ms/batch 50.87 | loss  2.73 | avg loss  2.67 | ppl 14.44 - 2025-05-28 19:43:54,722 - log
| epoch   4 step    20800 |   5026 batches | lr 4.26e-05 | ms/batch 51.03 | loss  2.38 | avg loss  2.66 | ppl 14.23 - 2025-05-28 19:43:59,825 - log
| epoch   4 step    20900 |   5126 batches | lr 4.18e-05 | ms/batch 50.78 | loss  2.72 | avg loss  2.65 | ppl 14.19 - 2025-05-28 19:44:04,904 - log
| epoch   4 step    21000 |   5226 batches | lr 4.1e-05 | ms/batch 50.80 | loss  2.54 | avg loss  2.67 | ppl 14.43 - 2025-05-28 19:44:09,985 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21000.ckpt - 2025-05-28 19:44:09,986 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21032.pkl - 2025-05-28 19:44:11,582 - log
start to train the model................ 5 - 2025-05-28 19:44:13,364 - log
| epoch   5 step    21100 |     68 batches | lr 4.02e-05 | ms/batch 35.10 | loss  2.52 | avg loss  2.69 | ppl 14.79 - 2025-05-28 19:44:16,874 - log
| epoch   5 step    21200 |    168 batches | lr 3.95e-05 | ms/batch 50.98 | loss  2.44 | avg loss  2.66 | ppl 14.35 - 2025-05-28 19:44:21,972 - log
| epoch   5 step    21300 |    268 batches | lr 3.87e-05 | ms/batch 50.81 | loss  2.64 | avg loss  2.64 | ppl 14.05 - 2025-05-28 19:44:27,054 - log
| epoch   5 step    21400 |    368 batches | lr 3.79e-05 | ms/batch 51.44 | loss  2.59 | avg loss  2.66 | ppl 14.25 - 2025-05-28 19:44:32,199 - log
| epoch   5 step    21500 |    468 batches | lr 3.71e-05 | ms/batch 51.24 | loss  2.40 | avg loss  2.63 | ppl 13.89 - 2025-05-28 19:44:37,324 - log
| epoch   5 step    21600 |    568 batches | lr 3.64e-05 | ms/batch 51.02 | loss  3.37 | avg loss  2.65 | ppl 14.13 - 2025-05-28 19:44:42,426 - log
| epoch   5 step    21700 |    668 batches | lr 3.56e-05 | ms/batch 51.00 | loss  2.85 | avg loss  2.64 | ppl 14.04 - 2025-05-28 19:44:47,527 - log
| epoch   5 step    21800 |    768 batches | lr 3.48e-05 | ms/batch 51.00 | loss  3.09 | avg loss  2.64 | ppl 13.96 - 2025-05-28 19:44:52,627 - log
| epoch   5 step    21900 |    868 batches | lr 3.4e-05 | ms/batch 51.26 | loss  2.56 | avg loss  2.61 | ppl 13.56 - 2025-05-28 19:44:57,754 - log
| epoch   5 step    22000 |    968 batches | lr 3.33e-05 | ms/batch 51.38 | loss  2.55 | avg loss  2.64 | ppl 13.97 - 2025-05-28 19:45:02,892 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.22000.ckpt - 2025-05-28 19:45:02,893 - log
eval samples: 0, loss: 1.6082487106323242 - 2025-05-28 19:45:02,923 - log
eval samples: 100, loss: 1.2948650121688843 - 2025-05-28 19:45:05,361 - log
eval samples: 200, loss: 1.1812114715576172 - 2025-05-28 19:45:07,825 - log
eval samples: 300, loss: 1.1903645992279053 - 2025-05-28 19:45:10,290 - log
eval samples: 400, loss: 1.407233715057373 - 2025-05-28 19:45:12,736 - log
eval samples: 500, loss: 1.2576349973678589 - 2025-05-28 19:45:15,177 - log
eval samples: 600, loss: 1.1877530813217163 - 2025-05-28 19:45:17,646 - log
eval samples: 700, loss: 1.074717402458191 - 2025-05-28 19:45:20,093 - log
eval samples: 800, loss: 1.1077675819396973 - 2025-05-28 19:45:22,560 - log
eval samples: 900, loss: 1.5465046167373657 - 2025-05-28 19:45:24,991 - log
eval samples: 1000, loss: 1.08961021900177 - 2025-05-28 19:45:27,424 - log
eval samples: 1100, loss: 1.314485788345337 - 2025-05-28 19:45:29,856 - log
average loss: +1.2976189494030932 - 2025-05-28 19:45:31,493 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:45:31,493 - log
| Eval  11 at step    22000 | time: 28.60s | valid loss  1.30 | valid ppl  3.66 | best ppl  3.66  - 2025-05-28 19:45:31,493 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:45:31,493 - log
| epoch   5 step    22100 |   1068 batches | lr 3.25e-05 | ms/batch 336.50 | loss  2.56 | avg loss  2.62 | ppl 13.76 - 2025-05-28 19:45:36,543 - log
| epoch   5 step    22200 |   1168 batches | lr 3.17e-05 | ms/batch 50.88 | loss  2.63 | avg loss  2.65 | ppl 14.14 - 2025-05-28 19:45:41,631 - log
| epoch   5 step    22300 |   1268 batches | lr 3.09e-05 | ms/batch 51.07 | loss  2.58 | avg loss  2.69 | ppl 14.68 - 2025-05-28 19:45:46,738 - log
| epoch   5 step    22400 |   1368 batches | lr 3.02e-05 | ms/batch 50.96 | loss  2.66 | avg loss  2.62 | ppl 13.72 - 2025-05-28 19:45:51,834 - log
| epoch   5 step    22500 |   1468 batches | lr 2.94e-05 | ms/batch 50.99 | loss  3.01 | avg loss  2.64 | ppl 13.98 - 2025-05-28 19:45:56,934 - log
| epoch   5 step    22600 |   1568 batches | lr 2.86e-05 | ms/batch 50.99 | loss  2.48 | avg loss  2.64 | ppl 14.00 - 2025-05-28 19:46:02,033 - log
| epoch   5 step    22700 |   1668 batches | lr 2.78e-05 | ms/batch 51.01 | loss  2.63 | avg loss  2.65 | ppl 14.12 - 2025-05-28 19:46:07,134 - log
| epoch   5 step    22800 |   1768 batches | lr 2.71e-05 | ms/batch 50.94 | loss  2.56 | avg loss  2.65 | ppl 14.15 - 2025-05-28 19:46:12,229 - log
| epoch   5 step    22900 |   1868 batches | lr 2.63e-05 | ms/batch 50.96 | loss  2.48 | avg loss  2.64 | ppl 13.97 - 2025-05-28 19:46:17,325 - log
| epoch   5 step    23000 |   1968 batches | lr 2.55e-05 | ms/batch 50.85 | loss  2.65 | avg loss  2.64 | ppl 14.04 - 2025-05-28 19:46:22,410 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.23000.ckpt - 2025-05-28 19:46:22,411 - log
| epoch   5 step    23100 |   2068 batches | lr 2.47e-05 | ms/batch 50.92 | loss  2.78 | avg loss  2.64 | ppl 14.00 - 2025-05-28 19:46:27,502 - log
| epoch   5 step    23200 |   2168 batches | lr 2.4e-05 | ms/batch 51.18 | loss  2.65 | avg loss  2.63 | ppl 13.91 - 2025-05-28 19:46:32,621 - log
| epoch   5 step    23300 |   2268 batches | lr 2.32e-05 | ms/batch 50.95 | loss  3.19 | avg loss  2.66 | ppl 14.33 - 2025-05-28 19:46:37,716 - log
| epoch   5 step    23400 |   2368 batches | lr 2.24e-05 | ms/batch 51.04 | loss  2.92 | avg loss  2.65 | ppl 14.11 - 2025-05-28 19:46:42,820 - log
| epoch   5 step    23500 |   2468 batches | lr 2.16e-05 | ms/batch 50.68 | loss  2.88 | avg loss  2.67 | ppl 14.48 - 2025-05-28 19:46:47,889 - log
| epoch   5 step    23600 |   2568 batches | lr 2.09e-05 | ms/batch 51.05 | loss  2.55 | avg loss  2.66 | ppl 14.32 - 2025-05-28 19:46:52,995 - log
| epoch   5 step    23700 |   2668 batches | lr 2.01e-05 | ms/batch 51.09 | loss  2.42 | avg loss  2.62 | ppl 13.70 - 2025-05-28 19:46:58,104 - log
| epoch   5 step    23800 |   2768 batches | lr 1.93e-05 | ms/batch 51.44 | loss  2.60 | avg loss  2.61 | ppl 13.63 - 2025-05-28 19:47:03,249 - log
| epoch   5 step    23900 |   2868 batches | lr 1.85e-05 | ms/batch 51.06 | loss  2.93 | avg loss  2.65 | ppl 14.11 - 2025-05-28 19:47:08,355 - log
| epoch   5 step    24000 |   2968 batches | lr 1.78e-05 | ms/batch 51.27 | loss  2.76 | avg loss  2.67 | ppl 14.45 - 2025-05-28 19:47:13,482 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.24000.ckpt - 2025-05-28 19:47:13,482 - log
eval samples: 0, loss: 1.6086368560791016 - 2025-05-28 19:47:13,513 - log
eval samples: 100, loss: 1.1228121519088745 - 2025-05-28 19:47:15,990 - log
eval samples: 200, loss: 1.3986577987670898 - 2025-05-28 19:47:18,445 - log
eval samples: 300, loss: 1.3135089874267578 - 2025-05-28 19:47:20,903 - log
eval samples: 400, loss: 1.059169054031372 - 2025-05-28 19:47:23,355 - log
eval samples: 500, loss: 1.652456521987915 - 2025-05-28 19:47:25,820 - log
eval samples: 600, loss: 1.4713783264160156 - 2025-05-28 19:47:28,280 - log
eval samples: 700, loss: 1.2430976629257202 - 2025-05-28 19:47:30,755 - log
eval samples: 800, loss: 1.1777387857437134 - 2025-05-28 19:47:33,199 - log
eval samples: 900, loss: 1.3382539749145508 - 2025-05-28 19:47:35,672 - log
eval samples: 1000, loss: 1.0263440608978271 - 2025-05-28 19:47:38,131 - log
eval samples: 1100, loss: 1.1253608465194702 - 2025-05-28 19:47:40,597 - log
average loss: +1.30135288023173 - 2025-05-28 19:47:42,254 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:47:42,254 - log
| Eval  12 at step    24000 | time: 28.77s | valid loss  1.30 | valid ppl  3.67 | best ppl  3.66  - 2025-05-28 19:47:42,254 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:47:42,254 - log
| epoch   5 step    24100 |   3068 batches | lr 1.7e-05 | ms/batch 338.93 | loss  2.62 | avg loss  2.66 | ppl 14.32 - 2025-05-28 19:47:47,375 - log
| epoch   5 step    24200 |   3168 batches | lr 1.62e-05 | ms/batch 51.26 | loss  2.76 | avg loss  2.69 | ppl 14.71 - 2025-05-28 19:47:52,501 - log
| epoch   5 step    24300 |   3268 batches | lr 1.54e-05 | ms/batch 51.19 | loss  2.73 | avg loss  2.67 | ppl 14.38 - 2025-05-28 19:47:57,621 - log
| epoch   5 step    24400 |   3368 batches | lr 1.47e-05 | ms/batch 51.61 | loss  2.80 | avg loss  2.64 | ppl 14.05 - 2025-05-28 19:48:02,782 - log
| epoch   5 step    24500 |   3468 batches | lr 1.39e-05 | ms/batch 51.34 | loss  2.47 | avg loss  2.66 | ppl 14.36 - 2025-05-28 19:48:07,916 - log
| epoch   5 step    24600 |   3568 batches | lr 1.31e-05 | ms/batch 51.20 | loss  2.72 | avg loss  2.67 | ppl 14.45 - 2025-05-28 19:48:13,036 - log
| epoch   5 step    24700 |   3668 batches | lr 1.23e-05 | ms/batch 51.14 | loss  2.89 | avg loss  2.65 | ppl 14.14 - 2025-05-28 19:48:18,151 - log
| epoch   5 step    24800 |   3768 batches | lr 1.16e-05 | ms/batch 51.57 | loss  2.28 | avg loss  2.61 | ppl 13.55 - 2025-05-28 19:48:23,309 - log
| epoch   5 step    24900 |   3868 batches | lr 1.08e-05 | ms/batch 51.70 | loss  2.77 | avg loss  2.65 | ppl 14.09 - 2025-05-28 19:48:28,479 - log
| epoch   5 step    25000 |   3968 batches | lr 1e-05 | ms/batch 51.50 | loss  2.78 | avg loss  2.65 | ppl 14.18 - 2025-05-28 19:48:33,630 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.25000.ckpt - 2025-05-28 19:48:33,630 - log
| epoch   5 step    25100 |   4068 batches | lr 9.23e-06 | ms/batch 51.66 | loss  2.63 | avg loss  2.63 | ppl 13.84 - 2025-05-28 19:48:38,796 - log
| epoch   5 step    25200 |   4168 batches | lr 8.45e-06 | ms/batch 51.56 | loss  2.48 | avg loss  2.65 | ppl 14.20 - 2025-05-28 19:48:43,952 - log
| epoch   5 step    25300 |   4268 batches | lr 7.68e-06 | ms/batch 51.60 | loss  3.02 | avg loss  2.62 | ppl 13.72 - 2025-05-28 19:48:49,112 - log
| epoch   5 step    25400 |   4368 batches | lr 6.9e-06 | ms/batch 51.49 | loss  2.49 | avg loss  2.65 | ppl 14.13 - 2025-05-28 19:48:54,261 - log
| epoch   5 step    25500 |   4468 batches | lr 6.13e-06 | ms/batch 51.12 | loss  2.36 | avg loss  2.65 | ppl 14.19 - 2025-05-28 19:48:59,373 - log
| epoch   5 step    25600 |   4568 batches | lr 5.35e-06 | ms/batch 51.14 | loss  2.78 | avg loss  2.67 | ppl 14.38 - 2025-05-28 19:49:04,488 - log
| epoch   5 step    25700 |   4668 batches | lr 4.58e-06 | ms/batch 51.44 | loss  2.43 | avg loss  2.65 | ppl 14.12 - 2025-05-28 19:49:09,632 - log
| epoch   5 step    25800 |   4768 batches | lr 3.8e-06 | ms/batch 51.44 | loss  2.54 | avg loss  2.65 | ppl 14.15 - 2025-05-28 19:49:14,776 - log
| epoch   5 step    25900 |   4868 batches | lr 3.02e-06 | ms/batch 51.60 | loss  2.76 | avg loss  2.64 | ppl 13.98 - 2025-05-28 19:49:19,936 - log
| epoch   5 step    26000 |   4968 batches | lr 2.25e-06 | ms/batch 51.56 | loss  2.75 | avg loss  2.64 | ppl 14.03 - 2025-05-28 19:49:25,092 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26000.ckpt - 2025-05-28 19:49:25,093 - log
eval samples: 0, loss: 1.0440574884414673 - 2025-05-28 19:49:25,125 - log
eval samples: 100, loss: 0.9878293871879578 - 2025-05-28 19:49:27,594 - log
eval samples: 200, loss: 1.20432710647583 - 2025-05-28 19:49:30,087 - log
eval samples: 300, loss: 1.8196864128112793 - 2025-05-28 19:49:32,569 - log
eval samples: 400, loss: 1.0254061222076416 - 2025-05-28 19:49:35,050 - log
eval samples: 500, loss: 1.527657389640808 - 2025-05-28 19:49:37,528 - log
eval samples: 600, loss: 1.315500020980835 - 2025-05-28 19:49:40,005 - log
eval samples: 700, loss: 1.6578822135925293 - 2025-05-28 19:49:42,496 - log
eval samples: 800, loss: 0.9361642599105835 - 2025-05-28 19:49:44,961 - log
eval samples: 900, loss: 1.6471096277236938 - 2025-05-28 19:49:47,425 - log
eval samples: 1000, loss: 1.5576107501983643 - 2025-05-28 19:49:49,886 - log
eval samples: 1100, loss: 1.4455596208572388 - 2025-05-28 19:49:52,360 - log
average loss: +1.2926992740328997 - 2025-05-28 19:49:54,010 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:49:54,010 - log
| Eval  13 at step    26000 | time: 28.91s | valid loss  1.29 | valid ppl  3.64 | best ppl  3.64  - 2025-05-28 19:49:54,010 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:49:54,010 - log
| epoch   5 step    26100 |   5068 batches | lr 1.47e-06 | ms/batch 340.39 | loss  2.70 | avg loss  2.63 | ppl 13.81 - 2025-05-28 19:49:59,132 - log
| epoch   5 step    26200 |   5168 batches | lr 6.98e-07 | ms/batch 51.49 | loss  2.90 | avg loss  2.65 | ppl 14.15 - 2025-05-28 19:50:04,282 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26290.pkl - 2025-05-28 19:50:08,831 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 19:50:10,515 - log
End of training - 2025-05-28 19:50:10,515 - log
ms/batch 64.87 - 2025-05-28 19:50:10,515 - log
==================================================================================================== - 2025-05-28 22:23:31,821 - log
        - random_seed : 110 - 2025-05-28 22:23:31,821 - log
        - lr : 0.0002 - 2025-05-28 22:23:31,821 - log
        - weight_decay : 0.01 - 2025-05-28 22:23:31,821 - log
        - correct_bias : False - 2025-05-28 22:23:31,821 - log
        - adam_epislon : 1e-06 - 2025-05-28 22:23:31,821 - log
        - no_decay_bias : False - 2025-05-28 22:23:31,821 - log
        - adam_beta1 : 0.9 - 2025-05-28 22:23:31,821 - log
        - adam_beta2 : 0.999 - 2025-05-28 22:23:31,821 - log
        - scheduler : linear - 2025-05-28 22:23:31,821 - log
        - max_step : None - 2025-05-28 22:23:31,821 - log
        - max_epoch : 5 - 2025-05-28 22:23:31,821 - log
        - warmup_step : 500 - 2025-05-28 22:23:31,821 - log
        - i_steps : 0 - 2025-05-28 22:23:31,821 - log
        - i_lrs : 0.00025 - 2025-05-28 22:23:31,821 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 22:23:31,821 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 22:23:31,821 - log
        - train_batch_size : 8 - 2025-05-28 22:23:31,821 - log
        - valid_batch_size : 4 - 2025-05-28 22:23:31,821 - log
        - grad_acc : 2 - 2025-05-28 22:23:31,821 - log
        - clip : 0.0 - 2025-05-28 22:23:31,821 - log
        - seq_len : 512 - 2025-05-28 22:23:31,821 - log
        - model_card : gpt2.sm - 2025-05-28 22:23:31,822 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 22:23:31,822 - log
        - fp16 : False - 2025-05-28 22:23:31,822 - log
        - log_interval : 100 - 2025-05-28 22:23:31,822 - log
        - eval_interval : 2000 - 2025-05-28 22:23:31,822 - log
        - save_interval : 1000 - 2025-05-28 22:23:31,822 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 22:23:31,822 - log
        - lora_dim : 4 - 2025-05-28 22:23:31,822 - log
        - lora_alpha : 32 - 2025-05-28 22:23:31,822 - log
        - obj : clm - 2025-05-28 22:23:31,822 - log
        - lora_dropout : 0.1 - 2025-05-28 22:23:31,822 - log
        - label_smooth : 0.1 - 2025-05-28 22:23:31,822 - log
        - roll_interval : -1 - 2025-05-28 22:23:31,822 - log
        - roll_lr : 1e-05 - 2025-05-28 22:23:31,822 - log
        - roll_step : 100 - 2025-05-28 22:23:31,822 - log
        - eval_epoch : 1 - 2025-05-28 22:23:31,822 - log
        - device : cuda - 2025-05-28 22:23:31,822 - log
==================================================================================================== - 2025-05-28 22:23:31,822 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 22:23:31,822 - log
loading model pretrained weight. - 2025-05-28 22:23:32,233 - log
set max_step: 26290 - 2025-05-28 22:23:33,176 - log
start to train the model................ 1 - 2025-05-28 22:23:33,177 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 368.80 | loss  5.49 | avg loss  5.85 | ppl 345.94 - 2025-05-28 22:24:10,056 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 234.67 | loss  4.11 | avg loss  4.69 | ppl 108.57 - 2025-05-28 22:24:33,524 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 235.30 | loss  3.50 | avg loss  3.48 | ppl 32.45 - 2025-05-28 22:24:57,055 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 235.82 | loss  3.28 | avg loss  3.24 | ppl 25.50 - 2025-05-28 22:25:20,638 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 236.38 | loss  2.99 | avg loss  3.12 | ppl 22.74 - 2025-05-28 22:25:44,277 - log
| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 236.97 | loss  3.53 | avg loss  3.09 | ppl 21.95 - 2025-05-28 22:26:07,974 - log
| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 237.15 | loss  3.21 | avg loss  3.04 | ppl 20.80 - 2025-05-28 22:26:31,690 - log
| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 236.95 | loss  3.06 | avg loss  2.96 | ppl 19.31 - 2025-05-28 22:26:55,385 - log
| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 237.30 | loss  2.80 | avg loss  2.96 | ppl 19.35 - 2025-05-28 22:27:19,116 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 237.29 | loss  2.89 | avg loss  2.98 | ppl 19.61 - 2025-05-28 22:27:42,845 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 22:27:42,845 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 237.05 | loss  3.01 | avg loss  2.95 | ppl 19.06 - 2025-05-28 22:28:06,551 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 237.31 | loss  2.60 | avg loss  2.97 | ppl 19.49 - 2025-05-28 22:28:30,282 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 237.05 | loss  3.17 | avg loss  2.89 | ppl 17.95 - 2025-05-28 22:28:53,987 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 237.09 | loss  2.81 | avg loss  2.87 | ppl 17.57 - 2025-05-28 22:29:17,696 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 237.04 | loss  2.61 | avg loss  2.87 | ppl 17.66 - 2025-05-28 22:29:41,401 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 237.32 | loss  2.55 | avg loss  2.90 | ppl 18.18 - 2025-05-28 22:30:05,134 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 237.59 | loss  3.05 | avg loss  2.86 | ppl 17.49 - 2025-05-28 22:30:28,893 - log
| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 237.01 | loss  2.97 | avg loss  2.86 | ppl 17.45 - 2025-05-28 22:30:52,594 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 237.27 | loss  3.04 | avg loss  2.86 | ppl 17.41 - 2025-05-28 22:31:16,322 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 237.49 | loss  2.86 | avg loss  2.85 | ppl 17.22 - 2025-05-28 22:31:40,072 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 22:31:40,072 - log
eval samples: 0, loss: 1.3311632871627808 - 2025-05-28 22:31:40,187 - log
eval samples: 100, loss: 1.7324848175048828 - 2025-05-28 22:31:51,424 - log
eval samples: 200, loss: 1.3950703144073486 - 2025-05-28 22:32:02,636 - log
eval samples: 300, loss: 1.0134600400924683 - 2025-05-28 22:32:13,891 - log
eval samples: 400, loss: 1.2134376764297485 - 2025-05-28 22:32:25,125 - log
eval samples: 500, loss: 1.395693302154541 - 2025-05-28 22:32:36,354 - log
eval samples: 600, loss: 1.7396842241287231 - 2025-05-28 22:32:47,571 - log
eval samples: 700, loss: 1.3864680528640747 - 2025-05-28 22:32:58,802 - log
eval samples: 800, loss: 1.2592263221740723 - 2025-05-28 22:33:10,021 - log
eval samples: 900, loss: 1.3506672382354736 - 2025-05-28 22:33:21,249 - log
eval samples: 1000, loss: 1.3702534437179565 - 2025-05-28 22:33:32,468 - log
eval samples: 1100, loss: 1.7569873332977295 - 2025-05-28 22:33:43,684 - log
average loss: +1.4628554549935746 - 2025-05-28 22:33:51,238 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:33:51,238 - log
| Eval   1 at step     2000 | time: 131.16s | valid loss  1.46 | valid ppl  4.32 | best ppl  4.32  - 2025-05-28 22:33:51,238 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:33:51,238 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 1549.11 | loss  3.16 | avg loss  2.84 | ppl 17.06 - 2025-05-28 22:34:14,983 - log
| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 237.14 | loss  2.77 | avg loss  2.84 | ppl 17.11 - 2025-05-28 22:34:38,697 - log
| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 237.40 | loss  3.24 | avg loss  2.85 | ppl 17.31 - 2025-05-28 22:35:02,438 - log
| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 237.18 | loss  2.57 | avg loss  2.80 | ppl 16.45 - 2025-05-28 22:35:26,157 - log
| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 237.31 | loss  3.26 | avg loss  2.86 | ppl 17.42 - 2025-05-28 22:35:49,888 - log
| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 237.01 | loss  2.68 | avg loss  2.82 | ppl 16.75 - 2025-05-28 22:36:13,590 - log
| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 237.31 | loss  2.87 | avg loss  2.81 | ppl 16.67 - 2025-05-28 22:36:37,322 - log
| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 237.21 | loss  2.82 | avg loss  2.82 | ppl 16.70 - 2025-05-28 22:37:01,044 - log
| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 237.14 | loss  2.73 | avg loss  2.81 | ppl 16.54 - 2025-05-28 22:37:24,758 - log
| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 237.55 | loss  2.85 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:37:48,514 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.3000.ckpt - 2025-05-28 22:37:48,514 - log
| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 237.34 | loss  2.75 | avg loss  2.83 | ppl 16.89 - 2025-05-28 22:38:12,248 - log
| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 237.65 | loss  3.06 | avg loss  2.81 | ppl 16.68 - 2025-05-28 22:38:36,014 - log
| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 237.28 | loss  2.83 | avg loss  2.79 | ppl 16.32 - 2025-05-28 22:38:59,743 - log
| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 237.53 | loss  2.62 | avg loss  2.78 | ppl 16.08 - 2025-05-28 22:39:23,496 - log
| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 237.45 | loss  3.12 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:39:47,242 - log
| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 237.21 | loss  2.76 | avg loss  2.76 | ppl 15.83 - 2025-05-28 22:40:10,964 - log
| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 237.35 | loss  2.53 | avg loss  2.77 | ppl 15.95 - 2025-05-28 22:40:34,700 - log
| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 237.36 | loss  2.92 | avg loss  2.78 | ppl 16.17 - 2025-05-28 22:40:58,436 - log
| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 237.53 | loss  2.80 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:41:22,190 - log
| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 237.39 | loss  2.61 | avg loss  2.79 | ppl 16.31 - 2025-05-28 22:41:45,930 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4000.ckpt - 2025-05-28 22:41:45,930 - log
eval samples: 0, loss: 1.6549922227859497 - 2025-05-28 22:41:46,043 - log
eval samples: 100, loss: 1.4869531393051147 - 2025-05-28 22:41:57,273 - log
eval samples: 200, loss: 1.1642392873764038 - 2025-05-28 22:42:08,516 - log
eval samples: 300, loss: 1.2595221996307373 - 2025-05-28 22:42:19,755 - log
eval samples: 400, loss: 1.8051190376281738 - 2025-05-28 22:42:30,976 - log
eval samples: 500, loss: 1.2350364923477173 - 2025-05-28 22:42:42,188 - log
eval samples: 600, loss: 1.403570532798767 - 2025-05-28 22:42:53,392 - log
eval samples: 700, loss: 1.4691437482833862 - 2025-05-28 22:43:04,642 - log
eval samples: 800, loss: 1.6780551671981812 - 2025-05-28 22:43:15,888 - log
eval samples: 900, loss: 1.6091654300689697 - 2025-05-28 22:43:27,113 - log
eval samples: 1000, loss: 1.374427080154419 - 2025-05-28 22:43:38,326 - log
eval samples: 1100, loss: 1.2569255828857422 - 2025-05-28 22:43:49,535 - log
average loss: +1.396705282826538 - 2025-05-28 22:43:57,047 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:43:57,047 - log
| Eval   2 at step     4000 | time: 131.11s | valid loss  1.40 | valid ppl  4.04 | best ppl  4.04  - 2025-05-28 22:43:57,047 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:43:57,047 - log
| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 1548.08 | loss  2.69 | avg loss  2.80 | ppl 16.43 - 2025-05-28 22:44:20,738 - log
| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 237.17 | loss  2.42 | avg loss  2.77 | ppl 15.99 - 2025-05-28 22:44:44,455 - log
| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 237.28 | loss  2.81 | avg loss  2.73 | ppl 15.31 - 2025-05-28 22:45:08,184 - log
| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 236.95 | loss  2.61 | avg loss  2.78 | ppl 16.14 - 2025-05-28 22:45:31,880 - log
| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 237.14 | loss  3.28 | avg loss  2.77 | ppl 16.02 - 2025-05-28 22:45:55,595 - log
| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 237.25 | loss  2.77 | avg loss  2.76 | ppl 15.87 - 2025-05-28 22:46:19,321 - log
| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 237.13 | loss  2.80 | avg loss  2.75 | ppl 15.59 - 2025-05-28 22:46:43,034 - log
| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 237.26 | loss  2.72 | avg loss  2.74 | ppl 15.44 - 2025-05-28 22:47:06,761 - log
| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 237.32 | loss  2.65 | avg loss  2.75 | ppl 15.65 - 2025-05-28 22:47:30,494 - log
| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 237.21 | loss  2.78 | avg loss  2.75 | ppl 15.71 - 2025-05-28 22:47:54,215 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5000.ckpt - 2025-05-28 22:47:54,216 - log
| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 237.13 | loss  2.79 | avg loss  2.71 | ppl 15.04 - 2025-05-28 22:48:17,929 - log
| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 237.28 | loss  2.71 | avg loss  2.77 | ppl 16.01 - 2025-05-28 22:48:41,657 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5258.pkl - 2025-05-28 22:48:55,166 - log
start to train the model................ 2 - 2025-05-28 22:48:57,134 - log
| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 99.91 | loss  2.53 | avg loss  2.76 | ppl 15.74 - 2025-05-28 22:49:07,125 - log
| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 236.88 | loss  2.57 | avg loss  2.72 | ppl 15.23 - 2025-05-28 22:49:30,813 - log
| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 237.04 | loss  2.63 | avg loss  2.75 | ppl 15.65 - 2025-05-28 22:49:54,518 - log
| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 237.20 | loss  2.59 | avg loss  2.71 | ppl 15.04 - 2025-05-28 22:50:18,239 - log
| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 237.20 | loss  2.53 | avg loss  2.70 | ppl 14.91 - 2025-05-28 22:50:41,959 - log
| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 237.27 | loss  2.45 | avg loss  2.70 | ppl 14.85 - 2025-05-28 22:51:05,686 - log
| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 237.48 | loss  2.80 | avg loss  2.74 | ppl 15.56 - 2025-05-28 22:51:29,434 - log
| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 237.82 | loss  2.61 | avg loss  2.74 | ppl 15.54 - 2025-05-28 22:51:53,217 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6000.ckpt - 2025-05-28 22:51:53,218 - log
eval samples: 0, loss: 1.1115864515304565 - 2025-05-28 22:51:53,331 - log
eval samples: 100, loss: 1.3974859714508057 - 2025-05-28 22:52:04,580 - log
eval samples: 200, loss: 1.4889051914215088 - 2025-05-28 22:52:15,798 - log
eval samples: 300, loss: 1.583939552307129 - 2025-05-28 22:52:27,056 - log
eval samples: 400, loss: 1.2336955070495605 - 2025-05-28 22:52:38,320 - log
eval samples: 500, loss: 1.3424943685531616 - 2025-05-28 22:52:49,554 - log
eval samples: 600, loss: 1.7084143161773682 - 2025-05-28 22:53:00,769 - log
eval samples: 700, loss: 1.4570744037628174 - 2025-05-28 22:53:11,998 - log
eval samples: 800, loss: 1.1419869661331177 - 2025-05-28 22:53:23,201 - log
eval samples: 900, loss: 1.6875618696212769 - 2025-05-28 22:53:34,406 - log
eval samples: 1000, loss: 1.4060337543487549 - 2025-05-28 22:53:45,652 - log
eval samples: 1100, loss: 1.1417508125305176 - 2025-05-28 22:53:56,868 - log
average loss: +1.358503176333153 - 2025-05-28 22:54:04,380 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:54:04,381 - log
| Eval   3 at step     6000 | time: 131.16s | valid loss  1.36 | valid ppl  3.89 | best ppl  3.89  - 2025-05-28 22:54:04,381 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:54:04,381 - log
| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 1549.61 | loss  2.68 | avg loss  2.72 | ppl 15.25 - 2025-05-28 22:54:28,179 - log
| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 237.54 | loss  2.62 | avg loss  2.69 | ppl 14.80 - 2025-05-28 22:54:51,933 - log
| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 237.56 | loss  2.72 | avg loss  2.73 | ppl 15.35 - 2025-05-28 22:55:15,689 - log
| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 237.31 | loss  3.11 | avg loss  2.73 | ppl 15.38 - 2025-05-28 22:55:39,421 - log
| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 237.55 | loss  2.86 | avg loss  2.73 | ppl 15.40 - 2025-05-28 22:56:03,177 - log
| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 237.62 | loss  2.97 | avg loss  2.67 | ppl 14.49 - 2025-05-28 22:56:26,939 - log
| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 237.96 | loss  2.67 | avg loss  2.73 | ppl 15.36 - 2025-05-28 22:56:50,736 - log
| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 238.22 | loss  2.92 | avg loss  2.71 | ppl 15.09 - 2025-05-28 22:57:14,558 - log
| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 238.28 | loss  2.70 | avg loss  2.69 | ppl 14.73 - 2025-05-28 22:57:38,386 - log
| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 237.60 | loss  2.59 | avg loss  2.72 | ppl 15.16 - 2025-05-28 22:58:02,146 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.7000.ckpt - 2025-05-28 22:58:02,147 - log
| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 237.76 | loss  2.70 | avg loss  2.71 | ppl 15.03 - 2025-05-28 22:58:25,923 - log
| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 237.50 | loss  3.02 | avg loss  2.70 | ppl 14.88 - 2025-05-28 22:58:49,674 - log
| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 237.78 | loss  2.55 | avg loss  2.73 | ppl 15.31 - 2025-05-28 22:59:13,452 - log
| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 238.51 | loss  2.65 | avg loss  2.73 | ppl 15.41 - 2025-05-28 22:59:37,304 - log
| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 237.40 | loss  2.85 | avg loss  2.71 | ppl 15.10 - 2025-05-28 23:00:01,044 - log
| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 237.16 | loss  2.36 | avg loss  2.68 | ppl 14.52 - 2025-05-28 23:00:24,760 - log
| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 237.50 | loss  2.68 | avg loss  2.71 | ppl 15.01 - 2025-05-28 23:00:48,510 - log
| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 237.39 | loss  2.69 | avg loss  2.72 | ppl 15.12 - 2025-05-28 23:01:12,249 - log
| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 237.14 | loss  2.63 | avg loss  2.71 | ppl 15.04 - 2025-05-28 23:01:35,964 - log
| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 237.22 | loss  2.69 | avg loss  2.74 | ppl 15.41 - 2025-05-28 23:01:59,686 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8000.ckpt - 2025-05-28 23:01:59,686 - log
eval samples: 0, loss: 1.3126604557037354 - 2025-05-28 23:01:59,800 - log
eval samples: 100, loss: 1.356477975845337 - 2025-05-28 23:02:11,006 - log
eval samples: 200, loss: 0.979518473148346 - 2025-05-28 23:02:22,226 - log
eval samples: 300, loss: 1.2098244428634644 - 2025-05-28 23:02:33,427 - log
eval samples: 400, loss: 1.1514136791229248 - 2025-05-28 23:02:44,613 - log
eval samples: 500, loss: 1.2979331016540527 - 2025-05-28 23:02:55,829 - log
eval samples: 600, loss: 1.8207266330718994 - 2025-05-28 23:03:07,033 - log
eval samples: 700, loss: 1.3808382749557495 - 2025-05-28 23:03:18,255 - log
eval samples: 800, loss: 1.5667622089385986 - 2025-05-28 23:03:29,488 - log
eval samples: 900, loss: 1.286858081817627 - 2025-05-28 23:03:40,698 - log
eval samples: 1000, loss: 1.607016921043396 - 2025-05-28 23:03:51,886 - log
eval samples: 1100, loss: 1.1297926902770996 - 2025-05-28 23:04:03,058 - log
average loss: +1.3333840376301989 - 2025-05-28 23:04:10,553 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:04:10,553 - log
| Eval   4 at step     8000 | time: 130.86s | valid loss  1.33 | valid ppl  3.79 | best ppl  3.79  - 2025-05-28 23:04:10,553 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:04:10,553 - log
| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 1545.98 | loss  2.80 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:04:34,284 - log
| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 237.24 | loss  2.68 | avg loss  2.68 | ppl 14.65 - 2025-05-28 23:04:58,008 - log
| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 237.34 | loss  2.81 | avg loss  2.70 | ppl 14.83 - 2025-05-28 23:05:21,743 - log
| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 237.22 | loss  2.38 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:05:45,466 - log
| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 237.09 | loss  2.86 | avg loss  2.73 | ppl 15.35 - 2025-05-28 23:06:09,175 - log
| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 236.90 | loss  2.61 | avg loss  2.71 | ppl 15.06 - 2025-05-28 23:06:32,866 - log
| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 236.74 | loss  2.78 | avg loss  2.70 | ppl 14.86 - 2025-05-28 23:06:56,540 - log
| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 237.22 | loss  2.59 | avg loss  2.67 | ppl 14.47 - 2025-05-28 23:07:20,263 - log
| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 237.32 | loss  2.41 | avg loss  2.69 | ppl 14.68 - 2025-05-28 23:07:43,995 - log
| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 236.76 | loss  2.81 | avg loss  2.70 | ppl 14.86 - 2025-05-28 23:08:07,671 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.9000.ckpt - 2025-05-28 23:08:07,672 - log
| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 237.34 | loss  2.73 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:08:31,406 - log
| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 237.25 | loss  2.18 | avg loss  2.70 | ppl 14.88 - 2025-05-28 23:08:55,132 - log
| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 237.03 | loss  2.58 | avg loss  2.69 | ppl 14.71 - 2025-05-28 23:09:18,835 - log
| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 237.11 | loss  2.65 | avg loss  2.69 | ppl 14.67 - 2025-05-28 23:09:42,546 - log
| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 237.08 | loss  2.82 | avg loss  2.73 | ppl 15.35 - 2025-05-28 23:10:06,254 - log
| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 237.41 | loss  2.48 | avg loss  2.71 | ppl 15.08 - 2025-05-28 23:10:29,995 - log
| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 237.15 | loss  2.33 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:10:53,711 - log
| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 237.18 | loss  2.52 | avg loss  2.66 | ppl 14.30 - 2025-05-28 23:11:17,429 - log
| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 237.34 | loss  3.11 | avg loss  2.72 | ppl 15.19 - 2025-05-28 23:11:41,163 - log
| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 237.19 | loss  2.55 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:12:04,882 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10000.ckpt - 2025-05-28 23:12:04,883 - log
eval samples: 0, loss: 1.0558615922927856 - 2025-05-28 23:12:04,995 - log
eval samples: 100, loss: 1.3361111879348755 - 2025-05-28 23:12:16,213 - log
eval samples: 200, loss: 1.7232375144958496 - 2025-05-28 23:12:27,425 - log
eval samples: 300, loss: 1.3218919038772583 - 2025-05-28 23:12:38,627 - log
eval samples: 400, loss: 1.3240370750427246 - 2025-05-28 23:12:49,824 - log
eval samples: 500, loss: 1.4658758640289307 - 2025-05-28 23:13:01,059 - log
eval samples: 600, loss: 1.5875098705291748 - 2025-05-28 23:13:12,263 - log
eval samples: 700, loss: 1.6034296751022339 - 2025-05-28 23:13:23,448 - log
eval samples: 800, loss: 1.3790138959884644 - 2025-05-28 23:13:34,651 - log
eval samples: 900, loss: 1.410516619682312 - 2025-05-28 23:13:45,870 - log
eval samples: 1000, loss: 1.309606909751892 - 2025-05-28 23:13:57,072 - log
eval samples: 1100, loss: 1.2331587076187134 - 2025-05-28 23:14:08,281 - log
average loss: +1.3155945563459233 - 2025-05-28 23:14:15,801 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:14:15,801 - log
| Eval   5 at step    10000 | time: 130.92s | valid loss  1.32 | valid ppl  3.73 | best ppl  3.73  - 2025-05-28 23:14:15,801 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:14:15,801 - log
| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 1546.95 | loss  2.54 | avg loss  2.67 | ppl 14.50 - 2025-05-28 23:14:39,578 - log
| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 237.48 | loss  2.83 | avg loss  2.66 | ppl 14.26 - 2025-05-28 23:15:03,326 - log
| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 237.45 | loss  2.66 | avg loss  2.73 | ppl 15.28 - 2025-05-28 23:15:27,071 - log
| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 237.56 | loss  2.82 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:15:50,828 - log
| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 237.30 | loss  2.52 | avg loss  2.68 | ppl 14.52 - 2025-05-28 23:16:14,558 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10516.pkl - 2025-05-28 23:16:18,114 - log
start to train the model................ 3 - 2025-05-28 23:16:19,909 - log
| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 199.57 | loss  2.17 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:16:39,866 - log
| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 237.47 | loss  2.44 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:17:03,614 - log
| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 237.39 | loss  2.73 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:17:27,354 - log
| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 237.59 | loss  2.54 | avg loss  2.67 | ppl 14.43 - 2025-05-28 23:17:51,113 - log
| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 237.45 | loss  2.68 | avg loss  2.69 | ppl 14.69 - 2025-05-28 23:18:14,859 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.11000.ckpt - 2025-05-28 23:18:14,859 - log
| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 237.49 | loss  2.79 | avg loss  2.69 | ppl 14.70 - 2025-05-28 23:18:38,608 - log
| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 237.35 | loss  2.50 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:19:02,343 - log
| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 237.37 | loss  2.62 | avg loss  2.67 | ppl 14.46 - 2025-05-28 23:19:26,081 - log
| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 237.47 | loss  2.78 | avg loss  2.66 | ppl 14.31 - 2025-05-28 23:19:49,829 - log
| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 237.13 | loss  2.87 | avg loss  2.64 | ppl 14.05 - 2025-05-28 23:20:13,542 - log
| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 237.34 | loss  2.99 | avg loss  2.68 | ppl 14.56 - 2025-05-28 23:20:37,277 - log
| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 237.33 | loss  3.32 | avg loss  2.67 | ppl 14.47 - 2025-05-28 23:21:01,010 - log
| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 237.04 | loss  2.49 | avg loss  2.68 | ppl 14.53 - 2025-05-28 23:21:24,715 - log
| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 237.27 | loss  2.62 | avg loss  2.63 | ppl 13.87 - 2025-05-28 23:21:48,442 - log
| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 237.60 | loss  2.92 | avg loss  2.65 | ppl 14.21 - 2025-05-28 23:22:12,203 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.12000.ckpt - 2025-05-28 23:22:12,203 - log
eval samples: 0, loss: 1.1977673768997192 - 2025-05-28 23:22:12,316 - log
eval samples: 100, loss: 1.4073446989059448 - 2025-05-28 23:22:23,530 - log
eval samples: 200, loss: 1.1455620527267456 - 2025-05-28 23:22:34,765 - log
eval samples: 300, loss: 1.561932921409607 - 2025-05-28 23:22:45,956 - log
eval samples: 400, loss: 1.4663468599319458 - 2025-05-28 23:22:57,180 - log
eval samples: 500, loss: 1.307837963104248 - 2025-05-28 23:23:08,402 - log
eval samples: 600, loss: 1.2117416858673096 - 2025-05-28 23:23:19,626 - log
eval samples: 700, loss: 1.311022400856018 - 2025-05-28 23:23:30,849 - log
eval samples: 800, loss: 1.183372139930725 - 2025-05-28 23:23:42,079 - log
eval samples: 900, loss: 1.4637432098388672 - 2025-05-28 23:23:53,320 - log
eval samples: 1000, loss: 0.8822451233863831 - 2025-05-28 23:24:04,510 - log
eval samples: 1100, loss: 1.3532031774520874 - 2025-05-28 23:24:15,710 - log
average loss: +1.3021555659807709 - 2025-05-28 23:24:23,215 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:24:23,215 - log
| Eval   6 at step    12000 | time: 131.01s | valid loss  1.30 | valid ppl  3.68 | best ppl  3.68  - 2025-05-28 23:24:23,215 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:24:23,215 - log
| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 1547.44 | loss  2.44 | avg loss  2.66 | ppl 14.25 - 2025-05-28 23:24:46,947 - log
| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 237.24 | loss  2.39 | avg loss  2.70 | ppl 14.85 - 2025-05-28 23:25:10,672 - log
| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 237.07 | loss  2.87 | avg loss  2.65 | ppl 14.20 - 2025-05-28 23:25:34,380 - log
| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 237.40 | loss  2.95 | avg loss  2.67 | ppl 14.49 - 2025-05-28 23:25:58,120 - log
| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 237.48 | loss  2.66 | avg loss  2.69 | ppl 14.75 - 2025-05-28 23:26:21,868 - log
| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 237.38 | loss  2.85 | avg loss  2.71 | ppl 14.98 - 2025-05-28 23:26:45,606 - log
| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 237.69 | loss  2.78 | avg loss  2.65 | ppl 14.13 - 2025-05-28 23:27:09,376 - log
| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 237.60 | loss  2.49 | avg loss  2.67 | ppl 14.38 - 2025-05-28 23:27:33,137 - log
| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 237.52 | loss  2.64 | avg loss  2.70 | ppl 14.81 - 2025-05-28 23:27:56,889 - log
| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 237.68 | loss  2.36 | avg loss  2.66 | ppl 14.35 - 2025-05-28 23:28:20,658 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.13000.ckpt - 2025-05-28 23:28:20,658 - log
| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 237.67 | loss  2.60 | avg loss  2.65 | ppl 14.20 - 2025-05-28 23:28:44,425 - log
| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 237.52 | loss  2.71 | avg loss  2.68 | ppl 14.61 - 2025-05-28 23:29:08,178 - log
| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 237.20 | loss  2.96 | avg loss  2.67 | ppl 14.49 - 2025-05-28 23:29:31,899 - log
| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 237.47 | loss  2.56 | avg loss  2.65 | ppl 14.21 - 2025-05-28 23:29:55,646 - log
| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 237.46 | loss  2.40 | avg loss  2.65 | ppl 14.18 - 2025-05-28 23:30:19,393 - log
| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 237.32 | loss  2.88 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:30:43,126 - log
| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 237.53 | loss  2.61 | avg loss  2.65 | ppl 14.12 - 2025-05-28 23:31:06,879 - log
| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 237.41 | loss  2.63 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:31:30,621 - log
| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 237.05 | loss  2.85 | avg loss  2.63 | ppl 13.86 - 2025-05-28 23:31:54,326 - log
| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 236.92 | loss  2.98 | avg loss  2.64 | ppl 13.99 - 2025-05-28 23:32:18,019 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.14000.ckpt - 2025-05-28 23:32:18,019 - log
eval samples: 0, loss: 1.3151981830596924 - 2025-05-28 23:32:18,132 - log
eval samples: 100, loss: 1.042459487915039 - 2025-05-28 23:32:29,341 - log
eval samples: 200, loss: 1.8930816650390625 - 2025-05-28 23:32:40,558 - log
eval samples: 300, loss: 1.054253101348877 - 2025-05-28 23:32:51,763 - log
eval samples: 400, loss: 1.1351889371871948 - 2025-05-28 23:33:02,972 - log
eval samples: 500, loss: 1.3047995567321777 - 2025-05-28 23:33:14,191 - log
eval samples: 600, loss: 1.018662929534912 - 2025-05-28 23:33:25,393 - log
eval samples: 700, loss: 0.7734968066215515 - 2025-05-28 23:33:36,611 - log
eval samples: 800, loss: 1.4857409000396729 - 2025-05-28 23:33:47,833 - log
eval samples: 900, loss: 1.1919777393341064 - 2025-05-28 23:33:59,072 - log
eval samples: 1000, loss: 1.4323145151138306 - 2025-05-28 23:34:10,273 - log
eval samples: 1100, loss: 1.3710964918136597 - 2025-05-28 23:34:21,469 - log
average loss: +1.2857964905871921 - 2025-05-28 23:34:28,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:34:28,955 - log
| Eval   7 at step    14000 | time: 130.93s | valid loss  1.29 | valid ppl  3.62 | best ppl  3.62  - 2025-05-28 23:34:28,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:34:28,955 - log
| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 1546.49 | loss  2.35 | avg loss  2.67 | ppl 14.51 - 2025-05-28 23:34:52,668 - log
| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 236.98 | loss  2.71 | avg loss  2.65 | ppl 14.14 - 2025-05-28 23:35:16,367 - log
| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 237.05 | loss  2.42 | avg loss  2.64 | ppl 13.96 - 2025-05-28 23:35:40,072 - log
| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 236.91 | loss  2.79 | avg loss  2.60 | ppl 13.50 - 2025-05-28 23:36:03,764 - log
| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 237.07 | loss  2.61 | avg loss  2.66 | ppl 14.35 - 2025-05-28 23:36:27,471 - log
| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 237.49 | loss  2.97 | avg loss  2.68 | ppl 14.58 - 2025-05-28 23:36:51,220 - log
| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 236.81 | loss  2.46 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:37:14,902 - log
| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 237.11 | loss  2.90 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:37:38,614 - log
| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 236.92 | loss  2.71 | avg loss  2.67 | ppl 14.39 - 2025-05-28 23:38:02,306 - log
| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 236.49 | loss  2.72 | avg loss  2.66 | ppl 14.27 - 2025-05-28 23:38:25,955 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15000.ckpt - 2025-05-28 23:38:25,956 - log
| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 237.03 | loss  2.73 | avg loss  2.65 | ppl 14.18 - 2025-05-28 23:38:49,659 - log
| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 236.99 | loss  2.73 | avg loss  2.67 | ppl 14.37 - 2025-05-28 23:39:13,358 - log
| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 236.63 | loss  2.66 | avg loss  2.63 | ppl 13.92 - 2025-05-28 23:39:37,022 - log
| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 236.92 | loss  2.47 | avg loss  2.69 | ppl 14.67 - 2025-05-28 23:40:00,714 - log
| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 236.88 | loss  2.71 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:40:24,402 - log
| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 236.74 | loss  2.40 | avg loss  2.64 | ppl 13.98 - 2025-05-28 23:40:48,077 - log
| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 236.99 | loss  2.75 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:41:11,777 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15774.pkl - 2025-05-28 23:41:29,071 - log
start to train the model................ 4 - 2025-05-28 23:41:30,946 - log
| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 61.79 | loss  2.69 | avg loss  2.68 | ppl 14.65 - 2025-05-28 23:41:37,125 - log
| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 236.94 | loss  2.55 | avg loss  2.64 | ppl 13.98 - 2025-05-28 23:42:00,819 - log
| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 236.97 | loss  2.84 | avg loss  2.62 | ppl 13.77 - 2025-05-28 23:42:24,516 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.16000.ckpt - 2025-05-28 23:42:24,517 - log
eval samples: 0, loss: 0.9133280515670776 - 2025-05-28 23:42:24,630 - log
eval samples: 100, loss: 1.4265486001968384 - 2025-05-28 23:42:35,819 - log
eval samples: 200, loss: 1.2352440357208252 - 2025-05-28 23:42:47,017 - log
eval samples: 300, loss: 1.3398112058639526 - 2025-05-28 23:42:58,218 - log
eval samples: 400, loss: 1.4475083351135254 - 2025-05-28 23:43:09,422 - log
eval samples: 500, loss: 1.4915218353271484 - 2025-05-28 23:43:20,628 - log
eval samples: 600, loss: 1.522979736328125 - 2025-05-28 23:43:31,820 - log
eval samples: 700, loss: 1.2399332523345947 - 2025-05-28 23:43:43,030 - log
eval samples: 800, loss: 1.5455082654953003 - 2025-05-28 23:43:54,241 - log
eval samples: 900, loss: 1.2995519638061523 - 2025-05-28 23:44:05,459 - log
eval samples: 1000, loss: 1.3533782958984375 - 2025-05-28 23:44:16,687 - log
eval samples: 1100, loss: 1.128389596939087 - 2025-05-28 23:44:27,906 - log
average loss: +1.2753706002888614 - 2025-05-28 23:44:35,429 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:44:35,429 - log
| Eval   8 at step    16000 | time: 130.91s | valid loss  1.28 | valid ppl  3.58 | best ppl  3.58  - 2025-05-28 23:44:35,430 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:44:35,430 - log
| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 1546.89 | loss  2.66 | avg loss  2.64 | ppl 14.08 - 2025-05-28 23:44:59,205 - log
| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 237.66 | loss  2.68 | avg loss  2.62 | ppl 13.75 - 2025-05-28 23:45:22,972 - log
| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 237.88 | loss  2.33 | avg loss  2.67 | ppl 14.39 - 2025-05-28 23:45:46,760 - log
| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 237.98 | loss  2.48 | avg loss  2.62 | ppl 13.70 - 2025-05-28 23:46:10,692 - log
| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 237.59 | loss  2.96 | avg loss  2.64 | ppl 14.04 - 2025-05-28 23:46:34,451 - log
| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 238.14 | loss  2.85 | avg loss  2.64 | ppl 14.02 - 2025-05-28 23:46:58,265 - log
| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 238.04 | loss  2.61 | avg loss  2.65 | ppl 14.16 - 2025-05-28 23:47:22,070 - log
| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 237.76 | loss  2.89 | avg loss  2.66 | ppl 14.29 - 2025-05-28 23:47:45,846 - log
| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 237.73 | loss  2.83 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:48:09,620 - log
| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 237.57 | loss  2.47 | avg loss  2.64 | ppl 14.04 - 2025-05-28 23:48:33,378 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.17000.ckpt - 2025-05-28 23:48:33,379 - log
| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 237.57 | loss  2.65 | avg loss  2.65 | ppl 14.22 - 2025-05-28 23:48:57,136 - log
| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 237.38 | loss  2.61 | avg loss  2.63 | ppl 13.93 - 2025-05-28 23:49:20,874 - log
| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 237.54 | loss  3.01 | avg loss  2.64 | ppl 13.96 - 2025-05-28 23:49:44,629 - log
| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 237.73 | loss  2.54 | avg loss  2.65 | ppl 14.09 - 2025-05-28 23:50:08,402 - log
| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 237.23 | loss  2.74 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:50:32,126 - log
| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 237.29 | loss  2.70 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:50:55,855 - log
| epoch   4 step    17700 |   1926 batches | lr 6.66e-05 | ms/batch 237.77 | loss  2.25 | avg loss  2.63 | ppl 13.93 - 2025-05-28 23:51:19,632 - log
| epoch   4 step    17800 |   2026 batches | lr 6.58e-05 | ms/batch 237.65 | loss  2.39 | avg loss  2.65 | ppl 14.17 - 2025-05-28 23:51:43,397 - log
| epoch   4 step    17900 |   2126 batches | lr 6.51e-05 | ms/batch 237.52 | loss  2.39 | avg loss  2.65 | ppl 14.13 - 2025-05-28 23:52:07,150 - log
| epoch   4 step    18000 |   2226 batches | lr 6.43e-05 | ms/batch 237.74 | loss  2.68 | avg loss  2.64 | ppl 13.97 - 2025-05-28 23:52:30,924 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.18000.ckpt - 2025-05-28 23:52:30,924 - log
eval samples: 0, loss: 1.490827202796936 - 2025-05-28 23:52:31,038 - log
eval samples: 100, loss: 1.25664222240448 - 2025-05-28 23:52:42,310 - log
eval samples: 200, loss: 1.105890154838562 - 2025-05-28 23:52:53,569 - log
eval samples: 300, loss: 1.1861921548843384 - 2025-05-28 23:53:04,796 - log
eval samples: 400, loss: 1.1625500917434692 - 2025-05-28 23:53:16,033 - log
eval samples: 500, loss: 1.1545740365982056 - 2025-05-28 23:53:27,293 - log
eval samples: 600, loss: 1.392898678779602 - 2025-05-28 23:53:38,560 - log
eval samples: 700, loss: 1.387902021408081 - 2025-05-28 23:53:49,813 - log
eval samples: 800, loss: 1.4378466606140137 - 2025-05-28 23:54:01,095 - log
eval samples: 900, loss: 0.9580254554748535 - 2025-05-28 23:54:12,352 - log
eval samples: 1000, loss: 1.6278181076049805 - 2025-05-28 23:54:23,685 - log
eval samples: 1100, loss: 1.278861165046692 - 2025-05-28 23:54:35,011 - log
average loss: +1.2846725874465623 - 2025-05-28 23:54:42,632 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:54:42,632 - log
| Eval   9 at step    18000 | time: 131.71s | valid loss  1.28 | valid ppl  3.61 | best ppl  3.58  - 2025-05-28 23:54:42,632 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:54:42,632 - log
| epoch   4 step    18100 |   2326 batches | lr 6.35e-05 | ms/batch 1555.28 | loss  2.64 | avg loss  2.66 | ppl 14.37 - 2025-05-28 23:55:06,453 - log
| epoch   4 step    18200 |   2426 batches | lr 6.27e-05 | ms/batch 237.89 | loss  2.61 | avg loss  2.63 | ppl 13.90 - 2025-05-28 23:55:30,242 - log
| epoch   4 step    18300 |   2526 batches | lr 6.2e-05 | ms/batch 237.94 | loss  2.55 | avg loss  2.64 | ppl 13.95 - 2025-05-28 23:55:54,036 - log
| epoch   4 step    18400 |   2626 batches | lr 6.12e-05 | ms/batch 238.49 | loss  2.42 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:56:17,886 - log
| epoch   4 step    18500 |   2726 batches | lr 6.04e-05 | ms/batch 238.14 | loss  2.60 | avg loss  2.67 | ppl 14.43 - 2025-05-28 23:56:41,701 - log
| epoch   4 step    18600 |   2826 batches | lr 5.96e-05 | ms/batch 237.90 | loss  2.64 | avg loss  2.62 | ppl 13.75 - 2025-05-28 23:57:05,492 - log
| epoch   4 step    18700 |   2926 batches | lr 5.89e-05 | ms/batch 237.75 | loss  2.41 | avg loss  2.62 | ppl 13.69 - 2025-05-28 23:57:29,267 - log
| epoch   4 step    18800 |   3026 batches | lr 5.81e-05 | ms/batch 238.03 | loss  2.74 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:57:53,071 - log
| epoch   4 step    18900 |   3126 batches | lr 5.73e-05 | ms/batch 237.83 | loss  2.66 | avg loss  2.63 | ppl 13.81 - 2025-05-28 23:58:16,854 - log
| epoch   4 step    19000 |   3226 batches | lr 5.65e-05 | ms/batch 237.85 | loss  2.69 | avg loss  2.63 | ppl 13.85 - 2025-05-28 23:58:40,639 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.19000.ckpt - 2025-05-28 23:58:40,639 - log
| epoch   4 step    19100 |   3326 batches | lr 5.58e-05 | ms/batch 238.08 | loss  2.48 | avg loss  2.62 | ppl 13.74 - 2025-05-28 23:59:04,447 - log
| epoch   4 step    19200 |   3426 batches | lr 5.5e-05 | ms/batch 238.10 | loss  2.71 | avg loss  2.67 | ppl 14.50 - 2025-05-28 23:59:28,258 - log
| epoch   4 step    19300 |   3526 batches | lr 5.42e-05 | ms/batch 237.95 | loss  2.66 | avg loss  2.64 | ppl 14.01 - 2025-05-28 23:59:52,054 - log
| epoch   4 step    19400 |   3626 batches | lr 5.34e-05 | ms/batch 237.76 | loss  2.73 | avg loss  2.65 | ppl 14.13 - 2025-05-29 00:00:15,830 - log
| epoch   4 step    19500 |   3726 batches | lr 5.27e-05 | ms/batch 238.07 | loss  2.40 | avg loss  2.61 | ppl 13.66 - 2025-05-29 00:00:39,637 - log
| epoch   4 step    19600 |   3826 batches | lr 5.19e-05 | ms/batch 237.99 | loss  2.55 | avg loss  2.64 | ppl 14.01 - 2025-05-29 00:01:03,436 - log
| epoch   4 step    19700 |   3926 batches | lr 5.11e-05 | ms/batch 237.88 | loss  2.59 | avg loss  2.65 | ppl 14.15 - 2025-05-29 00:01:27,225 - log
| epoch   4 step    19800 |   4026 batches | lr 5.03e-05 | ms/batch 237.94 | loss  2.25 | avg loss  2.64 | ppl 13.97 - 2025-05-29 00:01:51,019 - log
| epoch   4 step    19900 |   4126 batches | lr 4.96e-05 | ms/batch 238.21 | loss  2.61 | avg loss  2.64 | ppl 14.04 - 2025-05-29 00:02:14,841 - log
| epoch   4 step    20000 |   4226 batches | lr 4.88e-05 | ms/batch 237.73 | loss  2.71 | avg loss  2.62 | ppl 13.79 - 2025-05-29 00:02:38,614 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.20000.ckpt - 2025-05-29 00:02:38,615 - log
eval samples: 0, loss: 1.4035695791244507 - 2025-05-29 00:02:38,727 - log
eval samples: 100, loss: 1.2429178953170776 - 2025-05-29 00:02:49,963 - log
eval samples: 200, loss: 1.1335124969482422 - 2025-05-29 00:03:01,197 - log
eval samples: 300, loss: 1.4653289318084717 - 2025-05-29 00:03:12,443 - log
eval samples: 400, loss: 1.0698843002319336 - 2025-05-29 00:03:23,684 - log
eval samples: 500, loss: 1.1708630323410034 - 2025-05-29 00:03:34,923 - log
eval samples: 600, loss: 1.6883670091629028 - 2025-05-29 00:03:46,211 - log
eval samples: 700, loss: 0.9250451326370239 - 2025-05-29 00:03:57,452 - log
eval samples: 800, loss: 1.4140985012054443 - 2025-05-29 00:04:08,724 - log
eval samples: 900, loss: 1.2364351749420166 - 2025-05-29 00:04:19,969 - log
eval samples: 1000, loss: 1.2289245128631592 - 2025-05-29 00:04:31,235 - log
eval samples: 1100, loss: 1.0417289733886719 - 2025-05-29 00:04:42,467 - log
average loss: +1.2744250399609134 - 2025-05-29 00:04:50,017 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:04:50,018 - log
| Eval  10 at step    20000 | time: 131.40s | valid loss  1.27 | valid ppl  3.58 | best ppl  3.58  - 2025-05-29 00:04:50,018 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:04:50,018 - log
| epoch   4 step    20100 |   4326 batches | lr 4.8e-05 | ms/batch 1552.30 | loss  2.67 | avg loss  2.62 | ppl 13.78 - 2025-05-29 00:05:13,845 - log
| epoch   4 step    20200 |   4426 batches | lr 4.72e-05 | ms/batch 238.14 | loss  2.46 | avg loss  2.62 | ppl 13.68 - 2025-05-29 00:05:37,659 - log
| epoch   4 step    20300 |   4526 batches | lr 4.65e-05 | ms/batch 238.09 | loss  2.30 | avg loss  2.66 | ppl 14.33 - 2025-05-29 00:06:01,469 - log
| epoch   4 step    20400 |   4626 batches | lr 4.57e-05 | ms/batch 238.20 | loss  2.74 | avg loss  2.65 | ppl 14.21 - 2025-05-29 00:06:25,290 - log
| epoch   4 step    20500 |   4726 batches | lr 4.49e-05 | ms/batch 238.05 | loss  2.60 | avg loss  2.62 | ppl 13.73 - 2025-05-29 00:06:49,096 - log
| epoch   4 step    20600 |   4826 batches | lr 4.41e-05 | ms/batch 237.95 | loss  2.70 | avg loss  2.63 | ppl 13.91 - 2025-05-29 00:07:12,891 - log
| epoch   4 step    20700 |   4926 batches | lr 4.34e-05 | ms/batch 237.93 | loss  2.72 | avg loss  2.65 | ppl 14.18 - 2025-05-29 00:07:36,685 - log
| epoch   4 step    20800 |   5026 batches | lr 4.26e-05 | ms/batch 238.08 | loss  2.32 | avg loss  2.63 | ppl 13.84 - 2025-05-29 00:08:00,494 - log
| epoch   4 step    20900 |   5126 batches | lr 4.18e-05 | ms/batch 238.02 | loss  2.65 | avg loss  2.63 | ppl 13.92 - 2025-05-29 00:08:24,296 - log
| epoch   4 step    21000 |   5226 batches | lr 4.1e-05 | ms/batch 237.98 | loss  2.50 | avg loss  2.66 | ppl 14.23 - 2025-05-29 00:08:48,095 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21000.ckpt - 2025-05-29 00:08:48,095 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21032.pkl - 2025-05-29 00:08:55,456 - log
start to train the model................ 5 - 2025-05-29 00:08:57,255 - log
| epoch   5 step    21100 |     68 batches | lr 4.02e-05 | ms/batch 162.13 | loss  2.45 | avg loss  2.67 | ppl 14.47 - 2025-05-29 00:09:13,468 - log
| epoch   5 step    21200 |    168 batches | lr 3.95e-05 | ms/batch 237.93 | loss  2.50 | avg loss  2.65 | ppl 14.08 - 2025-05-29 00:09:37,262 - log
| epoch   5 step    21300 |    268 batches | lr 3.87e-05 | ms/batch 237.97 | loss  2.58 | avg loss  2.61 | ppl 13.62 - 2025-05-29 00:10:01,060 - log
| epoch   5 step    21400 |    368 batches | lr 3.79e-05 | ms/batch 238.34 | loss  2.65 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:10:24,894 - log
| epoch   5 step    21500 |    468 batches | lr 3.71e-05 | ms/batch 237.93 | loss  2.38 | avg loss  2.62 | ppl 13.70 - 2025-05-29 00:10:48,687 - log
| epoch   5 step    21600 |    568 batches | lr 3.64e-05 | ms/batch 238.05 | loss  3.35 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:11:12,492 - log
| epoch   5 step    21700 |    668 batches | lr 3.56e-05 | ms/batch 238.35 | loss  2.76 | avg loss  2.62 | ppl 13.75 - 2025-05-29 00:11:36,328 - log
| epoch   5 step    21800 |    768 batches | lr 3.48e-05 | ms/batch 238.16 | loss  2.98 | avg loss  2.62 | ppl 13.74 - 2025-05-29 00:12:00,144 - log
| epoch   5 step    21900 |    868 batches | lr 3.4e-05 | ms/batch 238.24 | loss  2.49 | avg loss  2.59 | ppl 13.33 - 2025-05-29 00:12:23,969 - log
| epoch   5 step    22000 |    968 batches | lr 3.33e-05 | ms/batch 238.79 | loss  2.56 | avg loss  2.62 | ppl 13.80 - 2025-05-29 00:12:47,848 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.22000.ckpt - 2025-05-29 00:12:47,849 - log
eval samples: 0, loss: 1.4649550914764404 - 2025-05-29 00:12:47,962 - log
eval samples: 100, loss: 1.378952980041504 - 2025-05-29 00:12:59,189 - log
eval samples: 200, loss: 1.2507259845733643 - 2025-05-29 00:13:10,448 - log
eval samples: 300, loss: 1.2470675706863403 - 2025-05-29 00:13:21,688 - log
eval samples: 400, loss: 1.4162238836288452 - 2025-05-29 00:13:32,924 - log
eval samples: 500, loss: 1.2719230651855469 - 2025-05-29 00:13:44,156 - log
eval samples: 600, loss: 1.1327487230300903 - 2025-05-29 00:13:55,374 - log
eval samples: 700, loss: 1.009505033493042 - 2025-05-29 00:14:06,594 - log
eval samples: 800, loss: 1.11649751663208 - 2025-05-29 00:14:17,815 - log
eval samples: 900, loss: 1.453521490097046 - 2025-05-29 00:14:29,058 - log
eval samples: 1000, loss: 1.1007330417633057 - 2025-05-29 00:14:40,269 - log
eval samples: 1100, loss: 1.2721974849700928 - 2025-05-29 00:14:51,493 - log
average loss: +1.278125743292374 - 2025-05-29 00:14:59,003 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:14:59,003 - log
| Eval  11 at step    22000 | time: 131.15s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59  - 2025-05-29 00:14:59,004 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:14:59,004 - log
| epoch   5 step    22100 |   1068 batches | lr 3.25e-05 | ms/batch 1549.39 | loss  2.56 | avg loss  2.60 | ppl 13.52 - 2025-05-29 00:15:22,787 - log
| epoch   5 step    22200 |   1168 batches | lr 3.17e-05 | ms/batch 237.86 | loss  2.61 | avg loss  2.64 | ppl 14.01 - 2025-05-29 00:15:46,575 - log
| epoch   5 step    22300 |   1268 batches | lr 3.09e-05 | ms/batch 237.89 | loss  2.60 | avg loss  2.67 | ppl 14.50 - 2025-05-29 00:16:10,364 - log
| epoch   5 step    22400 |   1368 batches | lr 3.02e-05 | ms/batch 237.89 | loss  2.62 | avg loss  2.60 | ppl 13.49 - 2025-05-29 00:16:34,153 - log
| epoch   5 step    22500 |   1468 batches | lr 2.94e-05 | ms/batch 237.69 | loss  2.98 | avg loss  2.62 | ppl 13.74 - 2025-05-29 00:16:57,923 - log
| epoch   5 step    22600 |   1568 batches | lr 2.86e-05 | ms/batch 237.95 | loss  2.37 | avg loss  2.61 | ppl 13.65 - 2025-05-29 00:17:21,718 - log
| epoch   5 step    22700 |   1668 batches | lr 2.78e-05 | ms/batch 237.91 | loss  2.63 | avg loss  2.63 | ppl 13.89 - 2025-05-29 00:17:45,510 - log
| epoch   5 step    22800 |   1768 batches | lr 2.71e-05 | ms/batch 238.09 | loss  2.54 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:18:09,319 - log
| epoch   5 step    22900 |   1868 batches | lr 2.63e-05 | ms/batch 238.09 | loss  2.43 | avg loss  2.61 | ppl 13.58 - 2025-05-29 00:18:33,129 - log
| epoch   5 step    23000 |   1968 batches | lr 2.55e-05 | ms/batch 238.00 | loss  2.64 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:18:56,929 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.23000.ckpt - 2025-05-29 00:18:56,929 - log
| epoch   5 step    23100 |   2068 batches | lr 2.47e-05 | ms/batch 238.04 | loss  2.79 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:19:20,734 - log
| epoch   5 step    23200 |   2168 batches | lr 2.4e-05 | ms/batch 237.74 | loss  2.54 | avg loss  2.60 | ppl 13.51 - 2025-05-29 00:19:44,508 - log
| epoch   5 step    23300 |   2268 batches | lr 2.32e-05 | ms/batch 238.17 | loss  3.10 | avg loss  2.63 | ppl 13.93 - 2025-05-29 00:20:08,325 - log
| epoch   5 step    23400 |   2368 batches | lr 2.24e-05 | ms/batch 237.93 | loss  2.78 | avg loss  2.63 | ppl 13.81 - 2025-05-29 00:20:32,119 - log
| epoch   5 step    23500 |   2468 batches | lr 2.16e-05 | ms/batch 237.67 | loss  2.89 | avg loss  2.65 | ppl 14.18 - 2025-05-29 00:20:55,886 - log
| epoch   5 step    23600 |   2568 batches | lr 2.09e-05 | ms/batch 237.73 | loss  2.48 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:21:19,660 - log
| epoch   5 step    23700 |   2668 batches | lr 2.01e-05 | ms/batch 237.94 | loss  2.39 | avg loss  2.60 | ppl 13.48 - 2025-05-29 00:21:43,454 - log
| epoch   5 step    23800 |   2768 batches | lr 1.93e-05 | ms/batch 237.86 | loss  2.58 | avg loss  2.60 | ppl 13.47 - 2025-05-29 00:22:07,241 - log
| epoch   5 step    23900 |   2868 batches | lr 1.85e-05 | ms/batch 237.73 | loss  2.90 | avg loss  2.63 | ppl 13.82 - 2025-05-29 00:22:31,015 - log
| epoch   5 step    24000 |   2968 batches | lr 1.78e-05 | ms/batch 238.07 | loss  2.70 | avg loss  2.65 | ppl 14.10 - 2025-05-29 00:22:54,823 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.24000.ckpt - 2025-05-29 00:22:54,823 - log
eval samples: 0, loss: 1.519622564315796 - 2025-05-29 00:22:54,936 - log
eval samples: 100, loss: 1.1023486852645874 - 2025-05-29 00:23:06,188 - log
eval samples: 200, loss: 1.3553576469421387 - 2025-05-29 00:23:17,417 - log
eval samples: 300, loss: 1.1929954290390015 - 2025-05-29 00:23:28,653 - log
eval samples: 400, loss: 1.008903980255127 - 2025-05-29 00:23:39,892 - log
eval samples: 500, loss: 1.6253399848937988 - 2025-05-29 00:23:51,129 - log
eval samples: 600, loss: 1.3831034898757935 - 2025-05-29 00:24:02,368 - log
eval samples: 700, loss: 1.1554266214370728 - 2025-05-29 00:24:13,595 - log
eval samples: 800, loss: 1.1323539018630981 - 2025-05-29 00:24:24,856 - log
eval samples: 900, loss: 1.40337073802948 - 2025-05-29 00:24:36,113 - log
eval samples: 1000, loss: 1.0282998085021973 - 2025-05-29 00:24:47,354 - log
eval samples: 1100, loss: 1.1170381307601929 - 2025-05-29 00:24:58,589 - log
average loss: +1.277289811317643 - 2025-05-29 00:25:06,103 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:25:06,103 - log
| Eval  12 at step    24000 | time: 131.28s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59  - 2025-05-29 00:25:06,103 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:25:06,103 - log
| epoch   5 step    24100 |   3068 batches | lr 1.7e-05 | ms/batch 1550.82 | loss  2.80 | avg loss  2.64 | ppl 14.03 - 2025-05-29 00:25:29,905 - log
| epoch   5 step    24200 |   3168 batches | lr 1.62e-05 | ms/batch 238.28 | loss  2.73 | avg loss  2.67 | ppl 14.44 - 2025-05-29 00:25:53,733 - log
| epoch   5 step    24300 |   3268 batches | lr 1.54e-05 | ms/batch 238.04 | loss  2.71 | avg loss  2.64 | ppl 14.06 - 2025-05-29 00:26:17,538 - log
| epoch   5 step    24400 |   3368 batches | lr 1.47e-05 | ms/batch 238.16 | loss  2.73 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:26:41,355 - log
| epoch   5 step    24500 |   3468 batches | lr 1.39e-05 | ms/batch 238.27 | loss  2.41 | avg loss  2.64 | ppl 13.95 - 2025-05-29 00:27:05,182 - log
| epoch   5 step    24600 |   3568 batches | lr 1.31e-05 | ms/batch 238.00 | loss  2.63 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:27:28,982 - log
| epoch   5 step    24700 |   3668 batches | lr 1.23e-05 | ms/batch 238.05 | loss  2.88 | avg loss  2.63 | ppl 13.82 - 2025-05-29 00:27:52,788 - log
| epoch   5 step    24800 |   3768 batches | lr 1.16e-05 | ms/batch 238.20 | loss  2.24 | avg loss  2.58 | ppl 13.20 - 2025-05-29 00:28:16,608 - log
| epoch   5 step    24900 |   3868 batches | lr 1.08e-05 | ms/batch 237.98 | loss  2.78 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:28:40,407 - log
| epoch   5 step    25000 |   3968 batches | lr 1e-05 | ms/batch 238.04 | loss  2.76 | avg loss  2.64 | ppl 14.02 - 2025-05-29 00:29:04,212 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.25000.ckpt - 2025-05-29 00:29:04,212 - log
| epoch   5 step    25100 |   4068 batches | lr 9.23e-06 | ms/batch 238.05 | loss  2.63 | avg loss  2.61 | ppl 13.55 - 2025-05-29 00:29:28,017 - log
| epoch   5 step    25200 |   4168 batches | lr 8.45e-06 | ms/batch 238.21 | loss  2.44 | avg loss  2.63 | ppl 13.93 - 2025-05-29 00:29:51,839 - log
| epoch   5 step    25300 |   4268 batches | lr 7.68e-06 | ms/batch 238.89 | loss  3.01 | avg loss  2.60 | ppl 13.48 - 2025-05-29 00:30:15,728 - log
| epoch   5 step    25400 |   4368 batches | lr 6.9e-06 | ms/batch 238.78 | loss  2.52 | avg loss  2.63 | ppl 13.91 - 2025-05-29 00:30:39,606 - log
| epoch   5 step    25500 |   4468 batches | lr 6.13e-06 | ms/batch 238.92 | loss  2.33 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:31:03,499 - log
| epoch   5 step    25600 |   4568 batches | lr 5.35e-06 | ms/batch 238.85 | loss  2.70 | avg loss  2.65 | ppl 14.10 - 2025-05-29 00:31:27,385 - log
| epoch   5 step    25700 |   4668 batches | lr 4.58e-06 | ms/batch 238.22 | loss  2.45 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:31:51,208 - log
| epoch   5 step    25800 |   4768 batches | lr 3.8e-06 | ms/batch 238.35 | loss  2.53 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:32:15,043 - log
| epoch   5 step    25900 |   4868 batches | lr 3.02e-06 | ms/batch 238.26 | loss  2.72 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:32:38,870 - log
| epoch   5 step    26000 |   4968 batches | lr 2.25e-06 | ms/batch 238.40 | loss  2.74 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:33:02,710 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26000.ckpt - 2025-05-29 00:33:02,711 - log
eval samples: 0, loss: 0.9992844462394714 - 2025-05-29 00:33:02,824 - log
eval samples: 100, loss: 1.017942190170288 - 2025-05-29 00:33:14,095 - log
eval samples: 200, loss: 1.122837781906128 - 2025-05-29 00:33:25,357 - log
eval samples: 300, loss: 1.7177746295928955 - 2025-05-29 00:33:36,665 - log
eval samples: 400, loss: 0.9698410034179688 - 2025-05-29 00:33:47,938 - log
eval samples: 500, loss: 1.6140111684799194 - 2025-05-29 00:33:59,214 - log
eval samples: 600, loss: 1.2799748182296753 - 2025-05-29 00:34:10,456 - log
eval samples: 700, loss: 1.5499166250228882 - 2025-05-29 00:34:21,721 - log
eval samples: 800, loss: 0.9487221240997314 - 2025-05-29 00:34:32,977 - log
eval samples: 900, loss: 1.5838167667388916 - 2025-05-29 00:34:44,227 - log
eval samples: 1000, loss: 1.4388667345046997 - 2025-05-29 00:34:55,465 - log
eval samples: 1100, loss: 1.3310248851776123 - 2025-05-29 00:35:06,720 - log
average loss: +1.2717146032477078 - 2025-05-29 00:35:14,253 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:35:14,254 - log
| Eval  13 at step    26000 | time: 131.54s | valid loss  1.27 | valid ppl  3.57 | best ppl  3.57  - 2025-05-29 00:35:14,254 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:35:14,254 - log
| epoch   5 step    26100 |   5068 batches | lr 1.47e-06 | ms/batch 1553.59 | loss  2.65 | avg loss  2.61 | ppl 13.64 - 2025-05-29 00:35:38,069 - log
| epoch   5 step    26200 |   5168 batches | lr 6.98e-07 | ms/batch 238.08 | loss  2.92 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:36:01,878 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26290.pkl - 2025-05-29 00:36:23,051 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:36:24,830 - log
End of training - 2025-05-29 00:36:24,830 - log
ms/batch 301.44 - 2025-05-29 00:36:24,831 - log
