==================================================================================================== - 2025-05-28 22:23:31,821 - log
        - random_seed : 110 - 2025-05-28 22:23:31,821 - log
        - lr : 0.0002 - 2025-05-28 22:23:31,821 - log
        - weight_decay : 0.01 - 2025-05-28 22:23:31,821 - log
        - correct_bias : False - 2025-05-28 22:23:31,821 - log
        - adam_epislon : 1e-06 - 2025-05-28 22:23:31,821 - log
        - no_decay_bias : False - 2025-05-28 22:23:31,821 - log
        - adam_beta1 : 0.9 - 2025-05-28 22:23:31,821 - log
        - adam_beta2 : 0.999 - 2025-05-28 22:23:31,821 - log
        - scheduler : linear - 2025-05-28 22:23:31,821 - log
        - max_step : None - 2025-05-28 22:23:31,821 - log
        - max_epoch : 5 - 2025-05-28 22:23:31,821 - log
        - warmup_step : 500 - 2025-05-28 22:23:31,821 - log
        - i_steps : 0 - 2025-05-28 22:23:31,821 - log
        - i_lrs : 0.00025 - 2025-05-28 22:23:31,821 - log
        - train_data : ./data/e2e/train.jsonl - 2025-05-28 22:23:31,821 - log
        - valid_data : ./data/e2e/valid.jsonl - 2025-05-28 22:23:31,821 - log
        - train_batch_size : 8 - 2025-05-28 22:23:31,821 - log
        - valid_batch_size : 4 - 2025-05-28 22:23:31,821 - log
        - grad_acc : 2 - 2025-05-28 22:23:31,821 - log
        - clip : 0.0 - 2025-05-28 22:23:31,821 - log
        - seq_len : 512 - 2025-05-28 22:23:31,821 - log
        - model_card : gpt2.sm - 2025-05-28 22:23:31,822 - log
        - init_checkpoint : ./pretrained_checkpoints/gpt2-pytorch_model.bin - 2025-05-28 22:23:31,822 - log
        - fp16 : False - 2025-05-28 22:23:31,822 - log
        - log_interval : 100 - 2025-05-28 22:23:31,822 - log
        - eval_interval : 2000 - 2025-05-28 22:23:31,822 - log
        - save_interval : 1000 - 2025-05-28 22:23:31,822 - log
        - work_dir : ./trained_models/GPT2_M/e2e - 2025-05-28 22:23:31,822 - log
        - lora_dim : 4 - 2025-05-28 22:23:31,822 - log
        - lora_alpha : 32 - 2025-05-28 22:23:31,822 - log
        - obj : clm - 2025-05-28 22:23:31,822 - log
        - lora_dropout : 0.1 - 2025-05-28 22:23:31,822 - log
        - label_smooth : 0.1 - 2025-05-28 22:23:31,822 - log
        - roll_interval : -1 - 2025-05-28 22:23:31,822 - log
        - roll_lr : 1e-05 - 2025-05-28 22:23:31,822 - log
        - roll_step : 100 - 2025-05-28 22:23:31,822 - log
        - eval_epoch : 1 - 2025-05-28 22:23:31,822 - log
        - device : cuda - 2025-05-28 22:23:31,822 - log
==================================================================================================== - 2025-05-28 22:23:31,822 - log
--------------------------------------------------train-------------------------------------------------- - 2025-05-28 22:23:31,822 - log
loading model pretrained weight. - 2025-05-28 22:23:32,233 - log
set max_step: 26290 - 2025-05-28 22:23:33,176 - log
start to train the model................ 1 - 2025-05-28 22:23:33,177 - log
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 368.80 | loss  5.49 | avg loss  5.85 | ppl 345.94 - 2025-05-28 22:24:10,056 - log
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 234.67 | loss  4.11 | avg loss  4.69 | ppl 108.57 - 2025-05-28 22:24:33,524 - log
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 235.30 | loss  3.50 | avg loss  3.48 | ppl 32.45 - 2025-05-28 22:24:57,055 - log
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 235.82 | loss  3.28 | avg loss  3.24 | ppl 25.50 - 2025-05-28 22:25:20,638 - log
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 236.38 | loss  2.99 | avg loss  3.12 | ppl 22.74 - 2025-05-28 22:25:44,277 - log
| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 236.97 | loss  3.53 | avg loss  3.09 | ppl 21.95 - 2025-05-28 22:26:07,974 - log
| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 237.15 | loss  3.21 | avg loss  3.04 | ppl 20.80 - 2025-05-28 22:26:31,690 - log
| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 236.95 | loss  3.06 | avg loss  2.96 | ppl 19.31 - 2025-05-28 22:26:55,385 - log
| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 237.30 | loss  2.80 | avg loss  2.96 | ppl 19.35 - 2025-05-28 22:27:19,116 - log
| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 237.29 | loss  2.89 | avg loss  2.98 | ppl 19.61 - 2025-05-28 22:27:42,845 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.1000.ckpt - 2025-05-28 22:27:42,845 - log
| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 237.05 | loss  3.01 | avg loss  2.95 | ppl 19.06 - 2025-05-28 22:28:06,551 - log
| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 237.31 | loss  2.60 | avg loss  2.97 | ppl 19.49 - 2025-05-28 22:28:30,282 - log
| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 237.05 | loss  3.17 | avg loss  2.89 | ppl 17.95 - 2025-05-28 22:28:53,987 - log
| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 237.09 | loss  2.81 | avg loss  2.87 | ppl 17.57 - 2025-05-28 22:29:17,696 - log
| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 237.04 | loss  2.61 | avg loss  2.87 | ppl 17.66 - 2025-05-28 22:29:41,401 - log
| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 237.32 | loss  2.55 | avg loss  2.90 | ppl 18.18 - 2025-05-28 22:30:05,134 - log
| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 237.59 | loss  3.05 | avg loss  2.86 | ppl 17.49 - 2025-05-28 22:30:28,893 - log
| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 237.01 | loss  2.97 | avg loss  2.86 | ppl 17.45 - 2025-05-28 22:30:52,594 - log
| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 237.27 | loss  3.04 | avg loss  2.86 | ppl 17.41 - 2025-05-28 22:31:16,322 - log
| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 237.49 | loss  2.86 | avg loss  2.85 | ppl 17.22 - 2025-05-28 22:31:40,072 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.2000.ckpt - 2025-05-28 22:31:40,072 - log
eval samples: 0, loss: 1.3311632871627808 - 2025-05-28 22:31:40,187 - log
eval samples: 100, loss: 1.7324848175048828 - 2025-05-28 22:31:51,424 - log
eval samples: 200, loss: 1.3950703144073486 - 2025-05-28 22:32:02,636 - log
eval samples: 300, loss: 1.0134600400924683 - 2025-05-28 22:32:13,891 - log
eval samples: 400, loss: 1.2134376764297485 - 2025-05-28 22:32:25,125 - log
eval samples: 500, loss: 1.395693302154541 - 2025-05-28 22:32:36,354 - log
eval samples: 600, loss: 1.7396842241287231 - 2025-05-28 22:32:47,571 - log
eval samples: 700, loss: 1.3864680528640747 - 2025-05-28 22:32:58,802 - log
eval samples: 800, loss: 1.2592263221740723 - 2025-05-28 22:33:10,021 - log
eval samples: 900, loss: 1.3506672382354736 - 2025-05-28 22:33:21,249 - log
eval samples: 1000, loss: 1.3702534437179565 - 2025-05-28 22:33:32,468 - log
eval samples: 1100, loss: 1.7569873332977295 - 2025-05-28 22:33:43,684 - log
average loss: +1.4628554549935746 - 2025-05-28 22:33:51,238 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:33:51,238 - log
| Eval   1 at step     2000 | time: 131.16s | valid loss  1.46 | valid ppl  4.32 | best ppl  4.32  - 2025-05-28 22:33:51,238 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:33:51,238 - log
| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 1549.11 | loss  3.16 | avg loss  2.84 | ppl 17.06 - 2025-05-28 22:34:14,983 - log
| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 237.14 | loss  2.77 | avg loss  2.84 | ppl 17.11 - 2025-05-28 22:34:38,697 - log
| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 237.40 | loss  3.24 | avg loss  2.85 | ppl 17.31 - 2025-05-28 22:35:02,438 - log
| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 237.18 | loss  2.57 | avg loss  2.80 | ppl 16.45 - 2025-05-28 22:35:26,157 - log
| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 237.31 | loss  3.26 | avg loss  2.86 | ppl 17.42 - 2025-05-28 22:35:49,888 - log
| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 237.01 | loss  2.68 | avg loss  2.82 | ppl 16.75 - 2025-05-28 22:36:13,590 - log
| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 237.31 | loss  2.87 | avg loss  2.81 | ppl 16.67 - 2025-05-28 22:36:37,322 - log
| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 237.21 | loss  2.82 | avg loss  2.82 | ppl 16.70 - 2025-05-28 22:37:01,044 - log
| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 237.14 | loss  2.73 | avg loss  2.81 | ppl 16.54 - 2025-05-28 22:37:24,758 - log
| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 237.55 | loss  2.85 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:37:48,514 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.3000.ckpt - 2025-05-28 22:37:48,514 - log
| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 237.34 | loss  2.75 | avg loss  2.83 | ppl 16.89 - 2025-05-28 22:38:12,248 - log
| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 237.65 | loss  3.06 | avg loss  2.81 | ppl 16.68 - 2025-05-28 22:38:36,014 - log
| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 237.28 | loss  2.83 | avg loss  2.79 | ppl 16.32 - 2025-05-28 22:38:59,743 - log
| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 237.53 | loss  2.62 | avg loss  2.78 | ppl 16.08 - 2025-05-28 22:39:23,496 - log
| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 237.45 | loss  3.12 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:39:47,242 - log
| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 237.21 | loss  2.76 | avg loss  2.76 | ppl 15.83 - 2025-05-28 22:40:10,964 - log
| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 237.35 | loss  2.53 | avg loss  2.77 | ppl 15.95 - 2025-05-28 22:40:34,700 - log
| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 237.36 | loss  2.92 | avg loss  2.78 | ppl 16.17 - 2025-05-28 22:40:58,436 - log
| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 237.53 | loss  2.80 | avg loss  2.78 | ppl 16.20 - 2025-05-28 22:41:22,190 - log
| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 237.39 | loss  2.61 | avg loss  2.79 | ppl 16.31 - 2025-05-28 22:41:45,930 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.4000.ckpt - 2025-05-28 22:41:45,930 - log
eval samples: 0, loss: 1.6549922227859497 - 2025-05-28 22:41:46,043 - log
eval samples: 100, loss: 1.4869531393051147 - 2025-05-28 22:41:57,273 - log
eval samples: 200, loss: 1.1642392873764038 - 2025-05-28 22:42:08,516 - log
eval samples: 300, loss: 1.2595221996307373 - 2025-05-28 22:42:19,755 - log
eval samples: 400, loss: 1.8051190376281738 - 2025-05-28 22:42:30,976 - log
eval samples: 500, loss: 1.2350364923477173 - 2025-05-28 22:42:42,188 - log
eval samples: 600, loss: 1.403570532798767 - 2025-05-28 22:42:53,392 - log
eval samples: 700, loss: 1.4691437482833862 - 2025-05-28 22:43:04,642 - log
eval samples: 800, loss: 1.6780551671981812 - 2025-05-28 22:43:15,888 - log
eval samples: 900, loss: 1.6091654300689697 - 2025-05-28 22:43:27,113 - log
eval samples: 1000, loss: 1.374427080154419 - 2025-05-28 22:43:38,326 - log
eval samples: 1100, loss: 1.2569255828857422 - 2025-05-28 22:43:49,535 - log
average loss: +1.396705282826538 - 2025-05-28 22:43:57,047 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:43:57,047 - log
| Eval   2 at step     4000 | time: 131.11s | valid loss  1.40 | valid ppl  4.04 | best ppl  4.04  - 2025-05-28 22:43:57,047 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:43:57,047 - log
| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 1548.08 | loss  2.69 | avg loss  2.80 | ppl 16.43 - 2025-05-28 22:44:20,738 - log
| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 237.17 | loss  2.42 | avg loss  2.77 | ppl 15.99 - 2025-05-28 22:44:44,455 - log
| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 237.28 | loss  2.81 | avg loss  2.73 | ppl 15.31 - 2025-05-28 22:45:08,184 - log
| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 236.95 | loss  2.61 | avg loss  2.78 | ppl 16.14 - 2025-05-28 22:45:31,880 - log
| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 237.14 | loss  3.28 | avg loss  2.77 | ppl 16.02 - 2025-05-28 22:45:55,595 - log
| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 237.25 | loss  2.77 | avg loss  2.76 | ppl 15.87 - 2025-05-28 22:46:19,321 - log
| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 237.13 | loss  2.80 | avg loss  2.75 | ppl 15.59 - 2025-05-28 22:46:43,034 - log
| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 237.26 | loss  2.72 | avg loss  2.74 | ppl 15.44 - 2025-05-28 22:47:06,761 - log
| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 237.32 | loss  2.65 | avg loss  2.75 | ppl 15.65 - 2025-05-28 22:47:30,494 - log
| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 237.21 | loss  2.78 | avg loss  2.75 | ppl 15.71 - 2025-05-28 22:47:54,215 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5000.ckpt - 2025-05-28 22:47:54,216 - log
| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 237.13 | loss  2.79 | avg loss  2.71 | ppl 15.04 - 2025-05-28 22:48:17,929 - log
| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 237.28 | loss  2.71 | avg loss  2.77 | ppl 16.01 - 2025-05-28 22:48:41,657 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.5258.pkl - 2025-05-28 22:48:55,166 - log
start to train the model................ 2 - 2025-05-28 22:48:57,134 - log
| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 99.91 | loss  2.53 | avg loss  2.76 | ppl 15.74 - 2025-05-28 22:49:07,125 - log
| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 236.88 | loss  2.57 | avg loss  2.72 | ppl 15.23 - 2025-05-28 22:49:30,813 - log
| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 237.04 | loss  2.63 | avg loss  2.75 | ppl 15.65 - 2025-05-28 22:49:54,518 - log
| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 237.20 | loss  2.59 | avg loss  2.71 | ppl 15.04 - 2025-05-28 22:50:18,239 - log
| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 237.20 | loss  2.53 | avg loss  2.70 | ppl 14.91 - 2025-05-28 22:50:41,959 - log
| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 237.27 | loss  2.45 | avg loss  2.70 | ppl 14.85 - 2025-05-28 22:51:05,686 - log
| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 237.48 | loss  2.80 | avg loss  2.74 | ppl 15.56 - 2025-05-28 22:51:29,434 - log
| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 237.82 | loss  2.61 | avg loss  2.74 | ppl 15.54 - 2025-05-28 22:51:53,217 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.6000.ckpt - 2025-05-28 22:51:53,218 - log
eval samples: 0, loss: 1.1115864515304565 - 2025-05-28 22:51:53,331 - log
eval samples: 100, loss: 1.3974859714508057 - 2025-05-28 22:52:04,580 - log
eval samples: 200, loss: 1.4889051914215088 - 2025-05-28 22:52:15,798 - log
eval samples: 300, loss: 1.583939552307129 - 2025-05-28 22:52:27,056 - log
eval samples: 400, loss: 1.2336955070495605 - 2025-05-28 22:52:38,320 - log
eval samples: 500, loss: 1.3424943685531616 - 2025-05-28 22:52:49,554 - log
eval samples: 600, loss: 1.7084143161773682 - 2025-05-28 22:53:00,769 - log
eval samples: 700, loss: 1.4570744037628174 - 2025-05-28 22:53:11,998 - log
eval samples: 800, loss: 1.1419869661331177 - 2025-05-28 22:53:23,201 - log
eval samples: 900, loss: 1.6875618696212769 - 2025-05-28 22:53:34,406 - log
eval samples: 1000, loss: 1.4060337543487549 - 2025-05-28 22:53:45,652 - log
eval samples: 1100, loss: 1.1417508125305176 - 2025-05-28 22:53:56,868 - log
average loss: +1.358503176333153 - 2025-05-28 22:54:04,380 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:54:04,381 - log
| Eval   3 at step     6000 | time: 131.16s | valid loss  1.36 | valid ppl  3.89 | best ppl  3.89  - 2025-05-28 22:54:04,381 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 22:54:04,381 - log
| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 1549.61 | loss  2.68 | avg loss  2.72 | ppl 15.25 - 2025-05-28 22:54:28,179 - log
| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 237.54 | loss  2.62 | avg loss  2.69 | ppl 14.80 - 2025-05-28 22:54:51,933 - log
| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 237.56 | loss  2.72 | avg loss  2.73 | ppl 15.35 - 2025-05-28 22:55:15,689 - log
| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 237.31 | loss  3.11 | avg loss  2.73 | ppl 15.38 - 2025-05-28 22:55:39,421 - log
| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 237.55 | loss  2.86 | avg loss  2.73 | ppl 15.40 - 2025-05-28 22:56:03,177 - log
| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 237.62 | loss  2.97 | avg loss  2.67 | ppl 14.49 - 2025-05-28 22:56:26,939 - log
| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 237.96 | loss  2.67 | avg loss  2.73 | ppl 15.36 - 2025-05-28 22:56:50,736 - log
| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 238.22 | loss  2.92 | avg loss  2.71 | ppl 15.09 - 2025-05-28 22:57:14,558 - log
| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 238.28 | loss  2.70 | avg loss  2.69 | ppl 14.73 - 2025-05-28 22:57:38,386 - log
| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 237.60 | loss  2.59 | avg loss  2.72 | ppl 15.16 - 2025-05-28 22:58:02,146 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.7000.ckpt - 2025-05-28 22:58:02,147 - log
| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 237.76 | loss  2.70 | avg loss  2.71 | ppl 15.03 - 2025-05-28 22:58:25,923 - log
| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 237.50 | loss  3.02 | avg loss  2.70 | ppl 14.88 - 2025-05-28 22:58:49,674 - log
| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 237.78 | loss  2.55 | avg loss  2.73 | ppl 15.31 - 2025-05-28 22:59:13,452 - log
| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 238.51 | loss  2.65 | avg loss  2.73 | ppl 15.41 - 2025-05-28 22:59:37,304 - log
| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 237.40 | loss  2.85 | avg loss  2.71 | ppl 15.10 - 2025-05-28 23:00:01,044 - log
| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 237.16 | loss  2.36 | avg loss  2.68 | ppl 14.52 - 2025-05-28 23:00:24,760 - log
| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 237.50 | loss  2.68 | avg loss  2.71 | ppl 15.01 - 2025-05-28 23:00:48,510 - log
| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 237.39 | loss  2.69 | avg loss  2.72 | ppl 15.12 - 2025-05-28 23:01:12,249 - log
| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 237.14 | loss  2.63 | avg loss  2.71 | ppl 15.04 - 2025-05-28 23:01:35,964 - log
| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 237.22 | loss  2.69 | avg loss  2.74 | ppl 15.41 - 2025-05-28 23:01:59,686 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.8000.ckpt - 2025-05-28 23:01:59,686 - log
eval samples: 0, loss: 1.3126604557037354 - 2025-05-28 23:01:59,800 - log
eval samples: 100, loss: 1.356477975845337 - 2025-05-28 23:02:11,006 - log
eval samples: 200, loss: 0.979518473148346 - 2025-05-28 23:02:22,226 - log
eval samples: 300, loss: 1.2098244428634644 - 2025-05-28 23:02:33,427 - log
eval samples: 400, loss: 1.1514136791229248 - 2025-05-28 23:02:44,613 - log
eval samples: 500, loss: 1.2979331016540527 - 2025-05-28 23:02:55,829 - log
eval samples: 600, loss: 1.8207266330718994 - 2025-05-28 23:03:07,033 - log
eval samples: 700, loss: 1.3808382749557495 - 2025-05-28 23:03:18,255 - log
eval samples: 800, loss: 1.5667622089385986 - 2025-05-28 23:03:29,488 - log
eval samples: 900, loss: 1.286858081817627 - 2025-05-28 23:03:40,698 - log
eval samples: 1000, loss: 1.607016921043396 - 2025-05-28 23:03:51,886 - log
eval samples: 1100, loss: 1.1297926902770996 - 2025-05-28 23:04:03,058 - log
average loss: +1.3333840376301989 - 2025-05-28 23:04:10,553 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:04:10,553 - log
| Eval   4 at step     8000 | time: 130.86s | valid loss  1.33 | valid ppl  3.79 | best ppl  3.79  - 2025-05-28 23:04:10,553 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:04:10,553 - log
| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 1545.98 | loss  2.80 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:04:34,284 - log
| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 237.24 | loss  2.68 | avg loss  2.68 | ppl 14.65 - 2025-05-28 23:04:58,008 - log
| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 237.34 | loss  2.81 | avg loss  2.70 | ppl 14.83 - 2025-05-28 23:05:21,743 - log
| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 237.22 | loss  2.38 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:05:45,466 - log
| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 237.09 | loss  2.86 | avg loss  2.73 | ppl 15.35 - 2025-05-28 23:06:09,175 - log
| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 236.90 | loss  2.61 | avg loss  2.71 | ppl 15.06 - 2025-05-28 23:06:32,866 - log
| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 236.74 | loss  2.78 | avg loss  2.70 | ppl 14.86 - 2025-05-28 23:06:56,540 - log
| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 237.22 | loss  2.59 | avg loss  2.67 | ppl 14.47 - 2025-05-28 23:07:20,263 - log
| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 237.32 | loss  2.41 | avg loss  2.69 | ppl 14.68 - 2025-05-28 23:07:43,995 - log
| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 236.76 | loss  2.81 | avg loss  2.70 | ppl 14.86 - 2025-05-28 23:08:07,671 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.9000.ckpt - 2025-05-28 23:08:07,672 - log
| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 237.34 | loss  2.73 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:08:31,406 - log
| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 237.25 | loss  2.18 | avg loss  2.70 | ppl 14.88 - 2025-05-28 23:08:55,132 - log
| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 237.03 | loss  2.58 | avg loss  2.69 | ppl 14.71 - 2025-05-28 23:09:18,835 - log
| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 237.11 | loss  2.65 | avg loss  2.69 | ppl 14.67 - 2025-05-28 23:09:42,546 - log
| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 237.08 | loss  2.82 | avg loss  2.73 | ppl 15.35 - 2025-05-28 23:10:06,254 - log
| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 237.41 | loss  2.48 | avg loss  2.71 | ppl 15.08 - 2025-05-28 23:10:29,995 - log
| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 237.15 | loss  2.33 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:10:53,711 - log
| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 237.18 | loss  2.52 | avg loss  2.66 | ppl 14.30 - 2025-05-28 23:11:17,429 - log
| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 237.34 | loss  3.11 | avg loss  2.72 | ppl 15.19 - 2025-05-28 23:11:41,163 - log
| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 237.19 | loss  2.55 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:12:04,882 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10000.ckpt - 2025-05-28 23:12:04,883 - log
eval samples: 0, loss: 1.0558615922927856 - 2025-05-28 23:12:04,995 - log
eval samples: 100, loss: 1.3361111879348755 - 2025-05-28 23:12:16,213 - log
eval samples: 200, loss: 1.7232375144958496 - 2025-05-28 23:12:27,425 - log
eval samples: 300, loss: 1.3218919038772583 - 2025-05-28 23:12:38,627 - log
eval samples: 400, loss: 1.3240370750427246 - 2025-05-28 23:12:49,824 - log
eval samples: 500, loss: 1.4658758640289307 - 2025-05-28 23:13:01,059 - log
eval samples: 600, loss: 1.5875098705291748 - 2025-05-28 23:13:12,263 - log
eval samples: 700, loss: 1.6034296751022339 - 2025-05-28 23:13:23,448 - log
eval samples: 800, loss: 1.3790138959884644 - 2025-05-28 23:13:34,651 - log
eval samples: 900, loss: 1.410516619682312 - 2025-05-28 23:13:45,870 - log
eval samples: 1000, loss: 1.309606909751892 - 2025-05-28 23:13:57,072 - log
eval samples: 1100, loss: 1.2331587076187134 - 2025-05-28 23:14:08,281 - log
average loss: +1.3155945563459233 - 2025-05-28 23:14:15,801 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:14:15,801 - log
| Eval   5 at step    10000 | time: 130.92s | valid loss  1.32 | valid ppl  3.73 | best ppl  3.73  - 2025-05-28 23:14:15,801 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:14:15,801 - log
| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 1546.95 | loss  2.54 | avg loss  2.67 | ppl 14.50 - 2025-05-28 23:14:39,578 - log
| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 237.48 | loss  2.83 | avg loss  2.66 | ppl 14.26 - 2025-05-28 23:15:03,326 - log
| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 237.45 | loss  2.66 | avg loss  2.73 | ppl 15.28 - 2025-05-28 23:15:27,071 - log
| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 237.56 | loss  2.82 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:15:50,828 - log
| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 237.30 | loss  2.52 | avg loss  2.68 | ppl 14.52 - 2025-05-28 23:16:14,558 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.10516.pkl - 2025-05-28 23:16:18,114 - log
start to train the model................ 3 - 2025-05-28 23:16:19,909 - log
| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 199.57 | loss  2.17 | avg loss  2.70 | ppl 14.91 - 2025-05-28 23:16:39,866 - log
| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 237.47 | loss  2.44 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:17:03,614 - log
| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 237.39 | loss  2.73 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:17:27,354 - log
| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 237.59 | loss  2.54 | avg loss  2.67 | ppl 14.43 - 2025-05-28 23:17:51,113 - log
| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 237.45 | loss  2.68 | avg loss  2.69 | ppl 14.69 - 2025-05-28 23:18:14,859 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.11000.ckpt - 2025-05-28 23:18:14,859 - log
| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 237.49 | loss  2.79 | avg loss  2.69 | ppl 14.70 - 2025-05-28 23:18:38,608 - log
| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 237.35 | loss  2.50 | avg loss  2.66 | ppl 14.36 - 2025-05-28 23:19:02,343 - log
| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 237.37 | loss  2.62 | avg loss  2.67 | ppl 14.46 - 2025-05-28 23:19:26,081 - log
| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 237.47 | loss  2.78 | avg loss  2.66 | ppl 14.31 - 2025-05-28 23:19:49,829 - log
| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 237.13 | loss  2.87 | avg loss  2.64 | ppl 14.05 - 2025-05-28 23:20:13,542 - log
| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 237.34 | loss  2.99 | avg loss  2.68 | ppl 14.56 - 2025-05-28 23:20:37,277 - log
| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 237.33 | loss  3.32 | avg loss  2.67 | ppl 14.47 - 2025-05-28 23:21:01,010 - log
| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 237.04 | loss  2.49 | avg loss  2.68 | ppl 14.53 - 2025-05-28 23:21:24,715 - log
| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 237.27 | loss  2.62 | avg loss  2.63 | ppl 13.87 - 2025-05-28 23:21:48,442 - log
| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 237.60 | loss  2.92 | avg loss  2.65 | ppl 14.21 - 2025-05-28 23:22:12,203 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.12000.ckpt - 2025-05-28 23:22:12,203 - log
eval samples: 0, loss: 1.1977673768997192 - 2025-05-28 23:22:12,316 - log
eval samples: 100, loss: 1.4073446989059448 - 2025-05-28 23:22:23,530 - log
eval samples: 200, loss: 1.1455620527267456 - 2025-05-28 23:22:34,765 - log
eval samples: 300, loss: 1.561932921409607 - 2025-05-28 23:22:45,956 - log
eval samples: 400, loss: 1.4663468599319458 - 2025-05-28 23:22:57,180 - log
eval samples: 500, loss: 1.307837963104248 - 2025-05-28 23:23:08,402 - log
eval samples: 600, loss: 1.2117416858673096 - 2025-05-28 23:23:19,626 - log
eval samples: 700, loss: 1.311022400856018 - 2025-05-28 23:23:30,849 - log
eval samples: 800, loss: 1.183372139930725 - 2025-05-28 23:23:42,079 - log
eval samples: 900, loss: 1.4637432098388672 - 2025-05-28 23:23:53,320 - log
eval samples: 1000, loss: 0.8822451233863831 - 2025-05-28 23:24:04,510 - log
eval samples: 1100, loss: 1.3532031774520874 - 2025-05-28 23:24:15,710 - log
average loss: +1.3021555659807709 - 2025-05-28 23:24:23,215 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:24:23,215 - log
| Eval   6 at step    12000 | time: 131.01s | valid loss  1.30 | valid ppl  3.68 | best ppl  3.68  - 2025-05-28 23:24:23,215 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:24:23,215 - log
| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 1547.44 | loss  2.44 | avg loss  2.66 | ppl 14.25 - 2025-05-28 23:24:46,947 - log
| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 237.24 | loss  2.39 | avg loss  2.70 | ppl 14.85 - 2025-05-28 23:25:10,672 - log
| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 237.07 | loss  2.87 | avg loss  2.65 | ppl 14.20 - 2025-05-28 23:25:34,380 - log
| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 237.40 | loss  2.95 | avg loss  2.67 | ppl 14.49 - 2025-05-28 23:25:58,120 - log
| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 237.48 | loss  2.66 | avg loss  2.69 | ppl 14.75 - 2025-05-28 23:26:21,868 - log
| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 237.38 | loss  2.85 | avg loss  2.71 | ppl 14.98 - 2025-05-28 23:26:45,606 - log
| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 237.69 | loss  2.78 | avg loss  2.65 | ppl 14.13 - 2025-05-28 23:27:09,376 - log
| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 237.60 | loss  2.49 | avg loss  2.67 | ppl 14.38 - 2025-05-28 23:27:33,137 - log
| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 237.52 | loss  2.64 | avg loss  2.70 | ppl 14.81 - 2025-05-28 23:27:56,889 - log
| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 237.68 | loss  2.36 | avg loss  2.66 | ppl 14.35 - 2025-05-28 23:28:20,658 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.13000.ckpt - 2025-05-28 23:28:20,658 - log
| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 237.67 | loss  2.60 | avg loss  2.65 | ppl 14.20 - 2025-05-28 23:28:44,425 - log
| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 237.52 | loss  2.71 | avg loss  2.68 | ppl 14.61 - 2025-05-28 23:29:08,178 - log
| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 237.20 | loss  2.96 | avg loss  2.67 | ppl 14.49 - 2025-05-28 23:29:31,899 - log
| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 237.47 | loss  2.56 | avg loss  2.65 | ppl 14.21 - 2025-05-28 23:29:55,646 - log
| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 237.46 | loss  2.40 | avg loss  2.65 | ppl 14.18 - 2025-05-28 23:30:19,393 - log
| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 237.32 | loss  2.88 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:30:43,126 - log
| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 237.53 | loss  2.61 | avg loss  2.65 | ppl 14.12 - 2025-05-28 23:31:06,879 - log
| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 237.41 | loss  2.63 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:31:30,621 - log
| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 237.05 | loss  2.85 | avg loss  2.63 | ppl 13.86 - 2025-05-28 23:31:54,326 - log
| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 236.92 | loss  2.98 | avg loss  2.64 | ppl 13.99 - 2025-05-28 23:32:18,019 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.14000.ckpt - 2025-05-28 23:32:18,019 - log
eval samples: 0, loss: 1.3151981830596924 - 2025-05-28 23:32:18,132 - log
eval samples: 100, loss: 1.042459487915039 - 2025-05-28 23:32:29,341 - log
eval samples: 200, loss: 1.8930816650390625 - 2025-05-28 23:32:40,558 - log
eval samples: 300, loss: 1.054253101348877 - 2025-05-28 23:32:51,763 - log
eval samples: 400, loss: 1.1351889371871948 - 2025-05-28 23:33:02,972 - log
eval samples: 500, loss: 1.3047995567321777 - 2025-05-28 23:33:14,191 - log
eval samples: 600, loss: 1.018662929534912 - 2025-05-28 23:33:25,393 - log
eval samples: 700, loss: 0.7734968066215515 - 2025-05-28 23:33:36,611 - log
eval samples: 800, loss: 1.4857409000396729 - 2025-05-28 23:33:47,833 - log
eval samples: 900, loss: 1.1919777393341064 - 2025-05-28 23:33:59,072 - log
eval samples: 1000, loss: 1.4323145151138306 - 2025-05-28 23:34:10,273 - log
eval samples: 1100, loss: 1.3710964918136597 - 2025-05-28 23:34:21,469 - log
average loss: +1.2857964905871921 - 2025-05-28 23:34:28,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:34:28,955 - log
| Eval   7 at step    14000 | time: 130.93s | valid loss  1.29 | valid ppl  3.62 | best ppl  3.62  - 2025-05-28 23:34:28,955 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:34:28,955 - log
| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 1546.49 | loss  2.35 | avg loss  2.67 | ppl 14.51 - 2025-05-28 23:34:52,668 - log
| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 236.98 | loss  2.71 | avg loss  2.65 | ppl 14.14 - 2025-05-28 23:35:16,367 - log
| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 237.05 | loss  2.42 | avg loss  2.64 | ppl 13.96 - 2025-05-28 23:35:40,072 - log
| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 236.91 | loss  2.79 | avg loss  2.60 | ppl 13.50 - 2025-05-28 23:36:03,764 - log
| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 237.07 | loss  2.61 | avg loss  2.66 | ppl 14.35 - 2025-05-28 23:36:27,471 - log
| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 237.49 | loss  2.97 | avg loss  2.68 | ppl 14.58 - 2025-05-28 23:36:51,220 - log
| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 236.81 | loss  2.46 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:37:14,902 - log
| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 237.11 | loss  2.90 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:37:38,614 - log
| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 236.92 | loss  2.71 | avg loss  2.67 | ppl 14.39 - 2025-05-28 23:38:02,306 - log
| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 236.49 | loss  2.72 | avg loss  2.66 | ppl 14.27 - 2025-05-28 23:38:25,955 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15000.ckpt - 2025-05-28 23:38:25,956 - log
| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 237.03 | loss  2.73 | avg loss  2.65 | ppl 14.18 - 2025-05-28 23:38:49,659 - log
| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 236.99 | loss  2.73 | avg loss  2.67 | ppl 14.37 - 2025-05-28 23:39:13,358 - log
| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 236.63 | loss  2.66 | avg loss  2.63 | ppl 13.92 - 2025-05-28 23:39:37,022 - log
| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 236.92 | loss  2.47 | avg loss  2.69 | ppl 14.67 - 2025-05-28 23:40:00,714 - log
| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 236.88 | loss  2.71 | avg loss  2.70 | ppl 14.89 - 2025-05-28 23:40:24,402 - log
| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 236.74 | loss  2.40 | avg loss  2.64 | ppl 13.98 - 2025-05-28 23:40:48,077 - log
| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 236.99 | loss  2.75 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:41:11,777 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.15774.pkl - 2025-05-28 23:41:29,071 - log
start to train the model................ 4 - 2025-05-28 23:41:30,946 - log
| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 61.79 | loss  2.69 | avg loss  2.68 | ppl 14.65 - 2025-05-28 23:41:37,125 - log
| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 236.94 | loss  2.55 | avg loss  2.64 | ppl 13.98 - 2025-05-28 23:42:00,819 - log
| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 236.97 | loss  2.84 | avg loss  2.62 | ppl 13.77 - 2025-05-28 23:42:24,516 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.16000.ckpt - 2025-05-28 23:42:24,517 - log
eval samples: 0, loss: 0.9133280515670776 - 2025-05-28 23:42:24,630 - log
eval samples: 100, loss: 1.4265486001968384 - 2025-05-28 23:42:35,819 - log
eval samples: 200, loss: 1.2352440357208252 - 2025-05-28 23:42:47,017 - log
eval samples: 300, loss: 1.3398112058639526 - 2025-05-28 23:42:58,218 - log
eval samples: 400, loss: 1.4475083351135254 - 2025-05-28 23:43:09,422 - log
eval samples: 500, loss: 1.4915218353271484 - 2025-05-28 23:43:20,628 - log
eval samples: 600, loss: 1.522979736328125 - 2025-05-28 23:43:31,820 - log
eval samples: 700, loss: 1.2399332523345947 - 2025-05-28 23:43:43,030 - log
eval samples: 800, loss: 1.5455082654953003 - 2025-05-28 23:43:54,241 - log
eval samples: 900, loss: 1.2995519638061523 - 2025-05-28 23:44:05,459 - log
eval samples: 1000, loss: 1.3533782958984375 - 2025-05-28 23:44:16,687 - log
eval samples: 1100, loss: 1.128389596939087 - 2025-05-28 23:44:27,906 - log
average loss: +1.2753706002888614 - 2025-05-28 23:44:35,429 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:44:35,429 - log
| Eval   8 at step    16000 | time: 130.91s | valid loss  1.28 | valid ppl  3.58 | best ppl  3.58  - 2025-05-28 23:44:35,430 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:44:35,430 - log
| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 1546.89 | loss  2.66 | avg loss  2.64 | ppl 14.08 - 2025-05-28 23:44:59,205 - log
| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 237.66 | loss  2.68 | avg loss  2.62 | ppl 13.75 - 2025-05-28 23:45:22,972 - log
| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 237.88 | loss  2.33 | avg loss  2.67 | ppl 14.39 - 2025-05-28 23:45:46,760 - log
| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 237.98 | loss  2.48 | avg loss  2.62 | ppl 13.70 - 2025-05-28 23:46:10,692 - log
| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 237.59 | loss  2.96 | avg loss  2.64 | ppl 14.04 - 2025-05-28 23:46:34,451 - log
| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 238.14 | loss  2.85 | avg loss  2.64 | ppl 14.02 - 2025-05-28 23:46:58,265 - log
| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 238.04 | loss  2.61 | avg loss  2.65 | ppl 14.16 - 2025-05-28 23:47:22,070 - log
| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 237.76 | loss  2.89 | avg loss  2.66 | ppl 14.29 - 2025-05-28 23:47:45,846 - log
| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 237.73 | loss  2.83 | avg loss  2.69 | ppl 14.74 - 2025-05-28 23:48:09,620 - log
| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 237.57 | loss  2.47 | avg loss  2.64 | ppl 14.04 - 2025-05-28 23:48:33,378 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.17000.ckpt - 2025-05-28 23:48:33,379 - log
| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 237.57 | loss  2.65 | avg loss  2.65 | ppl 14.22 - 2025-05-28 23:48:57,136 - log
| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 237.38 | loss  2.61 | avg loss  2.63 | ppl 13.93 - 2025-05-28 23:49:20,874 - log
| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 237.54 | loss  3.01 | avg loss  2.64 | ppl 13.96 - 2025-05-28 23:49:44,629 - log
| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 237.73 | loss  2.54 | avg loss  2.65 | ppl 14.09 - 2025-05-28 23:50:08,402 - log
| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 237.23 | loss  2.74 | avg loss  2.63 | ppl 13.94 - 2025-05-28 23:50:32,126 - log
| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 237.29 | loss  2.70 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:50:55,855 - log
| epoch   4 step    17700 |   1926 batches | lr 6.66e-05 | ms/batch 237.77 | loss  2.25 | avg loss  2.63 | ppl 13.93 - 2025-05-28 23:51:19,632 - log
| epoch   4 step    17800 |   2026 batches | lr 6.58e-05 | ms/batch 237.65 | loss  2.39 | avg loss  2.65 | ppl 14.17 - 2025-05-28 23:51:43,397 - log
| epoch   4 step    17900 |   2126 batches | lr 6.51e-05 | ms/batch 237.52 | loss  2.39 | avg loss  2.65 | ppl 14.13 - 2025-05-28 23:52:07,150 - log
| epoch   4 step    18000 |   2226 batches | lr 6.43e-05 | ms/batch 237.74 | loss  2.68 | avg loss  2.64 | ppl 13.97 - 2025-05-28 23:52:30,924 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.18000.ckpt - 2025-05-28 23:52:30,924 - log
eval samples: 0, loss: 1.490827202796936 - 2025-05-28 23:52:31,038 - log
eval samples: 100, loss: 1.25664222240448 - 2025-05-28 23:52:42,310 - log
eval samples: 200, loss: 1.105890154838562 - 2025-05-28 23:52:53,569 - log
eval samples: 300, loss: 1.1861921548843384 - 2025-05-28 23:53:04,796 - log
eval samples: 400, loss: 1.1625500917434692 - 2025-05-28 23:53:16,033 - log
eval samples: 500, loss: 1.1545740365982056 - 2025-05-28 23:53:27,293 - log
eval samples: 600, loss: 1.392898678779602 - 2025-05-28 23:53:38,560 - log
eval samples: 700, loss: 1.387902021408081 - 2025-05-28 23:53:49,813 - log
eval samples: 800, loss: 1.4378466606140137 - 2025-05-28 23:54:01,095 - log
eval samples: 900, loss: 0.9580254554748535 - 2025-05-28 23:54:12,352 - log
eval samples: 1000, loss: 1.6278181076049805 - 2025-05-28 23:54:23,685 - log
eval samples: 1100, loss: 1.278861165046692 - 2025-05-28 23:54:35,011 - log
average loss: +1.2846725874465623 - 2025-05-28 23:54:42,632 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:54:42,632 - log
| Eval   9 at step    18000 | time: 131.71s | valid loss  1.28 | valid ppl  3.61 | best ppl  3.58  - 2025-05-28 23:54:42,632 - log
---------------------------------------------------------------------------------------------------- - 2025-05-28 23:54:42,632 - log
| epoch   4 step    18100 |   2326 batches | lr 6.35e-05 | ms/batch 1555.28 | loss  2.64 | avg loss  2.66 | ppl 14.37 - 2025-05-28 23:55:06,453 - log
| epoch   4 step    18200 |   2426 batches | lr 6.27e-05 | ms/batch 237.89 | loss  2.61 | avg loss  2.63 | ppl 13.90 - 2025-05-28 23:55:30,242 - log
| epoch   4 step    18300 |   2526 batches | lr 6.2e-05 | ms/batch 237.94 | loss  2.55 | avg loss  2.64 | ppl 13.95 - 2025-05-28 23:55:54,036 - log
| epoch   4 step    18400 |   2626 batches | lr 6.12e-05 | ms/batch 238.49 | loss  2.42 | avg loss  2.67 | ppl 14.45 - 2025-05-28 23:56:17,886 - log
| epoch   4 step    18500 |   2726 batches | lr 6.04e-05 | ms/batch 238.14 | loss  2.60 | avg loss  2.67 | ppl 14.43 - 2025-05-28 23:56:41,701 - log
| epoch   4 step    18600 |   2826 batches | lr 5.96e-05 | ms/batch 237.90 | loss  2.64 | avg loss  2.62 | ppl 13.75 - 2025-05-28 23:57:05,492 - log
| epoch   4 step    18700 |   2926 batches | lr 5.89e-05 | ms/batch 237.75 | loss  2.41 | avg loss  2.62 | ppl 13.69 - 2025-05-28 23:57:29,267 - log
| epoch   4 step    18800 |   3026 batches | lr 5.81e-05 | ms/batch 238.03 | loss  2.74 | avg loss  2.63 | ppl 13.88 - 2025-05-28 23:57:53,071 - log
| epoch   4 step    18900 |   3126 batches | lr 5.73e-05 | ms/batch 237.83 | loss  2.66 | avg loss  2.63 | ppl 13.81 - 2025-05-28 23:58:16,854 - log
| epoch   4 step    19000 |   3226 batches | lr 5.65e-05 | ms/batch 237.85 | loss  2.69 | avg loss  2.63 | ppl 13.85 - 2025-05-28 23:58:40,639 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.19000.ckpt - 2025-05-28 23:58:40,639 - log
| epoch   4 step    19100 |   3326 batches | lr 5.58e-05 | ms/batch 238.08 | loss  2.48 | avg loss  2.62 | ppl 13.74 - 2025-05-28 23:59:04,447 - log
| epoch   4 step    19200 |   3426 batches | lr 5.5e-05 | ms/batch 238.10 | loss  2.71 | avg loss  2.67 | ppl 14.50 - 2025-05-28 23:59:28,258 - log
| epoch   4 step    19300 |   3526 batches | lr 5.42e-05 | ms/batch 237.95 | loss  2.66 | avg loss  2.64 | ppl 14.01 - 2025-05-28 23:59:52,054 - log
| epoch   4 step    19400 |   3626 batches | lr 5.34e-05 | ms/batch 237.76 | loss  2.73 | avg loss  2.65 | ppl 14.13 - 2025-05-29 00:00:15,830 - log
| epoch   4 step    19500 |   3726 batches | lr 5.27e-05 | ms/batch 238.07 | loss  2.40 | avg loss  2.61 | ppl 13.66 - 2025-05-29 00:00:39,637 - log
| epoch   4 step    19600 |   3826 batches | lr 5.19e-05 | ms/batch 237.99 | loss  2.55 | avg loss  2.64 | ppl 14.01 - 2025-05-29 00:01:03,436 - log
| epoch   4 step    19700 |   3926 batches | lr 5.11e-05 | ms/batch 237.88 | loss  2.59 | avg loss  2.65 | ppl 14.15 - 2025-05-29 00:01:27,225 - log
| epoch   4 step    19800 |   4026 batches | lr 5.03e-05 | ms/batch 237.94 | loss  2.25 | avg loss  2.64 | ppl 13.97 - 2025-05-29 00:01:51,019 - log
| epoch   4 step    19900 |   4126 batches | lr 4.96e-05 | ms/batch 238.21 | loss  2.61 | avg loss  2.64 | ppl 14.04 - 2025-05-29 00:02:14,841 - log
| epoch   4 step    20000 |   4226 batches | lr 4.88e-05 | ms/batch 237.73 | loss  2.71 | avg loss  2.62 | ppl 13.79 - 2025-05-29 00:02:38,614 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.20000.ckpt - 2025-05-29 00:02:38,615 - log
eval samples: 0, loss: 1.4035695791244507 - 2025-05-29 00:02:38,727 - log
eval samples: 100, loss: 1.2429178953170776 - 2025-05-29 00:02:49,963 - log
eval samples: 200, loss: 1.1335124969482422 - 2025-05-29 00:03:01,197 - log
eval samples: 300, loss: 1.4653289318084717 - 2025-05-29 00:03:12,443 - log
eval samples: 400, loss: 1.0698843002319336 - 2025-05-29 00:03:23,684 - log
eval samples: 500, loss: 1.1708630323410034 - 2025-05-29 00:03:34,923 - log
eval samples: 600, loss: 1.6883670091629028 - 2025-05-29 00:03:46,211 - log
eval samples: 700, loss: 0.9250451326370239 - 2025-05-29 00:03:57,452 - log
eval samples: 800, loss: 1.4140985012054443 - 2025-05-29 00:04:08,724 - log
eval samples: 900, loss: 1.2364351749420166 - 2025-05-29 00:04:19,969 - log
eval samples: 1000, loss: 1.2289245128631592 - 2025-05-29 00:04:31,235 - log
eval samples: 1100, loss: 1.0417289733886719 - 2025-05-29 00:04:42,467 - log
average loss: +1.2744250399609134 - 2025-05-29 00:04:50,017 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:04:50,018 - log
| Eval  10 at step    20000 | time: 131.40s | valid loss  1.27 | valid ppl  3.58 | best ppl  3.58  - 2025-05-29 00:04:50,018 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:04:50,018 - log
| epoch   4 step    20100 |   4326 batches | lr 4.8e-05 | ms/batch 1552.30 | loss  2.67 | avg loss  2.62 | ppl 13.78 - 2025-05-29 00:05:13,845 - log
| epoch   4 step    20200 |   4426 batches | lr 4.72e-05 | ms/batch 238.14 | loss  2.46 | avg loss  2.62 | ppl 13.68 - 2025-05-29 00:05:37,659 - log
| epoch   4 step    20300 |   4526 batches | lr 4.65e-05 | ms/batch 238.09 | loss  2.30 | avg loss  2.66 | ppl 14.33 - 2025-05-29 00:06:01,469 - log
| epoch   4 step    20400 |   4626 batches | lr 4.57e-05 | ms/batch 238.20 | loss  2.74 | avg loss  2.65 | ppl 14.21 - 2025-05-29 00:06:25,290 - log
| epoch   4 step    20500 |   4726 batches | lr 4.49e-05 | ms/batch 238.05 | loss  2.60 | avg loss  2.62 | ppl 13.73 - 2025-05-29 00:06:49,096 - log
| epoch   4 step    20600 |   4826 batches | lr 4.41e-05 | ms/batch 237.95 | loss  2.70 | avg loss  2.63 | ppl 13.91 - 2025-05-29 00:07:12,891 - log
| epoch   4 step    20700 |   4926 batches | lr 4.34e-05 | ms/batch 237.93 | loss  2.72 | avg loss  2.65 | ppl 14.18 - 2025-05-29 00:07:36,685 - log
| epoch   4 step    20800 |   5026 batches | lr 4.26e-05 | ms/batch 238.08 | loss  2.32 | avg loss  2.63 | ppl 13.84 - 2025-05-29 00:08:00,494 - log
| epoch   4 step    20900 |   5126 batches | lr 4.18e-05 | ms/batch 238.02 | loss  2.65 | avg loss  2.63 | ppl 13.92 - 2025-05-29 00:08:24,296 - log
| epoch   4 step    21000 |   5226 batches | lr 4.1e-05 | ms/batch 237.98 | loss  2.50 | avg loss  2.66 | ppl 14.23 - 2025-05-29 00:08:48,095 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21000.ckpt - 2025-05-29 00:08:48,095 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.21032.pkl - 2025-05-29 00:08:55,456 - log
start to train the model................ 5 - 2025-05-29 00:08:57,255 - log
| epoch   5 step    21100 |     68 batches | lr 4.02e-05 | ms/batch 162.13 | loss  2.45 | avg loss  2.67 | ppl 14.47 - 2025-05-29 00:09:13,468 - log
| epoch   5 step    21200 |    168 batches | lr 3.95e-05 | ms/batch 237.93 | loss  2.50 | avg loss  2.65 | ppl 14.08 - 2025-05-29 00:09:37,262 - log
| epoch   5 step    21300 |    268 batches | lr 3.87e-05 | ms/batch 237.97 | loss  2.58 | avg loss  2.61 | ppl 13.62 - 2025-05-29 00:10:01,060 - log
| epoch   5 step    21400 |    368 batches | lr 3.79e-05 | ms/batch 238.34 | loss  2.65 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:10:24,894 - log
| epoch   5 step    21500 |    468 batches | lr 3.71e-05 | ms/batch 237.93 | loss  2.38 | avg loss  2.62 | ppl 13.70 - 2025-05-29 00:10:48,687 - log
| epoch   5 step    21600 |    568 batches | lr 3.64e-05 | ms/batch 238.05 | loss  3.35 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:11:12,492 - log
| epoch   5 step    21700 |    668 batches | lr 3.56e-05 | ms/batch 238.35 | loss  2.76 | avg loss  2.62 | ppl 13.75 - 2025-05-29 00:11:36,328 - log
| epoch   5 step    21800 |    768 batches | lr 3.48e-05 | ms/batch 238.16 | loss  2.98 | avg loss  2.62 | ppl 13.74 - 2025-05-29 00:12:00,144 - log
| epoch   5 step    21900 |    868 batches | lr 3.4e-05 | ms/batch 238.24 | loss  2.49 | avg loss  2.59 | ppl 13.33 - 2025-05-29 00:12:23,969 - log
| epoch   5 step    22000 |    968 batches | lr 3.33e-05 | ms/batch 238.79 | loss  2.56 | avg loss  2.62 | ppl 13.80 - 2025-05-29 00:12:47,848 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.22000.ckpt - 2025-05-29 00:12:47,849 - log
eval samples: 0, loss: 1.4649550914764404 - 2025-05-29 00:12:47,962 - log
eval samples: 100, loss: 1.378952980041504 - 2025-05-29 00:12:59,189 - log
eval samples: 200, loss: 1.2507259845733643 - 2025-05-29 00:13:10,448 - log
eval samples: 300, loss: 1.2470675706863403 - 2025-05-29 00:13:21,688 - log
eval samples: 400, loss: 1.4162238836288452 - 2025-05-29 00:13:32,924 - log
eval samples: 500, loss: 1.2719230651855469 - 2025-05-29 00:13:44,156 - log
eval samples: 600, loss: 1.1327487230300903 - 2025-05-29 00:13:55,374 - log
eval samples: 700, loss: 1.009505033493042 - 2025-05-29 00:14:06,594 - log
eval samples: 800, loss: 1.11649751663208 - 2025-05-29 00:14:17,815 - log
eval samples: 900, loss: 1.453521490097046 - 2025-05-29 00:14:29,058 - log
eval samples: 1000, loss: 1.1007330417633057 - 2025-05-29 00:14:40,269 - log
eval samples: 1100, loss: 1.2721974849700928 - 2025-05-29 00:14:51,493 - log
average loss: +1.278125743292374 - 2025-05-29 00:14:59,003 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:14:59,003 - log
| Eval  11 at step    22000 | time: 131.15s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59  - 2025-05-29 00:14:59,004 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:14:59,004 - log
| epoch   5 step    22100 |   1068 batches | lr 3.25e-05 | ms/batch 1549.39 | loss  2.56 | avg loss  2.60 | ppl 13.52 - 2025-05-29 00:15:22,787 - log
| epoch   5 step    22200 |   1168 batches | lr 3.17e-05 | ms/batch 237.86 | loss  2.61 | avg loss  2.64 | ppl 14.01 - 2025-05-29 00:15:46,575 - log
| epoch   5 step    22300 |   1268 batches | lr 3.09e-05 | ms/batch 237.89 | loss  2.60 | avg loss  2.67 | ppl 14.50 - 2025-05-29 00:16:10,364 - log
| epoch   5 step    22400 |   1368 batches | lr 3.02e-05 | ms/batch 237.89 | loss  2.62 | avg loss  2.60 | ppl 13.49 - 2025-05-29 00:16:34,153 - log
| epoch   5 step    22500 |   1468 batches | lr 2.94e-05 | ms/batch 237.69 | loss  2.98 | avg loss  2.62 | ppl 13.74 - 2025-05-29 00:16:57,923 - log
| epoch   5 step    22600 |   1568 batches | lr 2.86e-05 | ms/batch 237.95 | loss  2.37 | avg loss  2.61 | ppl 13.65 - 2025-05-29 00:17:21,718 - log
| epoch   5 step    22700 |   1668 batches | lr 2.78e-05 | ms/batch 237.91 | loss  2.63 | avg loss  2.63 | ppl 13.89 - 2025-05-29 00:17:45,510 - log
| epoch   5 step    22800 |   1768 batches | lr 2.71e-05 | ms/batch 238.09 | loss  2.54 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:18:09,319 - log
| epoch   5 step    22900 |   1868 batches | lr 2.63e-05 | ms/batch 238.09 | loss  2.43 | avg loss  2.61 | ppl 13.58 - 2025-05-29 00:18:33,129 - log
| epoch   5 step    23000 |   1968 batches | lr 2.55e-05 | ms/batch 238.00 | loss  2.64 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:18:56,929 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.23000.ckpt - 2025-05-29 00:18:56,929 - log
| epoch   5 step    23100 |   2068 batches | lr 2.47e-05 | ms/batch 238.04 | loss  2.79 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:19:20,734 - log
| epoch   5 step    23200 |   2168 batches | lr 2.4e-05 | ms/batch 237.74 | loss  2.54 | avg loss  2.60 | ppl 13.51 - 2025-05-29 00:19:44,508 - log
| epoch   5 step    23300 |   2268 batches | lr 2.32e-05 | ms/batch 238.17 | loss  3.10 | avg loss  2.63 | ppl 13.93 - 2025-05-29 00:20:08,325 - log
| epoch   5 step    23400 |   2368 batches | lr 2.24e-05 | ms/batch 237.93 | loss  2.78 | avg loss  2.63 | ppl 13.81 - 2025-05-29 00:20:32,119 - log
| epoch   5 step    23500 |   2468 batches | lr 2.16e-05 | ms/batch 237.67 | loss  2.89 | avg loss  2.65 | ppl 14.18 - 2025-05-29 00:20:55,886 - log
| epoch   5 step    23600 |   2568 batches | lr 2.09e-05 | ms/batch 237.73 | loss  2.48 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:21:19,660 - log
| epoch   5 step    23700 |   2668 batches | lr 2.01e-05 | ms/batch 237.94 | loss  2.39 | avg loss  2.60 | ppl 13.48 - 2025-05-29 00:21:43,454 - log
| epoch   5 step    23800 |   2768 batches | lr 1.93e-05 | ms/batch 237.86 | loss  2.58 | avg loss  2.60 | ppl 13.47 - 2025-05-29 00:22:07,241 - log
| epoch   5 step    23900 |   2868 batches | lr 1.85e-05 | ms/batch 237.73 | loss  2.90 | avg loss  2.63 | ppl 13.82 - 2025-05-29 00:22:31,015 - log
| epoch   5 step    24000 |   2968 batches | lr 1.78e-05 | ms/batch 238.07 | loss  2.70 | avg loss  2.65 | ppl 14.10 - 2025-05-29 00:22:54,823 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.24000.ckpt - 2025-05-29 00:22:54,823 - log
eval samples: 0, loss: 1.519622564315796 - 2025-05-29 00:22:54,936 - log
eval samples: 100, loss: 1.1023486852645874 - 2025-05-29 00:23:06,188 - log
eval samples: 200, loss: 1.3553576469421387 - 2025-05-29 00:23:17,417 - log
eval samples: 300, loss: 1.1929954290390015 - 2025-05-29 00:23:28,653 - log
eval samples: 400, loss: 1.008903980255127 - 2025-05-29 00:23:39,892 - log
eval samples: 500, loss: 1.6253399848937988 - 2025-05-29 00:23:51,129 - log
eval samples: 600, loss: 1.3831034898757935 - 2025-05-29 00:24:02,368 - log
eval samples: 700, loss: 1.1554266214370728 - 2025-05-29 00:24:13,595 - log
eval samples: 800, loss: 1.1323539018630981 - 2025-05-29 00:24:24,856 - log
eval samples: 900, loss: 1.40337073802948 - 2025-05-29 00:24:36,113 - log
eval samples: 1000, loss: 1.0282998085021973 - 2025-05-29 00:24:47,354 - log
eval samples: 1100, loss: 1.1170381307601929 - 2025-05-29 00:24:58,589 - log
average loss: +1.277289811317643 - 2025-05-29 00:25:06,103 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:25:06,103 - log
| Eval  12 at step    24000 | time: 131.28s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59  - 2025-05-29 00:25:06,103 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:25:06,103 - log
| epoch   5 step    24100 |   3068 batches | lr 1.7e-05 | ms/batch 1550.82 | loss  2.80 | avg loss  2.64 | ppl 14.03 - 2025-05-29 00:25:29,905 - log
| epoch   5 step    24200 |   3168 batches | lr 1.62e-05 | ms/batch 238.28 | loss  2.73 | avg loss  2.67 | ppl 14.44 - 2025-05-29 00:25:53,733 - log
| epoch   5 step    24300 |   3268 batches | lr 1.54e-05 | ms/batch 238.04 | loss  2.71 | avg loss  2.64 | ppl 14.06 - 2025-05-29 00:26:17,538 - log
| epoch   5 step    24400 |   3368 batches | lr 1.47e-05 | ms/batch 238.16 | loss  2.73 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:26:41,355 - log
| epoch   5 step    24500 |   3468 batches | lr 1.39e-05 | ms/batch 238.27 | loss  2.41 | avg loss  2.64 | ppl 13.95 - 2025-05-29 00:27:05,182 - log
| epoch   5 step    24600 |   3568 batches | lr 1.31e-05 | ms/batch 238.00 | loss  2.63 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:27:28,982 - log
| epoch   5 step    24700 |   3668 batches | lr 1.23e-05 | ms/batch 238.05 | loss  2.88 | avg loss  2.63 | ppl 13.82 - 2025-05-29 00:27:52,788 - log
| epoch   5 step    24800 |   3768 batches | lr 1.16e-05 | ms/batch 238.20 | loss  2.24 | avg loss  2.58 | ppl 13.20 - 2025-05-29 00:28:16,608 - log
| epoch   5 step    24900 |   3868 batches | lr 1.08e-05 | ms/batch 237.98 | loss  2.78 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:28:40,407 - log
| epoch   5 step    25000 |   3968 batches | lr 1e-05 | ms/batch 238.04 | loss  2.76 | avg loss  2.64 | ppl 14.02 - 2025-05-29 00:29:04,212 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.25000.ckpt - 2025-05-29 00:29:04,212 - log
| epoch   5 step    25100 |   4068 batches | lr 9.23e-06 | ms/batch 238.05 | loss  2.63 | avg loss  2.61 | ppl 13.55 - 2025-05-29 00:29:28,017 - log
| epoch   5 step    25200 |   4168 batches | lr 8.45e-06 | ms/batch 238.21 | loss  2.44 | avg loss  2.63 | ppl 13.93 - 2025-05-29 00:29:51,839 - log
| epoch   5 step    25300 |   4268 batches | lr 7.68e-06 | ms/batch 238.89 | loss  3.01 | avg loss  2.60 | ppl 13.48 - 2025-05-29 00:30:15,728 - log
| epoch   5 step    25400 |   4368 batches | lr 6.9e-06 | ms/batch 238.78 | loss  2.52 | avg loss  2.63 | ppl 13.91 - 2025-05-29 00:30:39,606 - log
| epoch   5 step    25500 |   4468 batches | lr 6.13e-06 | ms/batch 238.92 | loss  2.33 | avg loss  2.63 | ppl 13.85 - 2025-05-29 00:31:03,499 - log
| epoch   5 step    25600 |   4568 batches | lr 5.35e-06 | ms/batch 238.85 | loss  2.70 | avg loss  2.65 | ppl 14.10 - 2025-05-29 00:31:27,385 - log
| epoch   5 step    25700 |   4668 batches | lr 4.58e-06 | ms/batch 238.22 | loss  2.45 | avg loss  2.64 | ppl 14.00 - 2025-05-29 00:31:51,208 - log
| epoch   5 step    25800 |   4768 batches | lr 3.8e-06 | ms/batch 238.35 | loss  2.53 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:32:15,043 - log
| epoch   5 step    25900 |   4868 batches | lr 3.02e-06 | ms/batch 238.26 | loss  2.72 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:32:38,870 - log
| epoch   5 step    26000 |   4968 batches | lr 2.25e-06 | ms/batch 238.40 | loss  2.74 | avg loss  2.62 | ppl 13.77 - 2025-05-29 00:33:02,710 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26000.ckpt - 2025-05-29 00:33:02,711 - log
eval samples: 0, loss: 0.9992844462394714 - 2025-05-29 00:33:02,824 - log
eval samples: 100, loss: 1.017942190170288 - 2025-05-29 00:33:14,095 - log
eval samples: 200, loss: 1.122837781906128 - 2025-05-29 00:33:25,357 - log
eval samples: 300, loss: 1.7177746295928955 - 2025-05-29 00:33:36,665 - log
eval samples: 400, loss: 0.9698410034179688 - 2025-05-29 00:33:47,938 - log
eval samples: 500, loss: 1.6140111684799194 - 2025-05-29 00:33:59,214 - log
eval samples: 600, loss: 1.2799748182296753 - 2025-05-29 00:34:10,456 - log
eval samples: 700, loss: 1.5499166250228882 - 2025-05-29 00:34:21,721 - log
eval samples: 800, loss: 0.9487221240997314 - 2025-05-29 00:34:32,977 - log
eval samples: 900, loss: 1.5838167667388916 - 2025-05-29 00:34:44,227 - log
eval samples: 1000, loss: 1.4388667345046997 - 2025-05-29 00:34:55,465 - log
eval samples: 1100, loss: 1.3310248851776123 - 2025-05-29 00:35:06,720 - log
average loss: +1.2717146032477078 - 2025-05-29 00:35:14,253 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:35:14,254 - log
| Eval  13 at step    26000 | time: 131.54s | valid loss  1.27 | valid ppl  3.57 | best ppl  3.57  - 2025-05-29 00:35:14,254 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:35:14,254 - log
| epoch   5 step    26100 |   5068 batches | lr 1.47e-06 | ms/batch 1553.59 | loss  2.65 | avg loss  2.61 | ppl 13.64 - 2025-05-29 00:35:38,069 - log
| epoch   5 step    26200 |   5168 batches | lr 6.98e-07 | ms/batch 238.08 | loss  2.92 | avg loss  2.63 | ppl 13.83 - 2025-05-29 00:36:01,878 - log
saving checkpoint, ./trained_models/GPT2_M/e2e/model.26290.pkl - 2025-05-29 00:36:23,051 - log
---------------------------------------------------------------------------------------------------- - 2025-05-29 00:36:24,830 - log
End of training - 2025-05-29 00:36:24,830 - log
ms/batch 301.44 - 2025-05-29 00:36:24,831 - log
