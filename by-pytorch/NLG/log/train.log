myrank: 0 local_rank: 0 device_count: 1 world_size: 1
====================================================================================================
        - platform : local
        - local_rank : 0
        - rank : 0
        - device : cuda:0
        - world_size : 1
        - random_seed : 110
        - lr : 0.0002
        - weight_decay : 0.01
        - correct_bias : True
        - adam_epislon : 1e-06
        - no_decay_bias : False
        - adam_beta1 : 0.9
        - adam_beta2 : 0.999
        - scheduler : linear
        - max_step : None
        - max_epoch : 5
        - warmup_step : 500
        - i_steps : 0
        - i_lrs : 0.00025
        - train_data : ./data/e2e/train.jsonl
        - valid_data : ./data/e2e/valid.jsonl
        - train_batch_size : 8
        - valid_batch_size : 4
        - grad_acc : 1
        - clip : 0.0
        - seq_len : 512
        - model_card : gpt2.md
        - init_checkpoint : ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin
        - fp16 : False
        - log_interval : 100
        - eval_interval : 2000
        - save_interval : 1000
        - work_dir : ./trained_models/GPT2_M/e2e
        - lora_dim : 4
        - lora_alpha : 32
        - obj : clm
        - lora_dropout : 0.1
        - label_smooth : 0.1
        - roll_interval : -1
        - roll_lr : 1e-05
        - roll_step : 100
        - eval_epoch : 1
        - dist : <module 'torch.distributed' from '/root/miniconda3/lib/python3.8/site-packages/torch/distributed/__init__.py'>
====================================================================================================
Experiment dir : ./trained_models/GPT2_M/e2e
loading model pretrained weight.
set max_step: 26290
start to train the model................ 1
/root/LoRA/examples/NLG/src/optimizer.py:117: UserWarning: This overload of addcdiv_ is deprecated:
	addcdiv_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  p.data.addcdiv_(-step_size, exp_avg, denom)
| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 965.68 | loss  5.18 | avg loss  5.56 | ppl 259.11
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 965.94 | loss  3.21 | avg loss  3.75 | ppl 42.57
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 967.22 | loss  2.97 | avg loss  3.08 | ppl 21.71
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 966.61 | loss  3.11 | avg loss  2.98 | ppl 19.59
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 967.49 | loss  2.84 | avg loss  2.89 | ppl 17.98
| epoch   1 step      600 |    600 batches | lr 0.000199 | ms/batch 967.01 | loss  2.76 | avg loss  2.83 | ppl 16.89
| epoch   1 step      700 |    700 batches | lr 0.000198 | ms/batch 965.57 | loss  2.87 | avg loss  2.79 | ppl 16.29
| epoch   1 step      800 |    800 batches | lr 0.000198 | ms/batch 964.75 | loss  2.47 | avg loss  2.76 | ppl 15.72
| epoch   1 step      900 |    900 batches | lr 0.000197 | ms/batch 965.54 | loss  2.50 | avg loss  2.74 | ppl 15.51
| epoch   1 step     1000 |   1000 batches | lr 0.000196 | ms/batch 965.41 | loss  3.18 | avg loss  2.76 | ppl 15.88
saving checkpoint ./trained_models/GPT2_M/e2e/model.1000.pt
| epoch   1 step     1100 |   1100 batches | lr 0.000195 | ms/batch 965.65 | loss  2.83 | avg loss  2.76 | ppl 15.78
| epoch   1 step     1200 |   1200 batches | lr 0.000195 | ms/batch 964.97 | loss  2.58 | avg loss  2.75 | ppl 15.66
| epoch   1 step     1300 |   1300 batches | lr 0.000194 | ms/batch 964.79 | loss  2.61 | avg loss  2.72 | ppl 15.15
| epoch   1 step     1400 |   1400 batches | lr 0.000193 | ms/batch 964.41 | loss  2.70 | avg loss  2.72 | ppl 15.15
| epoch   1 step     1500 |   1500 batches | lr 0.000192 | ms/batch 963.95 | loss  2.66 | avg loss  2.73 | ppl 15.27
| epoch   1 step     1600 |   1600 batches | lr 0.000191 | ms/batch 964.13 | loss  2.70 | avg loss  2.68 | ppl 14.59
| epoch   1 step     1700 |   1700 batches | lr 0.000191 | ms/batch 965.17 | loss  2.58 | avg loss  2.69 | ppl 14.79
| epoch   1 step     1800 |   1800 batches | lr 0.00019 | ms/batch 965.64 | loss  2.55 | avg loss  2.69 | ppl 14.67
| epoch   1 step     1900 |   1900 batches | lr 0.000189 | ms/batch 963.78 | loss  2.69 | avg loss  2.68 | ppl 14.65
| epoch   1 step     2000 |   2000 batches | lr 0.000188 | ms/batch 964.19 | loss  2.51 | avg loss  2.67 | ppl 14.44
saving checkpoint ./trained_models/GPT2_M/e2e/model.2000.pt
/root/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
eval samples: 0 loss: tensor(1.3109, device='cuda:0')
eval samples: 100 loss: tensor(0.9550, device='cuda:0')
eval samples: 200 loss: tensor(1.3877, device='cuda:0')
eval samples: 300 loss: tensor(1.1741, device='cuda:0')
eval samples: 400 loss: tensor(0.9290, device='cuda:0')
eval samples: 500 loss: tensor(0.7314, device='cuda:0')
eval samples: 600 loss: tensor(1.3043, device='cuda:0')
eval samples: 700 loss: tensor(0.8267, device='cuda:0')
eval samples: 800 loss: tensor(1.2156, device='cuda:0')
eval samples: 900 loss: tensor(1.7542, device='cuda:0')
eval samples: 1000 loss: tensor(1.1794, device='cuda:0')
eval samples: 1100 loss: tensor(1.4660, device='cuda:0')
average loss 1.3106846822655365
----------------------------------------------------------------------------------------------------
| Eval   1 at step     2000 | time: 256.42s | valid loss  1.31 | valid ppl  3.71 | best ppl  3.71 
----------------------------------------------------------------------------------------------------
| epoch   1 step     2100 |   2100 batches | lr 0.000188 | ms/batch 3528.02 | loss  2.70 | avg loss  2.67 | ppl 14.46
| epoch   1 step     2200 |   2200 batches | lr 0.000187 | ms/batch 964.25 | loss  2.92 | avg loss  2.66 | ppl 14.30
| epoch   1 step     2300 |   2300 batches | lr 0.000186 | ms/batch 964.47 | loss  2.68 | avg loss  2.65 | ppl 14.18
| epoch   1 step     2400 |   2400 batches | lr 0.000185 | ms/batch 964.38 | loss  2.41 | avg loss  2.66 | ppl 14.23
| epoch   1 step     2500 |   2500 batches | lr 0.000184 | ms/batch 964.58 | loss  2.82 | avg loss  2.67 | ppl 14.47
| epoch   1 step     2600 |   2600 batches | lr 0.000184 | ms/batch 964.78 | loss  2.80 | avg loss  2.65 | ppl 14.22
| epoch   1 step     2700 |   2700 batches | lr 0.000183 | ms/batch 964.53 | loss  2.71 | avg loss  2.65 | ppl 14.10
| epoch   1 step     2800 |   2800 batches | lr 0.000182 | ms/batch 964.67 | loss  2.38 | avg loss  2.68 | ppl 14.53
| epoch   1 step     2900 |   2900 batches | lr 0.000181 | ms/batch 963.90 | loss  2.86 | avg loss  2.66 | ppl 14.35
| epoch   1 step     3000 |   3000 batches | lr 0.000181 | ms/batch 964.76 | loss  2.67 | avg loss  2.63 | ppl 13.81
saving checkpoint ./trained_models/GPT2_M/e2e/model.3000.pt
| epoch   1 step     3100 |   3100 batches | lr 0.00018 | ms/batch 963.96 | loss  2.90 | avg loss  2.63 | ppl 13.83
| epoch   1 step     3200 |   3200 batches | lr 0.000179 | ms/batch 963.91 | loss  2.70 | avg loss  2.66 | ppl 14.32
| epoch   1 step     3300 |   3300 batches | lr 0.000178 | ms/batch 964.66 | loss  3.02 | avg loss  2.62 | ppl 13.71
| epoch   1 step     3400 |   3400 batches | lr 0.000178 | ms/batch 964.21 | loss  2.56 | avg loss  2.62 | ppl 13.70
| epoch   1 step     3500 |   3500 batches | lr 0.000177 | ms/batch 963.74 | loss  2.39 | avg loss  2.65 | ppl 14.11
| epoch   1 step     3600 |   3600 batches | lr 0.000176 | ms/batch 963.71 | loss  2.53 | avg loss  2.65 | ppl 14.19
| epoch   1 step     3700 |   3700 batches | lr 0.000175 | ms/batch 962.86 | loss  2.61 | avg loss  2.62 | ppl 13.80
| epoch   1 step     3800 |   3800 batches | lr 0.000174 | ms/batch 963.67 | loss  2.58 | avg loss  2.58 | ppl 13.17
| epoch   1 step     3900 |   3900 batches | lr 0.000174 | ms/batch 963.13 | loss  2.33 | avg loss  2.66 | ppl 14.24
| epoch   1 step     4000 |   4000 batches | lr 0.000173 | ms/batch 962.37 | loss  2.67 | avg loss  2.65 | ppl 14.17
saving checkpoint ./trained_models/GPT2_M/e2e/model.4000.pt
eval samples: 0 loss: tensor(1.2478, device='cuda:0')
eval samples: 100 loss: tensor(0.9417, device='cuda:0')
eval samples: 200 loss: tensor(1.3541, device='cuda:0')
eval samples: 300 loss: tensor(1.1447, device='cuda:0')
eval samples: 400 loss: tensor(0.9280, device='cuda:0')
eval samples: 500 loss: tensor(0.6583, device='cuda:0')
eval samples: 600 loss: tensor(1.2476, device='cuda:0')
eval samples: 700 loss: tensor(0.7790, device='cuda:0')
eval samples: 800 loss: tensor(1.2059, device='cuda:0')
eval samples: 900 loss: tensor(1.7763, device='cuda:0')
eval samples: 1000 loss: tensor(1.0642, device='cuda:0')
eval samples: 1100 loss: tensor(1.4676, device='cuda:0')
average loss 1.268988605384549
----------------------------------------------------------------------------------------------------
| Eval   2 at step     4000 | time: 255.81s | valid loss  1.27 | valid ppl  3.56 | best ppl  3.56 
----------------------------------------------------------------------------------------------------
| epoch   1 step     4100 |   4100 batches | lr 0.000172 | ms/batch 3521.44 | loss  2.71 | avg loss  2.60 | ppl 13.49
| epoch   1 step     4200 |   4200 batches | lr 0.000171 | ms/batch 962.98 | loss  2.68 | avg loss  2.61 | ppl 13.63
| epoch   1 step     4300 |   4300 batches | lr 0.000171 | ms/batch 964.17 | loss  2.43 | avg loss  2.57 | ppl 13.04
| epoch   1 step     4400 |   4400 batches | lr 0.00017 | ms/batch 964.86 | loss  2.66 | avg loss  2.60 | ppl 13.45
| epoch   1 step     4500 |   4500 batches | lr 0.000169 | ms/batch 965.50 | loss  2.77 | avg loss  2.63 | ppl 13.83
| epoch   1 step     4600 |   4600 batches | lr 0.000168 | ms/batch 965.95 | loss  2.57 | avg loss  2.62 | ppl 13.79
| epoch   1 step     4700 |   4700 batches | lr 0.000167 | ms/batch 965.55 | loss  2.39 | avg loss  2.62 | ppl 13.70
| epoch   1 step     4800 |   4800 batches | lr 0.000167 | ms/batch 966.25 | loss  2.83 | avg loss  2.59 | ppl 13.30
| epoch   1 step     4900 |   4900 batches | lr 0.000166 | ms/batch 967.06 | loss  2.65 | avg loss  2.60 | ppl 13.50
| epoch   1 step     5000 |   5000 batches | lr 0.000165 | ms/batch 967.26 | loss  2.35 | avg loss  2.58 | ppl 13.16
saving checkpoint ./trained_models/GPT2_M/e2e/model.5000.pt
| epoch   1 step     5100 |   5100 batches | lr 0.000164 | ms/batch 967.55 | loss  2.65 | avg loss  2.61 | ppl 13.61
| epoch   1 step     5200 |   5200 batches | lr 0.000164 | ms/batch 967.15 | loss  2.76 | avg loss  2.60 | ppl 13.41
saving checkpoint ./trained_models/GPT2_M/e2e/model.5258.pt
start to train the model................ 2
| epoch   2 step     5300 |     42 batches | lr 0.000163 | ms/batch 406.35 | loss  2.45 | avg loss  2.54 | ppl 12.65
| epoch   2 step     5400 |    142 batches | lr 0.000162 | ms/batch 968.49 | loss  2.50 | avg loss  2.60 | ppl 13.47
| epoch   2 step     5500 |    242 batches | lr 0.000161 | ms/batch 966.83 | loss  2.59 | avg loss  2.58 | ppl 13.26
| epoch   2 step     5600 |    342 batches | lr 0.00016 | ms/batch 965.72 | loss  2.21 | avg loss  2.60 | ppl 13.45
| epoch   2 step     5700 |    442 batches | lr 0.00016 | ms/batch 964.65 | loss  2.53 | avg loss  2.59 | ppl 13.40
| epoch   2 step     5800 |    542 batches | lr 0.000159 | ms/batch 965.68 | loss  2.61 | avg loss  2.56 | ppl 13.00
| epoch   2 step     5900 |    642 batches | lr 0.000158 | ms/batch 965.19 | loss  2.66 | avg loss  2.59 | ppl 13.39
| epoch   2 step     6000 |    742 batches | lr 0.000157 | ms/batch 966.91 | loss  2.31 | avg loss  2.56 | ppl 12.88
saving checkpoint ./trained_models/GPT2_M/e2e/model.6000.pt
eval samples: 0 loss: tensor(1.2223, device='cuda:0')
eval samples: 100 loss: tensor(0.9188, device='cuda:0')
eval samples: 200 loss: tensor(1.2457, device='cuda:0')
eval samples: 300 loss: tensor(1.1539, device='cuda:0')
eval samples: 400 loss: tensor(0.8873, device='cuda:0')
eval samples: 500 loss: tensor(0.6202, device='cuda:0')
eval samples: 600 loss: tensor(1.2489, device='cuda:0')
eval samples: 700 loss: tensor(0.7568, device='cuda:0')
eval samples: 800 loss: tensor(1.1822, device='cuda:0')
eval samples: 900 loss: tensor(1.7903, device='cuda:0')
eval samples: 1000 loss: tensor(1.0350, device='cuda:0')
eval samples: 1100 loss: tensor(1.4578, device='cuda:0')
average loss 1.2333497247875553
----------------------------------------------------------------------------------------------------
| Eval   3 at step     6000 | time: 256.29s | valid loss  1.23 | valid ppl  3.43 | best ppl  3.43 
----------------------------------------------------------------------------------------------------
| epoch   2 step     6100 |    842 batches | lr 0.000157 | ms/batch 3527.79 | loss  2.34 | avg loss  2.58 | ppl 13.16
| epoch   2 step     6200 |    942 batches | lr 0.000156 | ms/batch 964.34 | loss  2.42 | avg loss  2.58 | ppl 13.16
| epoch   2 step     6300 |   1042 batches | lr 0.000155 | ms/batch 964.79 | loss  2.83 | avg loss  2.58 | ppl 13.23
| epoch   2 step     6400 |   1142 batches | lr 0.000154 | ms/batch 964.05 | loss  2.37 | avg loss  2.58 | ppl 13.15
| epoch   2 step     6500 |   1242 batches | lr 0.000153 | ms/batch 963.68 | loss  2.43 | avg loss  2.60 | ppl 13.40
| epoch   2 step     6600 |   1342 batches | lr 0.000153 | ms/batch 964.35 | loss  2.85 | avg loss  2.57 | ppl 13.05
| epoch   2 step     6700 |   1442 batches | lr 0.000152 | ms/batch 965.03 | loss  2.51 | avg loss  2.57 | ppl 13.13
| epoch   2 step     6800 |   1542 batches | lr 0.000151 | ms/batch 965.35 | loss  2.56 | avg loss  2.59 | ppl 13.30
| epoch   2 step     6900 |   1642 batches | lr 0.00015 | ms/batch 964.52 | loss  2.48 | avg loss  2.53 | ppl 12.54
| epoch   2 step     7000 |   1742 batches | lr 0.00015 | ms/batch 965.03 | loss  2.70 | avg loss  2.58 | ppl 13.19
saving checkpoint ./trained_models/GPT2_M/e2e/model.7000.pt
| epoch   2 step     7100 |   1842 batches | lr 0.000149 | ms/batch 965.26 | loss  2.70 | avg loss  2.57 | ppl 13.05
| epoch   2 step     7200 |   1942 batches | lr 0.000148 | ms/batch 964.85 | loss  2.62 | avg loss  2.57 | ppl 13.03
| epoch   2 step     7300 |   2042 batches | lr 0.000147 | ms/batch 964.90 | loss  2.42 | avg loss  2.54 | ppl 12.70
| epoch   2 step     7400 |   2142 batches | lr 0.000146 | ms/batch 965.16 | loss  2.91 | avg loss  2.54 | ppl 12.68
| epoch   2 step     7500 |   2242 batches | lr 0.000146 | ms/batch 964.91 | loss  2.69 | avg loss  2.59 | ppl 13.34
| epoch   2 step     7600 |   2342 batches | lr 0.000145 | ms/batch 964.33 | loss  2.36 | avg loss  2.55 | ppl 12.78
| epoch   2 step     7700 |   2442 batches | lr 0.000144 | ms/batch 964.31 | loss  2.72 | avg loss  2.55 | ppl 12.79
| epoch   2 step     7800 |   2542 batches | lr 0.000143 | ms/batch 964.21 | loss  2.58 | avg loss  2.54 | ppl 12.63
| epoch   2 step     7900 |   2642 batches | lr 0.000143 | ms/batch 964.41 | loss  2.87 | avg loss  2.57 | ppl 13.11
| epoch   2 step     8000 |   2742 batches | lr 0.000142 | ms/batch 964.48 | loss  2.63 | avg loss  2.59 | ppl 13.33
saving checkpoint ./trained_models/GPT2_M/e2e/model.8000.pt
eval samples: 0 loss: tensor(1.2080, device='cuda:0')
eval samples: 100 loss: tensor(0.9238, device='cuda:0')
eval samples: 200 loss: tensor(1.2597, device='cuda:0')
eval samples: 300 loss: tensor(1.1013, device='cuda:0')
eval samples: 400 loss: tensor(0.8920, device='cuda:0')
eval samples: 500 loss: tensor(0.6469, device='cuda:0')
eval samples: 600 loss: tensor(1.1917, device='cuda:0')
eval samples: 700 loss: tensor(0.7409, device='cuda:0')
eval samples: 800 loss: tensor(1.1630, device='cuda:0')
eval samples: 900 loss: tensor(1.7668, device='cuda:0')
eval samples: 1000 loss: tensor(1.0223, device='cuda:0')
eval samples: 1100 loss: tensor(1.4347, device='cuda:0')
average loss 1.2180707292401627
----------------------------------------------------------------------------------------------------
| Eval   4 at step     8000 | time: 256.25s | valid loss  1.22 | valid ppl  3.38 | best ppl  3.38 
----------------------------------------------------------------------------------------------------
| epoch   2 step     8100 |   2842 batches | lr 0.000141 | ms/batch 3527.07 | loss  2.41 | avg loss  2.55 | ppl 12.80
| epoch   2 step     8200 |   2942 batches | lr 0.00014 | ms/batch 965.54 | loss  2.63 | avg loss  2.60 | ppl 13.46
| epoch   2 step     8300 |   3042 batches | lr 0.00014 | ms/batch 965.73 | loss  2.45 | avg loss  2.57 | ppl 13.06
| epoch   2 step     8400 |   3142 batches | lr 0.000139 | ms/batch 965.31 | loss  2.58 | avg loss  2.55 | ppl 12.85
| epoch   2 step     8500 |   3242 batches | lr 0.000138 | ms/batch 964.87 | loss  2.52 | avg loss  2.56 | ppl 12.93
| epoch   2 step     8600 |   3342 batches | lr 0.000137 | ms/batch 965.08 | loss  2.57 | avg loss  2.54 | ppl 12.74
| epoch   2 step     8700 |   3442 batches | lr 0.000136 | ms/batch 965.10 | loss  2.84 | avg loss  2.54 | ppl 12.74
| epoch   2 step     8800 |   3542 batches | lr 0.000136 | ms/batch 965.51 | loss  2.44 | avg loss  2.54 | ppl 12.73
| epoch   2 step     8900 |   3642 batches | lr 0.000135 | ms/batch 964.03 | loss  2.62 | avg loss  2.57 | ppl 13.07
| epoch   2 step     9000 |   3742 batches | lr 0.000134 | ms/batch 963.62 | loss  2.57 | avg loss  2.56 | ppl 12.91
saving checkpoint ./trained_models/GPT2_M/e2e/model.9000.pt
| epoch   2 step     9100 |   3842 batches | lr 0.000133 | ms/batch 965.04 | loss  2.50 | avg loss  2.55 | ppl 12.83
| epoch   2 step     9200 |   3942 batches | lr 0.000133 | ms/batch 964.66 | loss  2.67 | avg loss  2.56 | ppl 12.87
| epoch   2 step     9300 |   4042 batches | lr 0.000132 | ms/batch 964.05 | loss  2.53 | avg loss  2.57 | ppl 13.02
| epoch   2 step     9400 |   4142 batches | lr 0.000131 | ms/batch 964.27 | loss  2.57 | avg loss  2.55 | ppl 12.80
| epoch   2 step     9500 |   4242 batches | lr 0.00013 | ms/batch 963.77 | loss  2.57 | avg loss  2.55 | ppl 12.81
| epoch   2 step     9600 |   4342 batches | lr 0.000129 | ms/batch 963.18 | loss  2.89 | avg loss  2.56 | ppl 12.92
| epoch   2 step     9700 |   4442 batches | lr 0.000129 | ms/batch 963.58 | loss  2.32 | avg loss  2.54 | ppl 12.69
| epoch   2 step     9800 |   4542 batches | lr 0.000128 | ms/batch 963.87 | loss  2.55 | avg loss  2.54 | ppl 12.66
| epoch   2 step     9900 |   4642 batches | lr 0.000127 | ms/batch 963.45 | loss  2.24 | avg loss  2.52 | ppl 12.49
| epoch   2 step    10000 |   4742 batches | lr 0.000126 | ms/batch 964.42 | loss  2.64 | avg loss  2.56 | ppl 12.98
saving checkpoint ./trained_models/GPT2_M/e2e/model.10000.pt
eval samples: 0 loss: tensor(1.1786, device='cuda:0')
eval samples: 100 loss: tensor(0.8734, device='cuda:0')
eval samples: 200 loss: tensor(1.2571, device='cuda:0')
eval samples: 300 loss: tensor(1.0976, device='cuda:0')
eval samples: 400 loss: tensor(0.8734, device='cuda:0')
eval samples: 500 loss: tensor(0.5777, device='cuda:0')
eval samples: 600 loss: tensor(1.1538, device='cuda:0')
eval samples: 700 loss: tensor(0.6943, device='cuda:0')
eval samples: 800 loss: tensor(1.1472, device='cuda:0')
eval samples: 900 loss: tensor(1.7787, device='cuda:0')
eval samples: 1000 loss: tensor(0.9894, device='cuda:0')
eval samples: 1100 loss: tensor(1.4649, device='cuda:0')
average loss 1.1966486377258823
----------------------------------------------------------------------------------------------------
| Eval   5 at step    10000 | time: 256.34s | valid loss  1.20 | valid ppl  3.31 | best ppl  3.31 
----------------------------------------------------------------------------------------------------
| epoch   2 step    10100 |   4842 batches | lr 0.000126 | ms/batch 3528.74 | loss  2.34 | avg loss  2.55 | ppl 12.76
| epoch   2 step    10200 |   4942 batches | lr 0.000125 | ms/batch 965.08 | loss  2.47 | avg loss  2.55 | ppl 12.86
| epoch   2 step    10300 |   5042 batches | lr 0.000124 | ms/batch 965.86 | loss  2.83 | avg loss  2.54 | ppl 12.63
| epoch   2 step    10400 |   5142 batches | lr 0.000123 | ms/batch 964.82 | loss  2.71 | avg loss  2.54 | ppl 12.64
| epoch   2 step    10500 |   5242 batches | lr 0.000122 | ms/batch 964.43 | loss  2.46 | avg loss  2.52 | ppl 12.38
saving checkpoint ./trained_models/GPT2_M/e2e/model.10516.pt
start to train the model................ 3
| epoch   3 step    10600 |     84 batches | lr 0.000122 | ms/batch 811.33 | loss  2.19 | avg loss  2.53 | ppl 12.51
| epoch   3 step    10700 |    184 batches | lr 0.000121 | ms/batch 966.70 | loss  2.61 | avg loss  2.54 | ppl 12.70
| epoch   3 step    10800 |    284 batches | lr 0.00012 | ms/batch 965.70 | loss  2.78 | avg loss  2.53 | ppl 12.50
| epoch   3 step    10900 |    384 batches | lr 0.000119 | ms/batch 966.40 | loss  2.85 | avg loss  2.53 | ppl 12.53
| epoch   3 step    11000 |    484 batches | lr 0.000119 | ms/batch 967.20 | loss  2.64 | avg loss  2.52 | ppl 12.49
saving checkpoint ./trained_models/GPT2_M/e2e/model.11000.pt
| epoch   3 step    11100 |    584 batches | lr 0.000118 | ms/batch 965.24 | loss  2.60 | avg loss  2.58 | ppl 13.21
| epoch   3 step    11200 |    684 batches | lr 0.000117 | ms/batch 965.22 | loss  2.59 | avg loss  2.53 | ppl 12.57
| epoch   3 step    11300 |    784 batches | lr 0.000116 | ms/batch 965.65 | loss  2.59 | avg loss  2.55 | ppl 12.79
| epoch   3 step    11400 |    884 batches | lr 0.000115 | ms/batch 965.44 | loss  2.54 | avg loss  2.54 | ppl 12.67
| epoch   3 step    11500 |    984 batches | lr 0.000115 | ms/batch 966.60 | loss  2.27 | avg loss  2.54 | ppl 12.71
| epoch   3 step    11600 |   1084 batches | lr 0.000114 | ms/batch 966.01 | loss  2.19 | avg loss  2.50 | ppl 12.15
| epoch   3 step    11700 |   1184 batches | lr 0.000113 | ms/batch 966.01 | loss  2.37 | avg loss  2.52 | ppl 12.42
| epoch   3 step    11800 |   1284 batches | lr 0.000112 | ms/batch 966.33 | loss  2.54 | avg loss  2.50 | ppl 12.14
| epoch   3 step    11900 |   1384 batches | lr 0.000112 | ms/batch 966.06 | loss  2.43 | avg loss  2.51 | ppl 12.29
| epoch   3 step    12000 |   1484 batches | lr 0.000111 | ms/batch 964.08 | loss  2.57 | avg loss  2.52 | ppl 12.48
saving checkpoint ./trained_models/GPT2_M/e2e/model.12000.pt
eval samples: 0 loss: tensor(1.1986, device='cuda:0')
eval samples: 100 loss: tensor(0.8892, device='cuda:0')
eval samples: 200 loss: tensor(1.2682, device='cuda:0')
eval samples: 300 loss: tensor(1.1300, device='cuda:0')
eval samples: 400 loss: tensor(0.9089, device='cuda:0')
eval samples: 500 loss: tensor(0.6050, device='cuda:0')
eval samples: 600 loss: tensor(1.1596, device='cuda:0')
eval samples: 700 loss: tensor(0.7053, device='cuda:0')
eval samples: 800 loss: tensor(1.1729, device='cuda:0')
eval samples: 900 loss: tensor(1.7867, device='cuda:0')
eval samples: 1000 loss: tensor(0.9569, device='cuda:0')
eval samples: 1100 loss: tensor(1.4843, device='cuda:0')
average loss 1.211860730529648
----------------------------------------------------------------------------------------------------
| Eval   6 at step    12000 | time: 255.85s | valid loss  1.21 | valid ppl  3.36 | best ppl  3.36 
----------------------------------------------------------------------------------------------------
| epoch   3 step    12100 |   1584 batches | lr 0.00011 | ms/batch 3522.07 | loss  2.49 | avg loss  2.54 | ppl 12.65
| epoch   3 step    12200 |   1684 batches | lr 0.000109 | ms/batch 963.45 | loss  2.46 | avg loss  2.55 | ppl 12.80
| epoch   3 step    12300 |   1784 batches | lr 0.000108 | ms/batch 963.58 | loss  2.48 | avg loss  2.53 | ppl 12.61
| epoch   3 step    12400 |   1884 batches | lr 0.000108 | ms/batch 963.15 | loss  2.56 | avg loss  2.50 | ppl 12.20
| epoch   3 step    12500 |   1984 batches | lr 0.000107 | ms/batch 962.90 | loss  2.74 | avg loss  2.53 | ppl 12.55
| epoch   3 step    12600 |   2084 batches | lr 0.000106 | ms/batch 964.01 | loss  2.83 | avg loss  2.47 | ppl 11.84
| epoch   3 step    12700 |   2184 batches | lr 0.000105 | ms/batch 963.43 | loss  2.35 | avg loss  2.52 | ppl 12.45
| epoch   3 step    12800 |   2284 batches | lr 0.000105 | ms/batch 963.75 | loss  2.60 | avg loss  2.53 | ppl 12.51
| epoch   3 step    12900 |   2384 batches | lr 0.000104 | ms/batch 963.15 | loss  2.43 | avg loss  2.53 | ppl 12.52
| epoch   3 step    13000 |   2484 batches | lr 0.000103 | ms/batch 962.23 | loss  3.04 | avg loss  2.54 | ppl 12.73
saving checkpoint ./trained_models/GPT2_M/e2e/model.13000.pt
| epoch   3 step    13100 |   2584 batches | lr 0.000102 | ms/batch 962.87 | loss  2.36 | avg loss  2.50 | ppl 12.12
| epoch   3 step    13200 |   2684 batches | lr 0.000102 | ms/batch 963.87 | loss  2.43 | avg loss  2.55 | ppl 12.86
| epoch   3 step    13300 |   2784 batches | lr 0.000101 | ms/batch 963.44 | loss  2.44 | avg loss  2.50 | ppl 12.20
| epoch   3 step    13400 |   2884 batches | lr 0.0001 | ms/batch 963.02 | loss  2.63 | avg loss  2.50 | ppl 12.17
| epoch   3 step    13500 |   2984 batches | lr 9.92e-05 | ms/batch 964.50 | loss  2.90 | avg loss  2.54 | ppl 12.65
| epoch   3 step    13600 |   3084 batches | lr 9.84e-05 | ms/batch 964.24 | loss  2.32 | avg loss  2.53 | ppl 12.58
| epoch   3 step    13700 |   3184 batches | lr 9.76e-05 | ms/batch 964.38 | loss  2.67 | avg loss  2.56 | ppl 12.96
| epoch   3 step    13800 |   3284 batches | lr 9.69e-05 | ms/batch 964.64 | loss  2.29 | avg loss  2.49 | ppl 12.08
| epoch   3 step    13900 |   3384 batches | lr 9.61e-05 | ms/batch 964.57 | loss  2.89 | avg loss  2.54 | ppl 12.70
| epoch   3 step    14000 |   3484 batches | lr 9.53e-05 | ms/batch 964.78 | loss  2.67 | avg loss  2.55 | ppl 12.75
saving checkpoint ./trained_models/GPT2_M/e2e/model.14000.pt
eval samples: 0 loss: tensor(1.1335, device='cuda:0')
eval samples: 100 loss: tensor(0.8561, device='cuda:0')
eval samples: 200 loss: tensor(1.2805, device='cuda:0')
eval samples: 300 loss: tensor(1.0845, device='cuda:0')
eval samples: 400 loss: tensor(0.9060, device='cuda:0')
eval samples: 500 loss: tensor(0.5853, device='cuda:0')
eval samples: 600 loss: tensor(1.1900, device='cuda:0')
eval samples: 700 loss: tensor(0.7217, device='cuda:0')
eval samples: 800 loss: tensor(1.1267, device='cuda:0')
eval samples: 900 loss: tensor(1.7719, device='cuda:0')
eval samples: 1000 loss: tensor(0.9512, device='cuda:0')
eval samples: 1100 loss: tensor(1.4728, device='cuda:0')
average loss 1.1879359161200589
----------------------------------------------------------------------------------------------------
| Eval   7 at step    14000 | time: 255.83s | valid loss  1.19 | valid ppl  3.28 | best ppl  3.28 
----------------------------------------------------------------------------------------------------
| epoch   3 step    14100 |   3584 batches | lr 9.45e-05 | ms/batch 3522.73 | loss  2.56 | avg loss  2.47 | ppl 11.88
| epoch   3 step    14200 |   3684 batches | lr 9.38e-05 | ms/batch 963.94 | loss  2.51 | avg loss  2.50 | ppl 12.14
| epoch   3 step    14300 |   3784 batches | lr 9.3e-05 | ms/batch 964.21 | loss  2.74 | avg loss  2.50 | ppl 12.17
| epoch   3 step    14400 |   3884 batches | lr 9.22e-05 | ms/batch 963.98 | loss  2.25 | avg loss  2.51 | ppl 12.30
| epoch   3 step    14500 |   3984 batches | lr 9.14e-05 | ms/batch 964.77 | loss  2.37 | avg loss  2.53 | ppl 12.50
| epoch   3 step    14600 |   4084 batches | lr 9.07e-05 | ms/batch 964.30 | loss  2.52 | avg loss  2.53 | ppl 12.58
| epoch   3 step    14700 |   4184 batches | lr 8.99e-05 | ms/batch 963.45 | loss  2.49 | avg loss  2.50 | ppl 12.14
| epoch   3 step    14800 |   4284 batches | lr 8.91e-05 | ms/batch 964.53 | loss  2.51 | avg loss  2.53 | ppl 12.61
| epoch   3 step    14900 |   4384 batches | lr 8.83e-05 | ms/batch 964.24 | loss  2.52 | avg loss  2.51 | ppl 12.29
| epoch   3 step    15000 |   4484 batches | lr 8.76e-05 | ms/batch 964.75 | loss  2.79 | avg loss  2.54 | ppl 12.72
saving checkpoint ./trained_models/GPT2_M/e2e/model.15000.pt
| epoch   3 step    15100 |   4584 batches | lr 8.68e-05 | ms/batch 964.75 | loss  2.33 | avg loss  2.50 | ppl 12.24
| epoch   3 step    15200 |   4684 batches | lr 8.6e-05 | ms/batch 965.17 | loss  2.51 | avg loss  2.50 | ppl 12.20
| epoch   3 step    15300 |   4784 batches | lr 8.52e-05 | ms/batch 965.49 | loss  2.48 | avg loss  2.54 | ppl 12.69
| epoch   3 step    15400 |   4884 batches | lr 8.45e-05 | ms/batch 965.20 | loss  2.40 | avg loss  2.52 | ppl 12.42
| epoch   3 step    15500 |   4984 batches | lr 8.37e-05 | ms/batch 964.84 | loss  2.43 | avg loss  2.52 | ppl 12.37
| epoch   3 step    15600 |   5084 batches | lr 8.29e-05 | ms/batch 963.22 | loss  2.63 | avg loss  2.51 | ppl 12.25
| epoch   3 step    15700 |   5184 batches | lr 8.21e-05 | ms/batch 963.23 | loss  2.47 | avg loss  2.48 | ppl 11.96
saving checkpoint ./trained_models/GPT2_M/e2e/model.15774.pt
start to train the model................ 4
| epoch   4 step    15800 |     26 batches | lr 8.13e-05 | ms/batch 250.78 | loss  2.38 | avg loss  2.50 | ppl 12.12
| epoch   4 step    15900 |    126 batches | lr 8.06e-05 | ms/batch 965.46 | loss  2.38 | avg loss  2.50 | ppl 12.13
| epoch   4 step    16000 |    226 batches | lr 7.98e-05 | ms/batch 963.61 | loss  2.52 | avg loss  2.52 | ppl 12.41
saving checkpoint ./trained_models/GPT2_M/e2e/model.16000.pt
eval samples: 0 loss: tensor(1.1614, device='cuda:0')
eval samples: 100 loss: tensor(0.8739, device='cuda:0')
eval samples: 200 loss: tensor(1.2379, device='cuda:0')
eval samples: 300 loss: tensor(1.0967, device='cuda:0')
eval samples: 400 loss: tensor(0.8924, device='cuda:0')
eval samples: 500 loss: tensor(0.5558, device='cuda:0')
eval samples: 600 loss: tensor(1.1437, device='cuda:0')
eval samples: 700 loss: tensor(0.6742, device='cuda:0')
eval samples: 800 loss: tensor(1.1574, device='cuda:0')
eval samples: 900 loss: tensor(1.7585, device='cuda:0')
eval samples: 1000 loss: tensor(0.9343, device='cuda:0')
eval samples: 1100 loss: tensor(1.4444, device='cuda:0')
average loss 1.182796740368621
----------------------------------------------------------------------------------------------------
| Eval   8 at step    16000 | time: 255.73s | valid loss  1.18 | valid ppl  3.26 | best ppl  3.26 
----------------------------------------------------------------------------------------------------
| epoch   4 step    16100 |    326 batches | lr 7.9e-05 | ms/batch 3520.27 | loss  2.71 | avg loss  2.49 | ppl 12.02
| epoch   4 step    16200 |    426 batches | lr 7.82e-05 | ms/batch 963.55 | loss  2.44 | avg loss  2.50 | ppl 12.13
| epoch   4 step    16300 |    526 batches | lr 7.75e-05 | ms/batch 963.49 | loss  2.86 | avg loss  2.48 | ppl 11.97
| epoch   4 step    16400 |    626 batches | lr 7.67e-05 | ms/batch 962.73 | loss  2.89 | avg loss  2.50 | ppl 12.14
| epoch   4 step    16500 |    726 batches | lr 7.59e-05 | ms/batch 962.92 | loss  2.73 | avg loss  2.53 | ppl 12.50
| epoch   4 step    16600 |    826 batches | lr 7.51e-05 | ms/batch 964.18 | loss  2.71 | avg loss  2.49 | ppl 12.11
| epoch   4 step    16700 |    926 batches | lr 7.44e-05 | ms/batch 963.39 | loss  2.66 | avg loss  2.51 | ppl 12.29
| epoch   4 step    16800 |   1026 batches | lr 7.36e-05 | ms/batch 962.27 | loss  2.66 | avg loss  2.51 | ppl 12.27
| epoch   4 step    16900 |   1126 batches | lr 7.28e-05 | ms/batch 963.11 | loss  2.19 | avg loss  2.48 | ppl 11.88
| epoch   4 step    17000 |   1226 batches | lr 7.2e-05 | ms/batch 963.60 | loss  2.35 | avg loss  2.47 | ppl 11.87
saving checkpoint ./trained_models/GPT2_M/e2e/model.17000.pt
| epoch   4 step    17100 |   1326 batches | lr 7.13e-05 | ms/batch 964.60 | loss  2.63 | avg loss  2.53 | ppl 12.55
| epoch   4 step    17200 |   1426 batches | lr 7.05e-05 | ms/batch 965.04 | loss  2.40 | avg loss  2.47 | ppl 11.83
| epoch   4 step    17300 |   1526 batches | lr 6.97e-05 | ms/batch 964.75 | loss  2.36 | avg loss  2.50 | ppl 12.24
| epoch   4 step    17400 |   1626 batches | lr 6.89e-05 | ms/batch 965.07 | loss  2.31 | avg loss  2.50 | ppl 12.22
| epoch   4 step    17500 |   1726 batches | lr 6.82e-05 | ms/batch 964.93 | loss  2.69 | avg loss  2.51 | ppl 12.34
| epoch   4 step    17600 |   1826 batches | lr 6.74e-05 | ms/batch 963.07 | loss  2.81 | avg loss  2.52 | ppl 12.39
